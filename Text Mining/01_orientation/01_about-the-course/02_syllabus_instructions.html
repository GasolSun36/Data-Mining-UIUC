<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Course Description
 </h1>
 <p>
  Recent years have seen a
  <strong>
   dramatic growth
  </strong>
  of natural language
  <strong>
   text data
  </strong>
  , including web pages, news articles, scientific literature, emails, enterprise documents, and social media such as blog articles, forum posts, product reviews, and tweets. This has led to an increasing demand for powerful software tools to help people analyze and manage vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans and are accompanied by semantically rich content. As such, text data are
  <strong>
   especially valuable for discovering knowledge about people’s opinions and preferences
  </strong>
  , in addition to many other kinds of knowledge that we encode in text. However, in contrast to structured data, which conform to well-defined schemas, and are thus relatively easy for computers to handle, text has less explicit structure, thus requiring computer processing toward understanding of the content encoded in text.  The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to mining and analysis of text data have been developed over the past few decades. They are usually
  <strong>
   very robust
  </strong>
  and can be applied to analyze and manage text data in
  <strong>
   any natural language
  </strong>
  and about
  <strong>
   any topic
  </strong>
  .
 </p>
 <p>
  This course provides an introduction to some of these approaches with an
  <strong>
   emphasis on approaches that do not require (much) manual effort
  </strong>
  , including those for mining word associations, mining and analyzing topics in text, clustering and categorizing text data, opinion mining and sentiment analysis, and joint analysis of text and non-textual data. You will learn the
  <strong>
   most useful basic concepts, principles, and techniques
  </strong>
  in text mining and analytics that can be applied to build
  <strong>
   a wide range of text mining and analytics application systems
  </strong>
  .
 </p>
 <h1 level="1">
  Course Goals and Objectives
 </h1>
 <p>
  By the end the course, you will be able to do the following:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Explain many basic concepts and multiple major algorithms in text mining and analytics.
   </p>
  </li>
  <li>
   <p>
    Explain how statistical language models, particularly topic models, can be applied to arbitrary text data to discover and analyze topics in text.
   </p>
  </li>
  <li>
   <p>
    Implement some text mining and analytics algorithms, run text mining experiments, and experiment with ideas on a real text mining task to improve the mining results (if you complete the programming assignment).
   </p>
  </li>
 </ul>
 <h1 level="1">
  Course Outline
 </h1>
 <p>
  The course consists of
  <strong>
   6 weekly modules
  </strong>
  . Please note: There are no required readings for this course. All readings listed below are optional.
 </p>
 <h3 level="3">
  <strong>
   Week 1
  </strong>
 </h3>
 <p>
  <strong>
   Key Concepts:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Part of speech tagging
   </p>
  </li>
  <li>
   <p>
    Syntactic analysis
   </p>
  </li>
  <li>
   <p>
    Semantic analysis
   </p>
  </li>
  <li>
   <p>
    Ambiguity
   </p>
  </li>
  <li>
   <p>
    Text representation, especially bag-of-words representation
   </p>
  </li>
  <li>
   <p>
    Context of a word; context similarity
   </p>
  </li>
  <li>
   <p>
    Paradigmatic relation
   </p>
  </li>
  <li>
   <p>
    Syntagmatic relation
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Recommended Readings:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining.
    </em>
    ACM and Morgan &amp; Claypool Publishers, 2016. Chapters 1-4, Chapter 13.
   </p>
  </li>
  <li>
   <p>
    Chris Manning and Hinrich Schütze,
    <em>
     Foundations of Statistical Natural Language Processing.
    </em>
    MIT Press. Cambridge, MA: May 1999. (Chapter 5 on collocations)
   </p>
  </li>
  <li>
   <p>
    Chengxiang Zhai,
    <em>
     Exploiting context to identify lexical atoms: A statistical view of linguistic context
    </em>
    . Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brazil, Feb. 4-6, 1997. pp. 119-129.
   </p>
  </li>
  <li>
   <p>
    Shan Jiang and ChengXiang Zhai,
    <em>
     Random walks on adjacency graphs for mining lexical relations from big text data
    </em>
    . Proceedings of IEEE BigData Conference 2014, pp. 549-554.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Week 2
  </strong>
 </p>
 <p>
  <strong>
   Key Concepts:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Entropy
   </p>
  </li>
  <li>
   <p>
    Conditional entropy
   </p>
  </li>
  <li>
   <p>
    Mutual information
   </p>
  </li>
  <li>
   <p>
    Topic and coverage of topic
   </p>
  </li>
  <li>
   <p>
    Language model
   </p>
  </li>
  <li>
   <p>
    Generative model
   </p>
  </li>
  <li>
   <p>
    Unigram language model
   </p>
  </li>
  <li>
   <p>
    Word distribution
   </p>
  </li>
  <li>
   <p>
    Background language model
   </p>
  </li>
  <li>
   <p>
    Parameters of a probabilistic model
   </p>
  </li>
  <li>
   <p>
    Likelihood
   </p>
  </li>
  <li>
   <p>
    Bayes rule
   </p>
  </li>
  <li>
   <p>
    Maximum likelihood estimation
   </p>
  </li>
  <li>
   <p>
    Prior and posterior distributions
   </p>
  </li>
  <li>
   <p>
    Bayesian estimation &amp; inference
   </p>
  </li>
  <li>
   <p>
    Maximum a posteriori (MAP) estimate
   </p>
  </li>
  <li>
   <p>
    Prior model
   </p>
  </li>
  <li>
   <p>
    Posterior mode
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Recommended Readings:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining.
    </em>
    ACM and Morgan &amp; Claypool Publishers, 2016. Chapters 13, 17
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Week 3
  </strong>
 </p>
 <p>
  <strong>
   Key Concepts:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Mixture model
   </p>
  </li>
  <li>
   <p>
    Component model
   </p>
  </li>
  <li>
   <p>
    Constraints on probabilities
   </p>
  </li>
  <li>
   <p>
    Probabilistic Latent Semantic Analysis (PLSA)
   </p>
  </li>
  <li>
   <p>
    Expectation-Maximization (EM) algorithm
   </p>
  </li>
  <li>
   <p>
    E-step and M-step
   </p>
  </li>
  <li>
   <p>
    Hidden variables
   </p>
  </li>
  <li>
   <p>
    Hill climbing
   </p>
  </li>
  <li>
   <p>
    Local maximum
   </p>
  </li>
  <li>
   <p>
    Latent Dirichlet Allocation (LDA)
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Recommended Readings:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining
    </em>
    . ACM and Morgan &amp; Claypool Publishers, 2016. Chapter 17.
   </p>
  </li>
  <li>
   <p>
    Blei, D. 2012.
    <em>
     Probabilistic Topic Models
    </em>
    . Communications of the ACM 55 (4): 77–84. doi: 10.1145/2133806.2133826.
   </p>
  </li>
  <li>
   <p>
    Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
    <em>
     Automatic Labeling of Multinomial Topic Models
    </em>
    . Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246.
   </p>
  </li>
  <li>
   <p>
    Yue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011.
    <em>
     Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA
    </em>
    . Information Retrieval, 14, 2 (April 2011), 178-203. doi: 10.1007/s10791-010-9141-9.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Week 4
  </strong>
 </p>
 <p>
  <strong>
   Key Concepts:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Clustering, document clustering, and term clustering
   </p>
  </li>
  <li>
   <p>
    Clustering bias
   </p>
  </li>
  <li>
   <p>
    Perspective of similarity
   </p>
  </li>
  <li>
   <p>
    Mixture model, likelihood, and maximum likelihood estimation
   </p>
  </li>
  <li>
   <p>
    EM algorithm, E-step, M-step, underflow, normalization (to avoid underflow)
   </p>
  </li>
  <li>
   <p>
    Hierarchical Agglomerative Clustering, and k-Means
   </p>
  </li>
  <li>
   <p>
    Direction evaluation (of clustering), indirect evaluation (of clustering)
   </p>
  </li>
  <li>
   <p>
    Text categorization, topic categorization, sentiment categorization, email routing
   </p>
  </li>
  <li>
   <p>
    Spam filtering
   </p>
  </li>
  <li>
   <p>
    Naïve Bayes classifier
   </p>
  </li>
  <li>
   <p>
    Smoothing
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Recommended Readings:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining
    </em>
    . ACM and Morgan &amp; Claypool Publishers, 2016. Chapters 14 &amp; 15.
   </p>
  </li>
  <li>
   <p>
    Manning, Chris D., Prabhakar Raghavan, and Hinrich Schütze.
    <em>
     Introduction to Information Retrieval
    </em>
    . Cambridge: Cambridge University Press, 2007.  (Chapters 13-16)
   </p>
  </li>
  <li>
   <p>
    Yang, Yiming.
    <em>
     An Evaluation of Statistical Approaches to Text Categorization
    </em>
    . Inf. Retr. 1, 1-2 (May 1999), 69-90. doi: 10.1023/A:1009982220290
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Week 5
  </strong>
 </p>
 <p>
  <strong>
   Key Concepts:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Generative classifier vs. discriminative classifier
   </p>
  </li>
  <li>
   <p>
    Training data
   </p>
  </li>
  <li>
   <p>
    Logistic regression
   </p>
  </li>
  <li>
   <p>
    K-Nearest Neighbor classifier
   </p>
  </li>
  <li>
   <p>
    Support Vector Machine (SVM), margin, and linear separator
   </p>
  </li>
  <li>
   <p>
    Classification accuracy, precision, recall, F measure, macro-averaging, and micro-averaging
   </p>
  </li>
  <li>
   <p>
    Opinion holder, opinion target, sentiment, opinion representation
   </p>
  </li>
  <li>
   <p>
    Sentiment classification
   </p>
  </li>
  <li>
   <p>
    Features, n-grams, frequent patterns, and overfitting
   </p>
  </li>
  <li>
   <p>
    Ordinal logistic regression
   </p>
  </li>
  <li>
   <p>
    Rating prediction
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Recommended Readings:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining.
    </em>
    ACM and Morgan &amp; Claypool Publishers, 2016. Chapters 15 &amp; 18
   </p>
  </li>
  <li>
   <p>
    Yang, Yiming. An Evaluation of Statistical Approaches to Text Categorization. Inf. Retr. 1, 1-2 (May 1999), 69-90. doi: 10.1023/A:1009982220290
   </p>
  </li>
  <li>
   <p>
    Bing Liu,
    <em>
     Sentiment analysis and opinion mining.
    </em>
    Morgan &amp; Claypool Publishers, 2012.
   </p>
  </li>
  <li>
   <p>
    Bo Pang and Lillian Lee,
    <em>
     Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieva
    </em>
    l 2(1-2), pp. 1–135, 2008.
   </p>
  </li>
 </ul>
 <h3 level="3">
  <strong>
   Week 6
  </strong>
 </h3>
 <p>
  <strong>
   Key Concepts:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Aspect rating and aspect weight
   </p>
  </li>
  <li>
   <p>
    Latent aspect rating analysis (LARA)
   </p>
  </li>
  <li>
   <p>
    Latent rating regression model
   </p>
  </li>
  <li>
   <p>
    Generative model
   </p>
  </li>
  <li>
   <p>
    Rating prediction
   </p>
  </li>
  <li>
   <p>
    Normal/Gaussian distribution
   </p>
  </li>
  <li>
   <p>
    Prior vs. posterior probability
   </p>
  </li>
  <li>
   <p>
    Text-based prediction
   </p>
  </li>
  <li>
   <p>
    The “data mining loop”
   </p>
  </li>
  <li>
   <p>
    Context (of text data) and contextual text mining
   </p>
  </li>
  <li>
   <p>
    Contextual probabilistic latent semantic analysis (CPLSA): views of a topic and coverage of topics
   </p>
  </li>
  <li>
   <p>
    Spatiotemporal trends of topics
   </p>
  </li>
  <li>
   <p>
    Event impact analysis
   </p>
  </li>
  <li>
   <p>
    Network-regularized topic modeling
   </p>
  </li>
  <li>
   <p>
    NetPLSA
   </p>
  </li>
  <li>
   <p>
    Causal topics
   </p>
  </li>
  <li>
   <p>
    Iterative topic modeling with time series supervision
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Recommended Readings:
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    C. Zhai and S. Massung,
    <em>
     Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining.
    </em>
    ACM and Morgan &amp; Claypool Publishers, 2016. Chapters 18 &amp; 19
   </p>
  </li>
  <li>
   <p>
    Hongning Wang, Yue Lu, and ChengXiang Zhai,
    <em>
     Latent aspect rating analysis on review text data: a rating regression approach
    </em>
    . In Proceedings of ACM KDD 2010, pp. 783-792, 2010. doi: 10.1145/1835804.1835903
   </p>
  </li>
  <li>
   <p>
    Hongning Wang, Yue Lu, and ChengXiang Zhai. 2011.
    <em>
     Latent aspect rating analysis without aspect keyword supervision
    </em>
    . In Proceedings of ACM KDD 2011, pp. 618-626. doi: 10.1145/2020408.2020505
   </p>
  </li>
  <li>
   <p>
    ChengXiang Zhai, Atulya Velivelli, and Bei Yu.
    <em>
     A cross-collection mixture model for comparative text mining
    </em>
    . In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. doi: 10.1145/1014052.1014150
   </p>
  </li>
  <li>
   <p>
    Qiaozhu Mei,
    <a href="http://hdl.handle.net/2142/14707">
     Contextual Text Mining
    </a>
    , Ph.D. Thesis, University of Illinois at Urbana-Champaign, 2009.
   </p>
  </li>
  <li>
   <p>
    Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier.
    <em>
     Mining causal topics in text data: Iterative topic modeling with time series feedback
    </em>
    . In Proceedings of the 22nd ACM international conference on information &amp; knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. doi: 10.1145/2505515.2505612
   </p>
  </li>
  <li>
   <p>
    Noah Smith,
    <em>
     T
    </em>
    <em>
     ext-Driven Forecasting
    </em>
    . Retrieved May 31, 2015 from
    <a href="http://www.cs.cmu.edu/~nasmith/papers/smith.whitepaper10.pdf">
     http://www.cs.cmu.edu/~nasmith/papers/smith.whitepaper10.pdf
    </a>
   </p>
  </li>
 </ul>
 <h1 level="1">
  Elements of This Course
 </h1>
 <p>
  The course is comprised of the following elements:
 </p>
 <p>
  <strong>
   Lecture videos
  </strong>
  . Each week your instructor will teach you the concepts you need to know through a collection of short video lectures. You may either stream these videos for playback within the browser by clicking on their titles, or you can download each video for later offline playback by clicking the download icon.
 </p>
 <p>
  The videos each week usually total 1.5 to 2 hours, but you generally need to spend at least the same amount of time digesting the content in the video. The actual amount of time needed to digest the content would naturally vary according to your background.
 </p>
 <p>
  <strong>
   Quizzes
  </strong>
  . Each week will include one for-credit quiz. Your cumulative score will be used when calculating your final score in the class. There is no time limit on how long you take to complete each quiz. The deadline for all quizzes is the last day of the course.
 </p>
 <p>
  The weekly quiz should take less than 1 hour to finish, assuming you have mastered the materials to be tested in the quiz, and you should make sure that you have mastered the materials before you attempt to work on the quizzes. You should use the practice quizzes to help you understand and master the materials.
 </p>
 <p>
  <strong>
   Programming assignment
  </strong>
  . The programming assignment for this course is optional, but it provides an opportunity for you to practice your programming skills and apply what you've learned in the course. Plan about 2 hours each week to work on it if you plan to finish it; you may need to budget more time for this if you are not familiar with C++ programming.
 </p>
 <h1 level="1">
  Information about Lectures
 </h1>
 <p>
  The lectures in this course contain the most important information you need to know. You can access these lectures in each week's lesson section. The following resources accompany each video:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    The play button will open the video up in your browser window and stream the lecture to you. The duration of the video (in hours-minutes-seconds format) is also listed.
   </p>
  </li>
  <li>
   <p>
    English subtitles are available for all videos.All video lectures have a discussion forum dedicated to them. This is a great place to discuss any questions you have about the content of the video or to share your ideas and responses to the video.
   </p>
  </li>
 </ul>
 <h1 level="1">
  Discussion Forums
 </h1>
 <p>
  The discussion forums are a key element of this course. Be sure to read more
  <strong>
   <a href="https://www.coursera.org/learn/text-mining/supplement/njNbI/about-the-discussion-forums">
    about the discussion forums
   </a>
  </strong>
  and how you can make the most of them in this class.
 </p>
 <h1 level="1">
  How to Pass the Course
 </h1>
 <p>
  I am continually looking to improve this course and may encounter some issues requiring us to make changes sooner rather than later. As such, this syllabus is subject to change. I appreciate your input and ask that you have patience as we make adjustments to this course. I also recognize that this is no ordinary course. You may have different perspectives and different goals for this course than some of your peers, or that I could have anticipated. Therefore, I want to empower you to customize this course to meet your needs.
 </p>
 <p>
  To qualify for a Course Certificate, simply start verifying your coursework at the beginning of the course, get an
  <strong>
   7
  </strong>
  <strong>
   0% or higher
  </strong>
  on all quizzes and assignments combined, and pay the fee. Coursera
  <a href="https://learner.coursera.help/hc/en-us/articles/209819033-Apply-for-Financial-Aid">
   Financial Aid
  </a>
  is available to offset the registration cost for learners with demonstrated economic needs. If you have questions about Course Certificates,
  <a href="https://learner.coursera.help/hc/en-us/articles/208280196-Course-Certificates">
   please see the help topics here
  </a>
  .
 </p>
 <p>
  Also note that this course is in the
  <a href="https://www.coursera.org/specialization/datamining/20">
   Data Mining Specialization
  </a>
  offered by the University of Illinois at Urbana-Champaign. By earning a Course Certificate in this course, you are on your way toward earning a
  <a href="https://www.coursera.org/specializations/datamining">
   Specialization Certificate in Data Mining
  </a>
  . You may also choose to pre-pay for the entire Specialization, at a discount. See more information about
  <a href="https://learner.coursera.help/hc/en-us/articles/208280146-Pay-for-a-course-or-Specialization">
   Specialization payments
  </a>
  here.
 </p>
 <p>
  <strong>
   If you choose not to pay the fee
  </strong>
  , you can still audit the course. You will still be able to view all videos, submit practice quizzes, and view required assessments. Auditing does not include the option to submit required assessments. As such, you will not be able to earn a grade or a Course Certificate.
 </p>
 <h2 level="2">
  Getting and Giving Help
 </h2>
 <p>
  You can get/give help via the following means:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Use the
    <strong>
     <a href="https://courserahelp.zendesk.com/hc/en-us/">
      Learner Help Center
     </a>
    </strong>
    to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the
    <strong>
     Contact Us!
    </strong>
    link available on each topic's page within the Learner Help Center.
   </p>
  </li>
  <li>
   <p>
    Use the
    <strong>
     <a href="https://www.coursera.org/learn/text-mining/discussions/forums/5mcKtywqEeaaVA48G_0dEQ">
      Content Issues
     </a>
    </strong>
    forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and Community Mentors will monitor this forum and respond to issues.
   </p>
  </li>
 </ul>
 <p>
  Note: Due to the large number of learners enrolled in this course, I am not able to answer emails sent directly to my account. Rather, all questions should be reported as described above.
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
