这一讲继续讨论 文本分类的判别分类器 这一讲里
我们要介绍 另一个判别分类器
叫做支持向量机（SVM） 这种分类方法十分流行 在文本分类上也很有效 为了介绍这一分类方法 我们考虑存在两大类的简单情况 我们有两个话题类
这里的01和02 我们要将文档
分入这两类 然后通过一个特征因子x来表示一篇文档 现在这个分类器的想法
是构造一个线性分类器 像你看到的这样 这与回归十分相似，是吧？ 然后我们可以说
如果函数值为正 我们就说对象归入分类1 否则我们就说它属于分类2 这样就使得0成为了几类之间的决策边界 一般来讲隐藏的边缘空间
像是0 对应一个超平面 现在我们展示一个二维空间下的简单情况
只有X1和X2 这可以对应你看到的这条线 所以这是一条由 三个参数决定的线
贝塔0、贝塔1和贝塔2 现在这条线指向这个方向 如果我们增加X1
X2也会增加 那么我们知道贝塔1和贝塔2有不同的符号
一个为负[英文原文有误] 另一个为正 我们假设贝塔1为负
贝塔2为正 现在我们可以检验 两侧的数据实例 这里数据用圆表示一类 钻石表示另一类 那么问题就是选择像这样的一个点
然后考察 这个数据点上表达式
或者说分类器的值是什么 那么你怎么认为？ 一般我们可以利用这个函数来估计 像之前说的如果值为正
我们就将其归入分类1 如果为负就归入分类2 直观上讲这条线分开了两个类别
我们希望 一侧的点值为正
另一侧为负 我们的问题建立在
我刚提到的假设之上 我们来检验一个点
像这个 你认为这个表达式的值是什么呢？ 通过看这个表达式的值我们可以检验这个符号 我们可以将 线上的值与这个点的值相比较 它们有相同的X1
但是一个有更高的X2值 我们现在来看X2的系数符号 我们看这里是正的 也就是说
该点的函数值 比线上点的函数值要高 所以就是正的对吧？ 那么我们也就大概知道
所有这一侧的点 函数值为正 你也可以确定这一侧的所有点
函数值为负 这种线性分类器 或者说线性分离方法
就是这样将点分为两类的 那么自然就要问
哪个线性分类器是最好的呢？ 我这里划了一条线
能够分出这两类 这条线当然是由
系数贝塔来决定 不同的系数
对应不同的线 那么我们可以想象
也会有其他线可以达到相同的效果 比如说伽马 可以定义一条新的线
用来分离数据 当然也有一些线无法起到分离作用
它们就不是好的选择 但是问题是
如果我们有多条线 都可以分开两类
哪一个最好呢？ 实际上你可以想象
选择这条线可以用许多不同的方法 那么逻辑回归分类器
就是你之前看到的 通过一些标准判断这条线的位置
用来作为线性分类器 利用对训练集的条件概率 决定哪条线最好 但是在支持向量机中我们会用另一个标准 来决定哪条线最好 这一次 你会看到这个标准
与分类箭头联系更紧密 基本想法就是选择
最大化空白 什么是空白呢？ 我这里选了一些虚线
用来表示 每类数据点的界限 空白就是这些线、 分类器和各类最近点的距离 你可以看到这里的空白
我展示的这里 你也可以定义另一侧的空白 寻找最大化空白的分类器 应该是在两个边界之间 你不想分类器与一边过于接近 这样更符合直觉 这就是支持向量机的基本思想 我们要选择最大化空白的一个线性分类器 这里我也调整了定义 这样我不再用贝塔代指这些参数 这里我改用w
虽然之前单词也可以用w 这里不要混淆了 w这里表示的是宽度
一定的宽度 那么我这里用小写b
表示有偏的常数贝塔0 这里数据用x表示 我们会用向量乘法来计算 这里我们将向量w转置
然后与特征向量相乘[英文可能有误] b是一个有偏的常数
w是各个特征的权重 我们有m个特征
就有m个权重 由向量表示 类似地这里的数据
文本对象 由相同数量元素组成的特征向量表示 Xi是一个特征值 比如词频
你可以确认 当我们将两个向量相乘的时候
点乘 我们可以得到与之前看到一类的线性分类器 这只是个不同的表示方式 我现在有一个与定义更为一致的方式 这个定义在人们讲到支持向量机的时候通常会用 这个方法可以更好地将幻灯片与其他阅读材料结合起来 好那么当我们最大化分类器产生的空白的时候 意味着分类器的边界只由 一些数据点决定
这些数据点我们叫做支持向量 这里展示了两个支持向量
一表示其中一类 二表示另一类 这些点大致定义了空白区域 你可以想象哪些是支持向量 那么中间的这条分类线会由它们决定 那么其他的数据并没有那么大的影响 你看如果你改变其他的数据点
并不会影响到空白 分类器会维持原状 主要受支持向量机的影响 抱歉
是主要受支持向量的影响 所以它被叫做支持向量机 好下一个问题当然就是 我们如何优化这条线？ 我们如何找到这条线
或者是分类器？ 这与寻找w和b的值一样 因为他们是决定分类器的参数 在最简单的情形下
线性支持向量机是一个简单的最优化问题 回想一下我们的分类其实一个线性分类器 我们有每个特征的权重
主要目标是消除权重w和b 分类器会判断X属于类theta1
如果其为正 否则我们就说它属于另一类 这就是我们的假设 所以在线性支持向量机中
我们要寻找这些参数值 优化空白以及训练偏误 训练数据基本上与其他分类器一样 我们有一个训练集
我们知道其中X向量的值 我们也知道对应的标签y_i 这里我们定义y_i只有两个值 它们不是你之前看到的0和1
而是-1和1 对应到这里的两个类别 现在你可能在想为什么我们不用0和1 而是-1和1 这只是为了数学上的便利
一会你就会看到 那么最优化过程的第一步 就是确保训练集的标签都要做对 也就是说如果x_i的标签y_i是1 我们希望这个分类值要大 这里我们就选1作为阈值 但如果你用其他阈值
你可以将这个常数 调整到参数b和w当中
使得右侧仍只留下1 另一方面如果y是-1
意味着是在另一类里 那么我们就希望分类值足够小 对于一个负数来说我们就希望它
小于或等于-1 现在我们有了两个不同的例子
不同的类别 我们如何将它们结合起来呢？ 这就是我们用-1表示 另一类的方便之处了 因为这里我们可以将两个限制条件合为一个 y_i乘以分类值必须大于或等于1 如果y_i等于1 你可以看到跟左边的限制条件是一样的 如果y_i为-1
你可以看到也是一样 也就是说这个式子实际上将两个限制统一了起来 这是限制条件的一种便捷的表示方式 第二个目标是什么？ 我们要最大化空白 我们希望确保分类器在训练数据中表现良好 但之后在能够区分数据的所有情况当中 我们还希望找到拥有最大空白的分类器 这里空白可以假设是与权重的大小有关的 那么我们将w转置乘以w 就可以得到所有权重的平方和 那么要在这个表达式中得到较小的值 我们就需要所有wi都要小 刚才我们假设
我们有一个约束条件 那就是对训练集中的数据要分类正确 现在我们有了一个新的目标
就是与最大化空白相关 简单点说就是最小化w转置乘以w 我们通常将其命名为Φ(w) 你可以看到
这基本上就是一个最优化问题 我们有一些需要优化的变量
就是权重w和b 我们也有一些约束条件 它们是线性的约束条件 目标函数是一个含有权重的多项式函数 这就是个含有线性约束的多项式程序 解决这一问题有标准的算法 解决了问题
我们就可以得到权重w和b 我们就能得到一个明确的分类器 我们可以利用这个分类器
判别任何新出现的文本对象 之前的判别式不容许任何错分 但是有时候数据对于分类器来说可能不是线性的 也就是说结果可能不会像你看到的那么好 之前你看到一条线恰好能够区分所有的点 如果我们容许一些错分
结果会怎么样呢？ 原理上是一样的 我们想最小化偏误
同时最大化空白 但这种情况下
我们遇到的是一个软边界 因为数据点可能不是完全可分的 我们可以简单修改
支持向量机的设定来适应这一点 这里你看到的和之前类似 但是我们将引入额外的变量ξi 我们将其对应到每一个数据实例 它将会记录每个实例容许的错分 最优化问题是非常相似的 特别地 你能看到我们在最优化问题中加了些东西 首先我们在约束条件中加入了一些偏误 现在我们允许分类器 犯一些错误 ξi表示允许的偏误 如果ξi定为0
那么就与原先的约束条件一致 我们希望每个数据都能被准确分类 如果我们允许它不等于0，那么我们就允许一些偏误了 事实上如果ξi非常大
偏误也会变得非常大 所以很自然
我们不希望这种情况发生 之后我们希望最小化ξi 因为ξi最小化可以控制偏误的程度 因此目标函数中 我们也在只有W的原有情况下加入了ξi 确保我们不仅会最小化权重 还会最小化偏误
就像你看到的这样 这里我们对所有数据做一个简单的求和 每一项都有ξi来记录它的偏误 我们把它们结合起来 我们希望最小化它们所有的偏误 你可以看到这里有个参数C
这是一个常数 用来权衡最小化偏误与最大化空白 如果C为0你可以看到 我们就退回了原有目标函数
在那里我们只进行了空白的最大化 我们并没有最优化训练偏误 ξi就可以为一个很大的值
满足约束条件 这当然并不好 C应该是一个非零正值 但是如果C非常大 我们可以看到目标函数会主要由训练偏误决定 最大化空白就会起次要作用了 如果这样的话 我们在最小化训练偏误上能够做到最好 但是我们不会管空白的情况 这会影响到对未来数据区分因子的概括 这是不好的 所以C在设定上尤其要小心 这跟k聚类的情况一样
你需要 最优化邻居的数量 这里你需要对C取最优 这通常是通过交叉验证实现的 通常你观察实证数据 来决定C的值
最优化分类器的表现 在修改之后 问题依然是一个线性约束条件下的多项式规划问题 所以最优算法仍然可以解决这种规划问题 同样如果我们得到了权重和偏误 我们就得到了分类新的对象的分类器 这就是支持向量机的基本思路 来总结一下文本分类的方法 我们引入了许多方法
其中一些是生成模型 有一些是判别方法 它们在优化后的表现是类似的 所以没有最好的方案
每个都有自己的优缺点 效果也会因数据、 问题而异 一个原因是因为特征表示在其中是十分重要的 这些方法都需要有效的特征表示 设计一个有效的特征集 我们需要领域知识
这里人起到了很重要的作用 虽然有许多新的机器学习方法和算法 在特征表示学习上可以起到作用 另一个常见的情况是 它们可能在表现上类似 但是犯了不同的错误 所以表现上可能相似 它们的错误可能是不同的 这也意味着对不同方法
在同一问题下的结果进行比较 会很有用
可能可以组合多种方法 提升结果的稳健性
不会犯相同的错误 所以组合不同的方法的结果 会更稳健
在实际中也更为实用 我们这里介绍的大多数技术属于监督式学习 是一种一般性的方法 也就是说这些方法可以应用在任何文本 或是分类问题当中 只要我们人能够协助
定义一些训练集 设计特征
那么监督式学习和所有这些分类器 可以用于解决这些分类问题 让我们可以将文本内容简化到类别上 或是预测与文本相关的 真实世界变量的整体情况 计算机尝试优化 人类提供的特征组合 就像我说的
组合的方法多种多样 目标函数也不尽相同 但是为了获得好的表现
它们都需要有效的特征 以及大量的训练数据 一般来讲如果你能够优化特征表示 提供更多的训练数据
你就可以做的更好 表现通常受到特征有效性更为明显的影响 而不是对特定分类器的选择 所以特征设计比 选择特定的分类器要更重要 那么如何设计出有效的特征呢？ 不走运的是这个可能与具体应用密切相关 所以没什么一般的规律可循 但是我们可以对分类问题进行分析 尝试理解哪些特征
对于我们进行类别的区分会有帮助 一般来讲我们需要许多领域内知识
来辅助特征设计 另一种寻找有效特征的方法就是 对分类结果进行误差分析 比如你可以 看哪一类更容易与其他类相混淆 你可以用一个混淆矩阵来系统检验 类别间的误差情况 之后你可以根据实际情况 找到是犯了哪种错误
哪个特征可以防止错误的发生 这就可以为你的特征设计提供新的思路 所以误差分析通常是很重要的 也是针对你的特定问题提供思路的地方 最后我们可以在机器学习技术充分利用这一点 比如特征选择这个技术我们并没有讲 但是它很重要 它可以选出最有用的特征 在你训练出一个完整的分类器之前 有时训练一个分类器可以帮助你识别哪些特征有较高的 数值。 还有其他方法来确保这个空隙 在模型中指的是识别空白的宽度 比如支持向量机最小化特征权重 你可以限制一些特征 强制只使用一部分特征 还有降维的技术 可以讲一个高维的特征空间转化为一个低维的空间 这是通过对特征进行各式各样的聚类实现的 度量因子会在这里用到 这些与我们之前提到的模型很相似 比如话题模型 比如概率性潜在语义分析或是 潜在狄里克雷分配可以降低特征的维数 想象单词是原有的特征表示 但这种表示可以对应到话题空间当中
假设我们有k个话题 那么一篇文档可以表示为 由话题对应的k个值组成的向量 我们可以用每个话题表示一个维度
我们就得到了k维空间 而不是对应到单词的高维空间 这就是另一种学习有效特征的方法 我们可以利用这些类别来监督 如此低维的结构 原有单词层面的特征也可以结合到 这些维度特征或是低维空间特征当中 提供多种方案
这通常很有用 深度学习是机器学习中一个新开发的技术 学习表征上它尤其有效 深度学习指的是深度神经网络
是另外一种分类器 你可以在模型中嵌入中间特征 这是一个高度非线性的分类器 一些新近的研究让我们
可以有效训练这种复杂网络 这种方法在语音识别和计算机推理中十分有效 最近也被应用到文本上 它已显示出了一些希望 这个方法一个主要的优点是 在特征设计的关系上 它可以学习中间的表征或是自动混合特征 这在学习表征中十分有价值 对于进行文本重新校准来说 虽然在文本域
单词是文本内容的典型表征 因为它们是人类交流使用的成像 它们也足以表征许多任务 如果需要新的表征 人们会发明一个新词 所以我们认为深度学习对于 文本处理的价值会比计算机推理以及语音识别中要低 因为存在其他对应的信息 可以用在特征设计当中 但是人们依然对有效特征的学习充满希望 尤其是在复杂任务当中 比如一个分析之所以有效 是因为他可以提供单词之外的信息 从训练示例来看 一般很难获得大量的训练例子
因为它涉及 人工劳动 但是我们还是有一些办法 一种是假设一些低质量的训练示例也可以利用起来 这些可以叫做伪训练示例 比如如果你在网上发评论
可能会有综合评价 那么训练一个分类器
意味着我们想要正或负值 我们把评论分为两类 我们可以假设所有五星评价都是正面的训练示例 一星是负面的 当然了 当然有时候五星也可以提到负面评价 训练样本的质量不高
但是它们仍然有用 另一个想法是利用未标注的数据 有一类技术叫做半监督机器学习技术 可以让你将已标注的数据与未标注的结合起来 在我们这里很容易看到
这种混合模型可以 用于文本聚类和文本分类 你可以想象如果你有许多
未标记的文本数据用来进行分类 你可以对文本数据进行聚类
学习其中类别 然后尝试以某种方式列出这些类别 通过训练数据定义的类别 我们已经知道哪个文档属于哪一类别 因此你可以利用算法来组合它们 它从本质上可以选取有用的单词并标注它们 你可以从另一个方向来想 一般我们 对没有标注的文本进行分类 我们会假设具有高置信度的分类结果是可信的 之后如果你有了更多的训练数据 你可以知道一些可以标为分类1
一些是分类2 虽然标注不是完全可信的 但是它们仍然很有用 假设它们是标注训练实例 我们可以将其与真实训练示例结合
优化分类方法 这个想法是很有用的 如果启用数据与训练数据不同 我们可能需要使用其他高级的机器学习方法 比如域自适应学习或迁移学习 这样我们就能 借用相关问题的一些不同的训练集 或者利用其他的分类任务 这些任务与现有任务存在着不同的数据分布 不过通常使用两个不同域的方法时 我们应该很谨慎
不要对训练域进行过度你和 但是我们仍希望利用相关训练数据的一些特征 比如对新闻的训练分类 可能并不能为Twitter分析提供有效的分类器 但是你仍可以从中学到一些
研究tweets的方法 因此机器学习可以有效地协助你解决这一问题 这里是推荐的文献
你可以从中得到更多细节 是关于我们讲过的方法的 [背景音乐]
翻译: MingShi |审阅:
Coursera Global Translator Community