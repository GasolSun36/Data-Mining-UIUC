1
00:00:00,366 --> 00:00:03,025
[声音]
翻译: PeiyS |审阅: 
Coursera Global Translator Community

2
00:00:08,261 --> 00:00:11,100
我们可以计算这个最大估计值

3
00:00:11,100 --> 00:00:12,823
通过使用最大期望算法

4
00:00:12,823 --> 00:00:16,544
所以在E-Step中，我们现在需要介绍更多的隐藏变量

5
00:00:16,544 --> 00:00:21,536
因为我们有更多的主题，所以现在我们的隐藏变量z

6
00:00:21,536 --> 00:00:25,690
是一个可取超值过两个的主题指标

7
00:00:25,690 --> 00:00:28,888
亦即我们将取k+1个值

8
00:00:28,888 --> 00:00:32,200
用b表示背景

9
00:00:32,200 --> 00:00:35,527
一旦锁定，即用来代表所有的k个主题

10
00:00:36,750 --> 00:00:40,740
所以，正如你所回忆的，现在E-Step是在增强数据

11
00:00:40,740 --> 00:00:44,640
通过预测隐藏变量的值

12
00:00:44,640 --> 00:00:47,700
所以我们将要来对一个单词进行预测

13
00:00:47,700 --> 00:00:52,990
看这个单词是否来自这些k+1个分布之一

14
00:00:52,990 --> 00:00:57,020
这个公式使我们可以预测

15
00:00:57,020 --> 00:01:01,540
文档d中的单词w产生于主题z中的j的概率

16
00:01:03,010 --> 00:01:06,050
下面的这个是预测概率

17
00:01:06,050 --> 00:01:08,740
关于这个单词产生于这个背景

18
00:01:08,740 --> 00:01:14,190
请注意我们在这里使用文档d来为单词做索引

19
00:01:14,190 --> 00:01:14,990
为什么呢？

20
00:01:14,990 --> 00:01:18,860
因为不管一个单词是否来自于一个特定的主题

21
00:01:18,860 --> 00:01:20,790
实际上它都依赖于一个文档

22
00:01:20,790 --> 00:01:22,210
你知道为什么吗？

23
00:01:22,210 --> 00:01:24,360
对，这是通过π的

24
00:01:24,360 --> 00:01:26,870
这些π与每个文档相关联

25
00:01:26,870 --> 00:01:31,020
每个文档都可以有潜在的不同的π值

26
00:01:31,020 --> 00:01:33,670
然后这些π值将会影响我们的预测

27
00:01:33,670 --> 00:01:35,220
所以这些π值在这里

28
00:01:35,220 --> 00:01:36,740
而这随文档而定

29
00:01:38,510 --> 00:01:41,100
并且这可能给出了不同的猜想

30
00:01:41,100 --> 00:01:44,880
关于在不同文件中的一个单词， 而这也是理想的

31
00:01:46,320 --> 00:01:50,490
在这两种情况下我们使用的都是贝叶斯法则正如我之前所解释的

32
00:01:50,490 --> 00:01:55,300
基本地评估从每个分组中产生单词的可能性

33
00:01:55,300 --> 00:01:56,100
这是标准化的

34
00:01:57,800 --> 00:01:59,130
那么m-step是怎么样的呢

35
00:01:59,130 --> 00:02:04,420
我们或许还记得m-step是我们利用推断z值

36
00:02:04,420 --> 00:02:05,630
去分裂计数

37
00:02:05,630 --> 00:02:09,920
然后收集正确的计数来重新估计参数

38
00:02:09,920 --> 00:02:14,490
所以在这种情况下我们可以重新估计我们的概率的范围

39
00:02:14,490 --> 00:02:21,540
而且这是以收集文件中所有的单词为基础再重新估算

40
00:02:22,650 --> 00:02:26,800
这就是为什么我们要在文档中统计单词的数量

41
00:02:26,800 --> 00:02:29,130
和对所有单词求和

42
00:02:29,130 --> 00:02:33,330
这个

43
00:02:34,340 --> 00:02:36,710
比如主题θ的主体j

44
00:02:36,710 --> 00:02:39,210
而这部分是我们从每一步的猜测

45
00:02:40,500 --> 00:02:44,930
这可以告诉我们这个单词确实来自于主题θ的主体j的盖里

46
00:02:44,930 --> 00:02:47,372
而且当我们将它们乘在一起

47
00:02:47,372 --> 00:02:51,801
我们就得到了属于主题θ主体j的折扣计数

48
00:02:51,801 --> 00:02:54,567
而当我们将所有的主题标准化

49
00:02:54,567 --> 00:02:58,524
我们可以得到所有主题的分布来表示概率范围

50
00:02:58,524 --> 00:03:04,619
同样地底部这个是一个主题内的一个单词的估计概率

51
00:03:04,619 --> 00:03:09,635
而且在这种情况下我们使用精确相同的计数，你可以看到这是

52
00:03:09,635 --> 00:03:14,651
相同的折扣计数，这告诉我们

53
00:03:14,651 --> 00:03:19,765
应该怎样在上面的主体z中分配这个单词，但是这个标准化程序是不同的

54
00:03:19,765 --> 00:03:24,325
因为在这种情况下我们感兴趣的是单词分布所以

55
00:03:24,325 --> 00:03:27,365
我们只是简单的将所有单词标准化

56
00:03:27,365 --> 00:03:33,202
这是不同的，作为对照我们标准化所有主题

57
00:03:33,202 --> 00:03:35,670
取两者进行对比应该是有益的

58
00:03:37,420 --> 00:03:39,568
这为我们提供了不同的分布情况

59
00:03:39,568 --> 00:03:46,142
而且这告诉我们应该如何改进参数

60
00:03:48,279 --> 00:03:55,275
正如我之前解释过的，在这两个公式中我们都有一个最大的

61
00:03:55,275 --> 00:04:00,534
估计值是以主题θ的主体j的分配单词计数为基础

62
00:04:00,534 --> 00:04:04,882
现在这个现象实际上在所有的最大期望算法中是很常见的

63
00:04:04,882 --> 00:04:09,909
在m步骤中你

64
00:04:09,909 --> 00:04:15,025
基于E步骤的结果的事件，然后你只需要收集相关的计数

65
00:04:15,025 --> 00:04:20,270
关于特定的参数并进行具有代表性的估算和标准化

66
00:04:20,270 --> 00:04:24,965
π

67
00:04:24,965 --> 00:04:32,290
实际上只要持续解释各种事件然后对他们进行标准化

68
00:04:32,290 --> 00:04:34,163
并且当我们这样想的时候

69
00:04:34,163 --> 00:04:37,993
我们对呈现最大期望算法也会有一个更加简明的方法

70
00:04:37,993 --> 00:04:42,440
它实际上帮助我们更好的理解这些公式

71
00:04:42,440 --> 00:04:44,890
所以我将对这方面进行更细节的复习

72
00:04:44,890 --> 00:04:48,692
因此作为一个运算法则，我们首先随机地初始化所有的未知参数

73
00:04:48,692 --> 00:04:50,200
好的

74
00:04:50,200 --> 00:04:55,000
所以对我们来说我们感兴趣的是那些范围参数和π

75
00:04:55,000 --> 00:04:59,830
还有单词分布等等，而我们只是随机的对他们进行标准化

76
00:04:59,830 --> 00:05:05,830
这是初始化步骤然后我们重复进行直到概率收敛

77
00:05:05,830 --> 00:05:08,390
现在我们怎么知道概率是否会收敛呢

78
00:05:08,390 --> 00:05:11,740
我们可以在每一步骤的时候进行可能性计算

79
00:05:11,740 --> 00:05:14,960
并且将现在的可能性和之前的可能性进行比较

80
00:05:14,960 --> 00:05:17,227
如果不发生很大的变化那么我们将要停止

81
00:05:19,520 --> 00:05:23,392
所以在每一步中我们将要进行E步骤和M步骤

82
00:05:23,392 --> 00:05:27,715
在E步骤中我们将要增强数据

83
00:05:27,715 --> 00:05:30,310
通过预测隐藏变量

84
00:05:30,310 --> 00:05:34,400
在这种情况下隐藏变量z和小变量d以及w

85
00:05:34,400 --> 00:05:41,030
表示子集d中的单词w是否来自于一个主题或者背景

86
00:05:41,030 --> 00:05:43,509
并且如果他是来自于某一个主题具体是哪个主题

87
00:05:43,509 --> 00:05:46,767
所以如果你们看一下E步骤的公式

88
00:05:46,767 --> 00:05:52,302
本质上我们是在对这些计数进行标准化

89
00:05:52,302 --> 00:05:58,820
基于这样一个分布观察到某个词的概率

90
00:05:58,820 --> 00:06:03,250
所以你可以看到基本上关于单词的预测

91
00:06:03,250 --> 00:06:07,650
来自主题z子集j是基于

92
00:06:07,650 --> 00:06:12,030
选择主题θ子集j作为单词分布来产生单词的可能性

93
00:06:12,030 --> 00:06:15,990
乘以在那个分布里观察到单词的概率

94
00:06:17,030 --> 00:06:22,030
并且我讲过这是成比例的因为在最大期望算法的应用中

95
00:06:22,030 --> 00:06:25,820
你可以持续记述这个数量的计数

96
00:06:25,820 --> 00:06:28,830
最后你只需要对其进行标准化

97
00:06:28,830 --> 00:06:32,080
所以这里的标准化过程是针对所有主题的

98
00:06:32,080 --> 00:06:34,310
然后你将会得到一个概率

99
00:06:36,410 --> 00:06:41,682
现在在M步骤中我们做一样的事情，我们将要收集这些

100
00:06:43,980 --> 00:06:46,300
并分配到每一个主题

101
00:06:47,770 --> 00:06:49,690
并且我们在主题之中分离单词

102
00:06:50,970 --> 00:06:53,740
然后我们将会对它们进行不同方式的标准化过程

103
00:06:53,740 --> 00:06:54,890
来获得真实的估计址

104
00:06:54,890 --> 00:07:00,680
因此举个例子我们可以对这之中所有的主题进行标准化来得到π的重新估计值

105
00:07:00,680 --> 00:07:02,040
覆盖范围

106
00:07:02,040 --> 00:07:08,230
或者我们可以对所有的单词进行重新标准化

107
00:07:08,230 --> 00:07:09,739
这将给予我们一个单词分布

108
00:07:10,960 --> 00:07:15,860
所以以这种方式来思考算法是很有用的因为当我们应用它时

109
00:07:15,860 --> 00:07:22,420
你可以仅仅是使用这些变量但是在每一种情况下持续追踪这些数量

110
00:07:23,800 --> 00:07:31,210
然后你只需要对这些标量进行表昏花来是他们呈现分布

111
00:07:32,210 --> 00:07:35,340
现在我没有给这个加约束条件

112
00:07:35,340 --> 00:07:38,550
因为我有意留下这个作为给你们的一个练习

113
00:07:38,550 --> 00:07:42,218
你可以看到这个式子的正规化子是什么

114
00:07:42,218 --> 00:07:47,430
这是一个稍有不同的公式但是本质上

115
00:07:47,430 --> 00:07:50,940
与你在这里看到的这个是一样的

116
00:07:50,940 --> 00:07:54,710
所以一般来说在最大期望算法的构想中你会看到你积累计数

117
00:07:54,710 --> 00:07:59,420
各种计数然后你对他们进行标准化

118
00:08:01,660 --> 00:08:06,752
所以作为总结我们介绍了概率潜在语义分析模型

119
00:08:06,752 --> 00:08:10,650
这是一个代表k个主题的k个一元语言模型的混合模型

120
00:08:11,830 --> 00:08:16,850
而且我们还增加了一个预先确定的背景语言模型

121
00:08:16,850 --> 00:08:19,360
来辅助发现差别主题

122
00:08:19,360 --> 00:08:22,370
因为这种背景语言模型可以辅助吸引常用术语

123
00:08:23,800 --> 00:08:28,589
而且我们选择最大估计值

124
00:08:28,589 --> 00:08:30,403
使我们可以从文本数据中发现主题知识

125
00:08:30,403 --> 00:08:35,304
在这种情况下PLSA可以使我们发现两件事，一是k个单词分布

126
00:08:35,304 --> 00:08:37,265
每一个分布代表一个主题

127
00:08:37,265 --> 00:08:40,779
另一个是在每一个文档中每一个主题的一部分

128
00:08:41,990 --> 00:08:46,510
并且，文档中关于主题覆盖性的这样详细的描述特性

129
00:08:46,510 --> 00:08:48,890
可以启用大量的图片分析

130
00:08:48,890 --> 00:08:53,970
比如，我们可以聚集一个特定的时间范围内的文档

131
00:08:53,970 --> 00:08:58,800
来评估一定时间范围内一个特定主题的覆盖率。

132
00:08:58,800 --> 00:09:02,540
这样可以让我们生成主题的时间链

133
00:09:02,540 --> 00:09:08,543
我们也可以聚集文件里涉及的与一个特定作者相关的所有主题

134
00:09:08,543 --> 00:09:14,198
然后我们可以将这位作者所写的主题归类，等等

135
00:09:14,198 --> 00:09:20,190
并且除了这一点，我们也可以聚类术语和集群文件

136
00:09:20,190 --> 00:09:23,230
实际上，每一个主题都可以被视为一个集群

137
00:09:23,230 --> 00:09:25,840
所以我们已经有了术语集群

138
00:09:25,840 --> 00:09:28,240
在较高的概率下，这些单词可以被认为是

139
00:09:29,630 --> 00:09:34,560
属于同一个由主题代表的集群

140
00:09:34,560 --> 00:09:37,060
相似地，文档也可以以同样的方法被聚类

141
00:09:37,060 --> 00:09:41,948
我们可以指派一篇文档给一个特定的主题集群

142
00:09:41,948 --> 00:09:45,944
在文档中涵盖的最多的这个主题集群

143
00:09:45,944 --> 00:09:50,610
所以请记住，π表明每个主题在文档中的被包含程度

144
00:09:50,610 --> 00:09:55,510
我们可以分配文档给有最高的π值的主题集群

145
00:09:57,340 --> 00:10:00,975
而且通常这项技术有许多有效的实际应用

146
00:10:03,146 --> 00:10:13,146
[背景音乐]
翻译: MingShi |审阅: 
Coursera Global Translator Community