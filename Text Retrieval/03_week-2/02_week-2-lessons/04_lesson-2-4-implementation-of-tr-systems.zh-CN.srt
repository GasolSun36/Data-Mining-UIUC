1
00:00:00,248 --> 00:00:06,368
她们都有自己的孩子和孙子

2
00:00:06,368 --> 00:00:10,308
本讲座是关于文本检索系统的实现。

3
00:00:12,878 --> 00:00:17,327
在本讲座中, 我们将讨论如何实现文本

4
00:00:17,327 --> 00:00:20,768
检索方法来构建搜索引擎。

5
00:00:20,768 --> 00:00:24,753
主要的挑战是管理大量的文本数据和

6
00:00:24,753 --> 00:00:30,698
使查询能够非常快速地得到答复, 并响应许多查询。

7
00:00:30,698 --> 00:00:34,858
这是一个典型的文本检索系统体系结构。

8
00:00:34,858 --> 00:00:39,805
我们可以看到文档首先由分词器处理

9
00:00:39,805 --> 00:00:43,498
获取标记单位, 例如单词。

10
00:00:43,498 --> 00:00:48,058
然后, 这些单词, 或令牌, 将由

11
00:00:48,058 --> 00:00:53,188
将创建索引的索引器, 该索引是

12
00:00:53,188 --> 00:00:57,280
用于快速回答查询的搜索引擎。

13
00:00:57,280 --> 00:01:01,830
而查询将经历类似的处理步骤。

14
00:01:01,830 --> 00:01:05,761
因此, 令牌人也会被告知查询,

15
00:01:05,761 --> 00:01:09,200
以便以相同的方式处理文本。

16
00:01:09,200 --> 00:01:12,960
同样的词将相互匹配。

17
00:01:12,960 --> 00:01:17,604
然后, 查询的表示将被提供给 Scorer,

18
00:01:17,604 --> 00:01:22,506
这将使用索引快速回答用户的查询通过

19
00:01:22,506 --> 00:01:25,268
给文档打分, 然后对其进行排名。

20
00:01:25,268 --> 00:01:27,628
结果将提供给用户。

21
00:01:27,628 --> 00:01:32,033
然后用户可以查看结果, 并向我们提供一些反馈, 可以

22
00:01:32,033 --> 00:01:35,126
明确判断哪些文件是好的,

23
00:01:35,126 --> 00:01:36,603
哪些文件是坏的。

24
00:01:36,603 --> 00:01:43,353
或隐式反馈, 如这样的用户不必做任何额外的。

25
00:01:43,353 --> 00:01:46,187
最终用户只需查看结果,

26
00:01:46,187 --> 00:01:49,193
跳过一些, 并点击一些结果查看。

27
00:01:49,193 --> 00:01:55,353
因此, 这些相互作用的信号可以被系统用来提高排名

28
00:01:55,353 --> 00:02:01,718
准确性, 前提是查看的文档比跳过的文档更好。

29
00:02:01,718 --> 00:02:05,678
因此, 搜索引擎系统可以分为三个部分。

30
00:02:05,678 --> 00:02:10,738
第一部分是indexer 第二部分是Scorer

31
00:02:10,738 --> 00:02:16,458
响应用户查询, 第三部分是反馈机制。

32
00:02:16,458 --> 00:02:21,072
现在通常情况下, 索引器是以脱机方式完成的, 因此

33
00:02:21,072 --> 00:02:24,179
您可以预先处理正确的数据和

34
00:02:24,179 --> 00:02:29,168
建立库存指数, 我们将在一会儿介绍

35
00:02:29,168 --> 00:02:34,819
然后, 这个数据结构可以被一个得分手的在线模块使用 "

36
00:02:34,819 --> 00:02:40,668
以动态处理用户的查询并快速生成搜索结果。

37
00:02:40,668 --> 00:02:45,368
反馈机制可以在线或离线完成, 具体取决于方法。

38
00:02:45,368 --> 00:02:50,367
索引器和得分手的实现是非常标准的,

39
00:02:50,367 --> 00:02:55,378
这是本次讲座和接下来的几首讲座的主要主题。

40
00:02:55,378 --> 00:02:59,843
另一方面, 反馈机制有很多变化

41
00:02:59,843 --> 00:03:02,378
这取决于使用哪种方法。

42
00:03:02,378 --> 00:03:08,818
因此, 这通常是以特定的算法方式完成的。

43
00:03:08,818 --> 00:03:11,538
让我们先谈谈分词器

44
00:03:11,538 --> 00:03:16,578
分词是通过相同形式的规范化词汇单位,

45
00:03:16,578 --> 00:03:21,368
这样语义上相似的单词就可以相互匹配。

46
00:03:21,368 --> 00:03:25,133
现在, 在像英语这样的语言, 词干经常被使用和

47
00:03:25,133 --> 00:03:29,548
这将把所有的屈折形式的单词映射到相同的根形式。

48
00:03:29,548 --> 00:03:33,047
因此, 例如, 计算机, 计算, 和

49
00:03:33,047 --> 00:03:37,078
计算都可以与词根计算匹配。

50
00:03:37,078 --> 00:03:43,628
这样, 所有这些不同形式的计算都可以相互匹配。

51
00:03:43,628 --> 00:03:46,433
现在通常情况下, 这是个好主意,

52
00:03:46,433 --> 00:03:52,337
以增加与此查询匹配的文档的覆盖范围。

53
00:03:52,337 --> 00:03:55,553
但这也并不总是有效的

54
00:03:55,553 --> 00:04:00,914
因为有时计算机和计算机之间有微妙的区别

55
00:04:00,914 --> 00:04:07,558
计算可能仍然表明在内容的覆盖面的差异。

56
00:04:07,558 --> 00:04:13,398
但在大多数情况下, 词干似乎是有效的

57
00:04:13,398 --> 00:04:19,363
当我们用其他语言 (例如中文) 标记文本时, 我们可能会

58
00:04:19,363 --> 00:04:25,338
在分割文本以查找单词边界时面临一些特殊的挑战。

59
00:04:25,338 --> 00:04:29,697
因为它是不明显的边界是

60
00:04:29,697 --> 00:04:32,928
没有空间把他们分开

61
00:04:32,928 --> 00:04:41,638
因此, 在这里, 我们当然必须使用一些特定于语言的处理技术。

62
00:04:41,638 --> 00:04:47,144
一旦我们做好分词, 那么我们会索引的文本文档, 比它将

63
00:04:47,144 --> 00:04:52,748
转换文档并执行一些数据结构, 以实现更快的搜索。

64
00:04:52,748 --> 00:04:58,298
基本的想法是尽可能地预先计算我们所能做到的。

65
00:04:58,298 --> 00:05:02,848
因此, 最常用的索引是调用倒置索引。

66
00:05:02,848 --> 00:05:06,555
这已经在许多搜索引擎中使用过

67
00:05:06,555 --> 00:05:09,768
以支持基本的搜索算法。

68
00:05:09,768 --> 00:05:13,426
有时其他索引, 例如,

69
00:05:13,426 --> 00:05:19,498
可能需要文档索引来支持反馈, 就像我说的。

70
00:05:19,498 --> 00:05:24,106
而这种技术并不是真正的标准

71
00:05:24,106 --> 00:05:28,828
他们根据反馈方法的不同而变化很大。

72
00:05:28,828 --> 00:05:34,549
要理解为什么我们要使用倒置索引, 它将是有用的

73
00:05:34,549 --> 00:05:40,698
您可以考虑如何快速响应单个术语查询。

74
00:05:40,698 --> 00:05:44,938
所以, 如果你想用更多的时间来考虑这个问题, 暂停视频。

75
00:05:44,938 --> 00:05:49,584
所以, 想想如何可以预先处理的文本数据, 使得

76
00:05:49,584 --> 00:05:54,768
你可以快速响应只有一个字的查询

77
00:05:54,768 --> 00:05:58,466
如果你想过这个问题

78
00:05:58,466 --> 00:06:02,811
你可能会意识到, 最好的地方是简单地创建

79
00:06:02,811 --> 00:06:07,718
与词汇中的每个术语匹配的文档列表。

80
00:06:07,718 --> 00:06:11,788
这样, 您基本上可以预先构造答案。

81
00:06:11,788 --> 00:06:17,503
所以, 当你看到一个术语, 你可以简单地获取随机列表的文件

82
00:06:17,503 --> 00:06:20,508
并将列表返回给用户。

83
00:06:20,508 --> 00:06:24,928
所以这是在这里对一个术语做出反应的最快方式。

84
00:06:24,928 --> 00:06:30,468
现在反转索引的概念实际上是这样的。

85
00:06:30,468 --> 00:06:36,017
我们要做一个预先构建的搜索索引, 这将允许

86
00:06:36,017 --> 00:06:41,388
我们可以快速找到与特定术语匹配的所有文档。

87
00:06:41,388 --> 00:06:43,878
因此, 让我们来看看这个例子。

88
00:06:43,878 --> 00:06:45,439
我们这里有三份文件

89
00:06:45,439 --> 00:06:49,168
这些都是你在以前的一些讲座中看到的文件。

90
00:06:49,168 --> 00:06:52,916
假设我们要为这些文档创建一个倒置索引。

91
00:06:52,916 --> 00:06:57,502
然后, 我们要维护一个字典, 在字典里, 我们将有一个条目对应一个词

92
00:06:57,502 --> 00:07:01,628
我们将存储一些关于这个术语的基本静态统计数据。

93
00:07:01,628 --> 00:07:05,960
例如, 与术语匹配的文档数, 或

94
00:07:05,960 --> 00:07:09,458
代码的总数或术语的频率,

95
00:07:09,458 --> 00:07:14,148
这意味着我们会记录这个词重复出现的次数。

96
00:07:14,148 --> 00:07:17,415
因此, 例如, 新闻,

97
00:07:17,415 --> 00:07:22,253
这个术语出现在所有三个文档中,

98
00:07:22,253 --> 00:07:26,198
所以文件的数量是三。

99
00:07:26,198 --> 00:07:32,820
你也可能会意识到我们需要这个文件计数, 或文件频率,

100
00:07:32,820 --> 00:07:38,002
用于计算矢量空间模型中使用的一些统计数据。

101
00:07:38,002 --> 00:07:42,422
你可以想想吗

102
00:07:42,422 --> 00:07:49,862
那么, 什么加权启发式就需要这个计数。

103
00:07:49,862 --> 00:07:53,622
好吧, 这就是想法, 对, 反向文档频率。

104
00:07:53,622 --> 00:07:58,300
所以, IDF 是一个术语的属性, 我们可以在这里计算。

105
00:07:58,300 --> 00:08:03,291
因此, 有了这里计算的文档, 就很容易计算出

106
00:08:03,291 --> 00:08:06,556
或者在这个时候, 或者用旧的索引, 或者。

107
00:08:06,556 --> 00:08:10,134
在随机的时间, 当我们看到一个查询。

108
00:08:10,134 --> 00:08:13,641
现在除了这些基本的统计数据外

109
00:08:13,641 --> 00:08:18,380
我们还会存储所有与新闻相匹配的文件

110
00:08:18,380 --> 00:08:23,049
这些条目存储在名为 "发布" 的文件中。

111
00:08:24,150 --> 00:08:27,595
因此, 在这种情况下, 它匹配三个文件和

112
00:08:27,595 --> 00:08:31,680
我们在这里存储有关这三个文档的信息。

113
00:08:31,680 --> 00:08:38,160
这是文档 id, 文档1和频率为1。

114
00:08:38,160 --> 00:08:45,240
tf 是1对于新闻而言, 在第二个文件, 它也是 1, 等等。

115
00:08:45,240 --> 00:08:50,864
因此, 从这个列表中, 我们可以得到所有的文件, 匹配的术语新闻和

116
00:08:50,864 --> 00:08:55,320
我们也可以知道这些文件中的新闻频率。

117
00:08:55,320 --> 00:08:58,214
所以, 如果查询只有一个单词, 新闻,那么

118
00:08:58,214 --> 00:09:01,628
我们可以很容易地查找此表, 以找到条目然后

119
00:09:01,628 --> 00:09:06,780
很快的去posting里,获取所有的匹配文件,

120
00:09:06,780 --> 00:09:08,180
所以, 让我们来看看另一个术语。

121
00:09:09,280 --> 00:09:12,600
这一次, 让我们来看看总统这个词。

122
00:09:14,130 --> 00:09:17,950
这只发生在一个文档中, 文档3。

123
00:09:17,950 --> 00:09:23,490
因此, 文档频率为 1, 但在本文档中发生了两次。

124
00:09:23,490 --> 00:09:29,210
所以频率计数是二, 频率计数用于

125
00:09:29,210 --> 00:09:33,250
一些其他可访问的方法, 我们可能会使用的频率

126
00:09:34,490 --> 00:09:38,770
评估一个术语在集合中的受欢迎程度。

127
00:09:38,770 --> 00:09:42,930
同样, 我们将有一个指针, 在这里的帖子,

128
00:09:42,930 --> 00:09:47,490
在这种情况下, 这里只有一个条目, 因为

129
00:09:48,900 --> 00:09:53,570
这个词只出现在一个文档中, 就在这里。

130
00:09:53,570 --> 00:09:57,320
文档 id 为 3, 它发生了两次。

131
00:09:59,610 --> 00:10:02,550
因此, 这就是倒置索引的基本思想。

132
00:10:02,550 --> 00:10:04,340
其实很简单, 对吧？

133
00:10:06,580 --> 00:10:12,370
使用此结构, 我们可以轻松地获取与术语匹配的所有文档。

134
00:10:12,370 --> 00:10:15,760
这将是为查询打分文档的基础。

135
00:10:15,760 --> 00:10:23,770
现在有时我们也想存储这些术语的位置。

136
00:10:25,220 --> 00:10:31,960
因此, 在其中许多情况下, 该术语只发生在文档中一次。

137
00:10:31,960 --> 00:10:34,320
因此, 在这种情况下, 只有一个位置。

138
00:10:35,810 --> 00:10:40,990
但在这种情况下, 这个词发生了两次, 所以有两个位置。

139
00:10:40,990 --> 00:10:44,690
现在的位置信息是非常有用的检查是否

140
00:10:44,690 --> 00:10:48,400
查询术语的匹配实际上是在一个小窗口中,

141
00:10:48,400 --> 00:10:51,360
让我们说, 五个字或十个字。

142
00:10:52,410 --> 00:11:00,700
或者, 两个查询术语的匹配实际上是两个词的短语。

143
00:11:00,700 --> 00:11:04,540
通过使用每个位置, 可以快速检查这一切。

144
00:11:05,920 --> 00:11:10,160
那么, 为什么倒置索引对快速搜索有好处呢？

145
00:11:10,160 --> 00:11:16,349
嗯, 我们刚刚讨论了使用两个答案单术语查询的可能性。

146
00:11:16,349 --> 00:11:17,990
这很容易。

147
00:11:17,990 --> 00:11:19,910
多术语查询呢？

148
00:11:19,910 --> 00:11:23,800
让我们先来看看布尔查询的一些特殊情况。

149
00:11:23,800 --> 00:11:27,740
布尔查询基本上是这样的布尔表达式。

150
00:11:27,740 --> 00:11:36,290
因此, 我希望文档中的值与术语 a 和术语 B 相匹配。

151
00:11:36,290 --> 00:11:38,770
所以这是一个结合的查询。

152
00:11:38,770 --> 00:11:45,440
或者我希望 web 文档与术语 a 或术语 B 匹配。

153
00:11:45,440 --> 00:11:46,540
这是一个排斥的查询。

154
00:11:46,540 --> 00:11:51,070
但是, 我们如何通过使用倒置索引来回答这样的查询呢？

155
00:11:52,090 --> 00:11:53,860
好吧, 如果你想一下,

156
00:11:53,860 --> 00:11:58,130
这将是显而易见的, 因为我们只是获取所有

157
00:11:58,130 --> 00:12:03,170
与术语 a 匹配的文档, 并获取与术语 B 匹配的所有文档。

158
00:12:03,170 --> 00:12:08,160
然后只需使用交叉点回答像 A 和 B 这样的查询。

159
00:12:08,160 --> 00:12:13,050
或者带联合回答查询 a 或 B。

160
00:12:13,050 --> 00:12:16,020
所以这一切都很容易回答。

161
00:12:16,020 --> 00:12:17,780
会很快的。

162
00:12:17,780 --> 00:12:20,850
那么多术语关键字查询呢？

163
00:12:20,850 --> 00:12:24,390
我们讨论了向量空间模型, 例如,

164
00:12:24,390 --> 00:12:28,940
我们将做这样的查询与文档匹配, 并生成分数。

165
00:12:28,940 --> 00:12:32,330
而分数是基于汇总的术语权重。

166
00:12:32,330 --> 00:12:35,670
因此, 在这种情况下, 它不是布尔查询, 但

167
00:12:35,670 --> 00:12:38,770
得分实际上可以用类似的方式完成。

168
00:12:38,770 --> 00:12:42,430
基本上, 它类似于分离布尔查询。

169
00:12:42,430 --> 00:12:45,140
基本上, 就像 A 或 B。

170
00:12:45,140 --> 00:12:50,680
我们采取的所有文件的联合, 匹配至少一个查询术语和

171
00:12:50,680 --> 00:12:53,320
然后我们将聚合术语权重。

172
00:12:53,320 --> 00:13:01,420
因此, 这是一般对评分文档使用倒置索引的基本思想。

173
00:13:01,420 --> 00:13:05,210
我们稍后将更详细地讨论这个问题。

174
00:13:05,210 --> 00:13:06,000
但现在但现在

175
00:13:06,000 --> 00:13:12,210
让我们来看看为什么倒置索引是一个好主意？

176
00:13:12,210 --> 00:13:17,470
为什么它会比按顺序扫描文档更有效。

177
00:13:17,470 --> 00:13:20,770
这是显而易见的方法。

178
00:13:20,770 --> 00:13:27,518
您只需计算每个文档的分数, 然后就可以对它们进行排序。

179
00:13:27,518 --> 00:13:29,936
这是一个简单的方法, 但

180
00:13:29,936 --> 00:13:34,496
这将是非常缓慢的想象财富, 有很多文件。

181
00:13:34,496 --> 00:13:39,620
如果您这样做, 则需要很长时间才能回答您的查询。

182
00:13:39,620 --> 00:13:44,975
那么现在的问题是, 为什么倒置索引会更快？

183
00:13:44,975 --> 00:13:48,780
好吧, 它必须做的是文字中的单词分布。

184
00:13:48,780 --> 00:13:54,010
所以, 这里有一些常见的现象, 单词分布在文本中。

185
00:13:54,010 --> 00:13:58,720
有些语言独立于看似稳定的模式。

186
00:14:00,300 --> 00:14:07,690
而这些模式的基本特点是以下模式。

187
00:14:07,690 --> 00:14:10,830
几个词, 如常见的词, 如, the,a,or

188
00:14:10,830 --> 00:14:14,780
we，在文本中非常非常频繁地出现。

189
00:14:14,780 --> 00:14:18,210
因此, 它们占单词出现的很大比例。

190
00:14:19,405 --> 00:14:22,885
但大多数词很少会出现。

191
00:14:22,885 --> 00:14:25,615
有很多词只发生过一次,

192
00:14:25,615 --> 00:14:29,790
让我们说, 在文档中或在集合中一次。

193
00:14:29,790 --> 00:14:33,306
而且这样的情况很多。

194
00:14:33,306 --> 00:14:37,977
有一个常见的事实是，在一个语料库最高频出现的那个单词

195
00:14:37,977 --> 00:14:40,462
在另一个语料库中很少出现

196
00:14:40,462 --> 00:14:45,800
这意味着, 尽管一般现象是适用的,

197
00:14:45,800 --> 00:14:51,060
在许多情况下观察到, 确切的字是常见的

198
00:14:51,060 --> 00:14:54,770
可能因上下文而异。

199
00:14:54,770 --> 00:14:59,450
因此, 这种现象的特点是所谓的齐普夫定律。

200
00:14:59,450 --> 00:15:02,210
这条法律规定一个词的等级

201
00:15:02,210 --> 00:15:06,370
乘以这个词的频率是大致不变的。

202
00:15:07,450 --> 00:15:13,045
所以, 如果我们正式地使用 F (w) 来表示频率,

203
00:15:13,045 --> 00:15:16,310
r (w) 表示一个词的等级。

204
00:15:16,310 --> 00:15:17,390
然后这就是公式。

205
00:15:17,390 --> 00:15:21,300
它基本上说的是一样的, 只是数学术语。

206
00:15:21,300 --> 00:15:28,510
其中 C 基本上是一个常数, 然后这里也有一个参数,

207
00:15:28,510 --> 00:15:34,180
阿尔法, 这可能会被调整, 以更好地适应任何经验观察。

208
00:15:34,180 --> 00:15:38,128
所以如果我按排序的顺序绘制单词频率,

209
00:15:38,128 --> 00:15:40,980
那么你就能更容易地看到这一点。

210
00:15:40,980 --> 00:15:43,660
x 轴基本上是单词排名。

211
00:15:43,660 --> 00:15:50,393
这是 r (w), y 轴是单词频率或 F (w)。

212
00:15:50,393 --> 00:15:57,448
现在这条曲线表明, 两者的乘积大致是常数。

213
00:15:57,448 --> 00:16:02,524
现在如果你看看这些词, 我们可以看到它们可以分成三组。

214
00:16:02,524 --> 00:16:06,870
中间, 是中间频率词。

215
00:16:06,870 --> 00:16:11,440
这些词倾向于在很多文件中出现

216
00:16:11,440 --> 00:16:14,890
但是它们不用于最高频的那些词

217
00:16:14,890 --> 00:16:17,070
它们也不是很低频的词

218
00:16:18,190 --> 00:16:21,620
因此, 它们往往被用于

219
00:16:22,700 --> 00:16:28,240
查询, 他们也倾向于有很高的 TF-IDF 重量。

220
00:16:28,240 --> 00:16:31,290
这些中频词。

221
00:16:31,290 --> 00:16:34,480
但如果你看一下曲线的左侧

222
00:16:35,820 --> 00:16:38,330
这些是频率最高的单词。

223
00:16:38,330 --> 00:16:39,620
它们出现的频率非常高。

224
00:16:39,620 --> 00:16:45,540
它们经常是停用词，像the,we等等

225
00:16:45,540 --> 00:16:49,440
这些话是非常, 非常频繁, 他们实际上是太频繁以至于

226
00:16:49,440 --> 00:16:54,226
忽略，他们一般不是很有用的检索。

227
00:16:54,226 --> 00:17:01,900
所以他们经常被删除, 这被称为停止字删除。

228
00:17:01,900 --> 00:17:06,960
所以, 你可以使用几乎只是在集合中的那种词, 一种

229
00:17:06,960 --> 00:17:09,620
推断什么词可能是停止的词。

230
00:17:09,620 --> 00:17:12,690
这些基本上是频率最高的词。

231
00:17:13,780 --> 00:17:18,500
而且它们在倒置指数中也占据了很大的空间。

232
00:17:18,500 --> 00:17:23,048
你可以想象, 这样一个词的过帐条目会很长。

233
00:17:23,048 --> 00:17:24,370
因此,

234
00:17:24,370 --> 00:17:28,299
如果你能删除这些词, 你可以在倒置索引中节省大量的空间。

235
00:17:29,890 --> 00:17:35,100
我们还展示了尾部部分, 其中有很多罕见的词。

236
00:17:35,100 --> 00:17:38,470
这些词并不经常发生, 这样的词很多。

237
00:17:39,680 --> 00:17:41,330
这些话实际上是非常有用的

238
00:17:41,330 --> 00:17:45,630
如果用户恰好对这样的主题感兴趣, 也可以搜索。

239
00:17:45,630 --> 00:17:49,730
但由于他们是罕见的, 它往往是真实的, 用户

240
00:17:49,730 --> 00:17:54,030
不一定对这些词感兴趣。

241
00:17:54,030 --> 00:17:58,970
但是, 保留它们将使我们能够准确地匹配这样一份文件。

242
00:17:58,970 --> 00:18:00,610
他们的IDF一般都很高。

243
00:18:05,559 --> 00:18:10,840
那么, 我们应该用什么样的数据结构来存储倒置索引呢？

244
00:18:10,840 --> 00:18:11,970
嗯, 它有两部分, 对吧。

245
00:18:11,970 --> 00:18:16,720
如果你还记得, 我们有字典, 我们也有帖子。

246
00:18:16,720 --> 00:18:21,810
字典有适度的大小, 虽然对于网络, 它仍然会是非常

247
00:18:21,810 --> 00:18:24,810
大, 但比较它的张贴它是更小的

248
00:18:26,220 --> 00:18:29,710
我们还需要有快速的随机访问的条目

249
00:18:29,710 --> 00:18:32,940
因为我们想很快就找到查询项。

250
00:18:32,940 --> 00:18:39,200
因此, 如果可能的话, 我们更愿意把这样的字典留在记忆中。

251
00:18:39,200 --> 00:18:43,333
如果集合不是很大, 这是可行的, 但

252
00:18:43,333 --> 00:18:47,810
如果集合非常大, 那么它一般是不可能的。

253
00:18:47,810 --> 00:18:52,100
如果词汇量非常大, 显然我们做不到。

254
00:18:52,100 --> 00:18:55,800
所以, 一般来说, 情况就是这样。

255
00:18:55,800 --> 00:18:58,578
因此, 我们经常使用的数据结构

256
00:18:58,578 --> 00:19:01,390
存储字典, 这将是直接访问。

257
00:19:01,390 --> 00:19:04,375
有一些结构, 如哈希表, 或

258
00:19:04,375 --> 00:19:09,090
如果我们不能将所有内容存储在内存中或使用磁盘, 则为 b 树。

259
00:19:09,090 --> 00:19:12,760
然后尝试构建一个结构, 使其能够快速查找条目。

260
00:19:14,530 --> 00:19:16,705
对于帖子来说, 它们是巨大的。

261
00:19:18,045 --> 00:19:24,815
而一般来说, 我们不必直接访问特定的条目。

262
00:19:24,815 --> 00:19:29,145
我们通常只会查找文档 Id 和

263
00:19:29,145 --> 00:19:32,850
与查询项匹配的所有文档的频率。

264
00:19:33,930 --> 00:19:36,570
因此, 将按顺序读取这些条目。

265
00:19:37,670 --> 00:19:43,704
因此, 因为它很大, 我们通常有存储张贴光盘上,

266
00:19:43,704 --> 00:19:49,826
他们必须留在光盘上, 他们将包含的信息, 如文档 Id,

267
00:19:49,826 --> 00:19:53,392
长期频率或长期的位置, 等等。

268
00:19:53,392 --> 00:19:58,300
现在, 因为他们是非常大的, 压缩往往是可取的。

269
00:19:59,360 --> 00:20:04,390
现在, 这不仅是为了节省光盘空间, 这当然是

270
00:20:04,390 --> 00:20:09,080
压缩的一个好处, 它不会占用那么大的空间。

271
00:20:09,080 --> 00:20:11,750
但这也是为了帮助提高速度。

272
00:20:13,110 --> 00:20:15,980
你知道为什么吗？

273
00:20:15,980 --> 00:20:23,470
嗯, 我们知道输入和输出会花费很多时间。

274
00:20:23,470 --> 00:20:28,320
与 CPU 所花费的时间相比。

275
00:20:28,320 --> 00:20:33,410
因此, CPU 速度要快得多, 但 IO 需要时间和

276
00:20:33,410 --> 00:20:39,335
因此, 通过压缩倒置索引, posting文件将变得更小,

277
00:20:39,335 --> 00:20:45,115
条目, 我们有读数, 和内存来处理一个查询项,

278
00:20:45,115 --> 00:20:50,150
将是较小的, 然后, 所以我们可以减少

279
00:20:50,150 --> 00:20:55,080
跟踪 IO 的数量, 这可以节省大量的时间。

280
00:20:55,080 --> 00:21:00,270
当然, 我们必须做更多的数据处理时, 我们

281
00:21:00,270 --> 00:21:03,630
解压缩内存中的数据。

282
00:21:03,630 --> 00:21:05,550
但正如我所说, CPU 速度很快。

283
00:21:05,550 --> 00:21:07,019
所以总之, 我们还能节省时间。

284
00:21:08,360 --> 00:21:11,301
因此, 这里的压缩既是为了节省光盘空间, 也是为了节省光盘空间。

285
00:21:11,301 --> 00:21:14,035
以加快索引的加载速度。

286
00:21:14,035 --> 00:21:24,035
她们都有自己的孩子和孙子