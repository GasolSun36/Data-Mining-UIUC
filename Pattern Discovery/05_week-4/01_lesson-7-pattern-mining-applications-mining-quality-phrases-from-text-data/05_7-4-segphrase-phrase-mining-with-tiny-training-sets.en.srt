1
00:00:01,510 --> 00:00:07,610
Now we introduce another interesting
phrase mining algorithm called segPhrase.

2
00:00:07,610 --> 00:00:11,870
This is phrase mining
with tiny training sets.

3
00:00:13,290 --> 00:00:16,345
We have introduced ToPMine.

4
00:00:16,345 --> 00:00:22,580
ToPMining is mining phrases
without any training data.

5
00:00:22,580 --> 00:00:26,390
However if we have a small
set of training data,

6
00:00:26,390 --> 00:00:31,680
it may substantially enhance
the quality of phrase mining.

7
00:00:31,680 --> 00:00:34,480
This was done by Jalo Liu.

8
00:00:35,710 --> 00:00:41,511
He and his collaborators working
on Mining Quality Phrases

9
00:00:41,511 --> 00:00:45,339
from Massive Text Corporate Data, and

10
00:00:45,339 --> 00:00:51,500
they published their work in SIGMOD 2015,
is as follows.

11
00:00:52,890 --> 00:01:00,680
If we have a raw corpus, if we use
a small set of training labels by human.

12
00:01:00,680 --> 00:01:05,937
Of course, later they extended to get
a general knowledge base like Freebase or

13
00:01:05,937 --> 00:01:06,807
Wikipedia.

14
00:01:06,807 --> 00:01:10,485
But in this algorithm we first just study,

15
00:01:10,485 --> 00:01:16,214
suppose we got a small set of
labels by human, like 300 labels.

16
00:01:16,214 --> 00:01:21,430
Then we feed them into a mining process.

17
00:01:21,430 --> 00:01:28,800
We may be able to get a quality phrases
and also get a segmented corpus.

18
00:01:28,800 --> 00:01:30,230
And a quality Phrases and

19
00:01:30,230 --> 00:01:36,530
segmented Corpus can mutually enhance each
other by phrase or segmentation process.

20
00:01:36,530 --> 00:01:41,480
So finally, we'll be able to get
a high quality phrases generated.

21
00:01:43,620 --> 00:01:46,180
So we look at the overall framework.

22
00:01:46,180 --> 00:01:50,398
And the overall framework, the first thing

23
00:01:50,398 --> 00:01:55,300
they introduced comparing
with ToPMine is first to

24
00:01:55,300 --> 00:02:00,630
frequent pattern mining,
feature extraction.

25
00:02:00,630 --> 00:02:05,320
But in the meantime,
they introduce a classification process.

26
00:02:05,320 --> 00:02:09,710
The reason they introduced
the classification process

27
00:02:09,710 --> 00:02:13,600
is because they have the labels.

28
00:02:13,600 --> 00:02:18,230
The label can,
based on the feature extracted,

29
00:02:18,230 --> 00:02:27,290
the classifier will be able to say we can
put more weights on the positive labels.

30
00:02:27,290 --> 00:02:34,550
And then put some negative
labels as a back phrases.

31
00:02:34,550 --> 00:02:36,720
We will be able to work out a classifier.

32
00:02:37,800 --> 00:02:43,140
Okay, that means SegPhrase
itself use a classifier,

33
00:02:43,140 --> 00:02:45,770
okay, and small labeled datasets.

34
00:02:47,280 --> 00:02:53,390
This datasets could be provided by experts
or by a distant supervised knowledge base,

35
00:02:53,390 --> 00:02:57,670
like Wikipedia or DBPedia, okay.

36
00:02:57,670 --> 00:03:03,220
Then If we look at the SegPhrase
is doing phrasal segmentation

37
00:03:04,250 --> 00:03:08,580
and phrase quality estimation.

38
00:03:08,580 --> 00:03:14,670
After that, they will go back and
feed into this classifier again.

39
00:03:14,670 --> 00:03:18,810
To get one more round to
enhance mined quality phrases.

40
00:03:18,810 --> 00:03:23,980
Okay, so that's why they call
this one more round after

41
00:03:25,500 --> 00:03:30,640
phrasal segmentation and phrase quality
estimation, to feed back to the classifier

42
00:03:30,640 --> 00:03:36,390
again as SegPhrase+ process.

43
00:03:36,390 --> 00:03:39,310
Let's look at how they do it.

44
00:03:39,310 --> 00:03:45,780
The first thing they did is doing
pattern mining for candidate sets,

45
00:03:45,780 --> 00:03:50,840
that means using frequent pattern mining
algorithm to get a candidate phrase sets.

46
00:03:51,910 --> 00:03:57,590
That means you mine frequent k-grams.

47
00:03:57,590 --> 00:04:00,480
This essentially is frequent phrases,

48
00:04:00,480 --> 00:04:05,460
but k is typically small,
for example 6 grams.

49
00:04:05,460 --> 00:04:09,433
Of course, for
some other applications that I have,

50
00:04:09,433 --> 00:04:14,044
some medical applications,
k could be even bigger.

51
00:04:14,044 --> 00:04:17,940
The popularity measure is
by raw frequent words and

52
00:04:17,940 --> 00:04:20,990
a phrase mined from the corpus,
this popularity.

53
00:04:23,210 --> 00:04:27,170
Then doing feature extraction
based on the concordance.

54
00:04:27,170 --> 00:04:32,718
That means whether you partition
the phrase into two parts and

55
00:04:32,718 --> 00:04:40,091
check whether the co-occurrence is
significantly higher than the pure random.

56
00:04:40,091 --> 00:04:46,742
Then another feature extraction
process Is informativeness.

57
00:04:46,742 --> 00:04:48,425
That, essentially,

58
00:04:48,425 --> 00:04:53,926
is you assume the quality phrase
typical start end with a non-stopword.

59
00:04:53,926 --> 00:05:00,086
For example, you say,
machine learning is, machine learning.

60
00:05:00,086 --> 00:05:02,523
Likely machine learning is a good phrase.

61
00:05:02,523 --> 00:05:05,958
Machine learning is, this is stop.

62
00:05:05,958 --> 00:05:08,434
Where it starting,
I'm ending with a stop words.

63
00:05:08,434 --> 00:05:13,867
Like this actually is a non-stopword but
is a stopword,

64
00:05:13,867 --> 00:05:19,570
so you basically say, this is,
is not part of the phrase.

65
00:05:19,570 --> 00:05:25,770
Another method or
heuristic is using average IDF.

66
00:05:25,770 --> 00:05:29,798
IDF is inverted document frequency

67
00:05:29,798 --> 00:05:34,380
over the words in the phrase
to measure the semantics.

68
00:05:34,380 --> 00:05:42,737
Simply says if this phrase is
frequent almost in all the documents,

69
00:05:42,737 --> 00:05:47,723
across all the documents in the corpus,

70
00:05:47,723 --> 00:05:50,956
then this IDF is too high.

71
00:05:50,956 --> 00:05:53,597
And it may not be very good at quality.

72
00:05:53,597 --> 00:05:59,108
And if this phrase is, actually, occur
more frequent in some documents but less

73
00:05:59,108 --> 00:06:04,728
frequent in other documents, it becomes
more interesting, more distinctive.

74
00:06:04,728 --> 00:06:09,979
Then another heuristic,
the algorithm users is

75
00:06:09,979 --> 00:06:16,256
considered the probability
of quality phrase in quotes,

76
00:06:16,256 --> 00:06:22,804
brackets, or connected by dash or
hyphen should be higher.

77
00:06:22,804 --> 00:06:29,108
For example,
like state-of-the-art actually is,

78
00:06:29,108 --> 00:06:37,570
it should be a phrase rather than
syncing these several different things.

79
00:06:37,570 --> 00:06:42,490
Then SegPhrase uses
a classification process.

80
00:06:42,490 --> 00:06:46,000
It explores a tiny set of training data.

81
00:06:46,000 --> 00:06:48,841
Usually four gigabytes corpus.

82
00:06:48,841 --> 00:06:52,075
You just need it to have your labels.

83
00:06:52,075 --> 00:06:54,600
But what kind of label it is?

84
00:06:54,600 --> 00:07:01,850
For example, you may say, support
vector machine is high quality phrase.

85
00:07:01,850 --> 00:07:06,410
The experiment shows is not a good phrase.

86
00:07:06,410 --> 00:07:09,140
You just based on the quality of one or
zero.

87
00:07:11,200 --> 00:07:14,410
Then we can construct
a models to distinguish

88
00:07:14,410 --> 00:07:16,210
quality phrases from the poor ones.

89
00:07:17,750 --> 00:07:22,882
And this classifier [COUGH] actually in it

90
00:07:22,882 --> 00:07:31,280
use Random Forest algorithm to boost
different datasets with limited labels.

91
00:07:31,280 --> 00:07:35,740
So you do not using very
small number of labels.

92
00:07:35,740 --> 00:07:38,730
Then you'll miss coverage of the others.

93
00:07:38,730 --> 00:07:44,255
So the Random Forest usually
gives you less bias.

94
00:07:44,255 --> 00:07:49,210
Then phrasal segmentation can tell
which phrase is more appropriate.

95
00:07:49,210 --> 00:07:54,796
For example, you look at this one,
you say a standard feature

96
00:07:54,796 --> 00:08:00,500
vector machine learning setup
is used to describe something.

97
00:08:00,500 --> 00:08:05,509
Actually, instead of thinking this,

98
00:08:05,509 --> 00:08:12,960
like if you say feature vector here,
it's a good phrase.

99
00:08:12,960 --> 00:08:17,160
In this particular one, the vector
machine should be not be combined, but

100
00:08:17,160 --> 00:08:19,910
machine learning should be combined.

101
00:08:19,910 --> 00:08:24,143
So that simply says when you rectify it,

102
00:08:24,143 --> 00:08:30,920
you will not say vector machine
actually got as the real count.

103
00:08:30,920 --> 00:08:34,880
But the feature vector is a good count,
and machine learning is a good count.

104
00:08:36,760 --> 00:08:41,730
Then we can partition a sequence of
the words by maximizing the likelihood.

105
00:08:42,740 --> 00:08:44,920
And consider length penalty.

106
00:08:44,920 --> 00:08:48,350
That means you get too long,
maybe it's not a very good phrase.

107
00:08:48,350 --> 00:08:51,360
Filter out phrases with
low rectified frequency.

108
00:08:54,110 --> 00:09:01,240
Then if the process is a classification,
then do phrasal segmentation.

109
00:09:01,240 --> 00:09:04,930
This part is the SegPhrase the first one.

110
00:09:06,640 --> 00:09:12,840
And for putting plus, actually it's
you feed this phrasal segmentation

111
00:09:12,840 --> 00:09:18,000
process into classification again,
then you do phrasal segmentation again.

112
00:09:18,000 --> 00:09:21,050
This actually derive even better results.

113
00:09:21,050 --> 00:09:25,660
We call this one SegPhrase +.

114
00:09:25,660 --> 00:09:29,630
Now we probably can see
the experimental results.

115
00:09:30,780 --> 00:09:35,570
The data actually used was DBLP and
Yelp was in millions of

116
00:09:35,570 --> 00:09:40,600
documents and tens of millions or
hundreds of millions of words.

117
00:09:40,600 --> 00:09:45,260
But every time, we just used this 300
human label phrases for training.

118
00:09:46,720 --> 00:09:54,253
And then fo r the phrase evaluation,
we use Wiki Phrase and

119
00:09:54,253 --> 00:09:59,637
sampled 500 times 7 Wiki-uncovered

120
00:09:59,637 --> 00:10:05,140
phrases with evaluated by reviewers.

121
00:10:05,140 --> 00:10:09,109
Then you probably can see the readers
comparing with other algorithms.

122
00:10:10,930 --> 00:10:16,120
And you can see comparing to
a few comparative algorithms.

123
00:10:16,120 --> 00:10:19,950
Of course,
it may not be a very fair comparison

124
00:10:19,950 --> 00:10:23,860
if you think TopMine does
not use any training at all.

125
00:10:23,860 --> 00:10:27,060
Well, anyway we put it down
there to give you a relatively

126
00:10:27,060 --> 00:10:30,452
where TopMine is if we do
not use any training data.

127
00:10:31,720 --> 00:10:36,448
So this is a precision recall curve for

128
00:10:36,448 --> 00:10:42,665
DBLP data using Wiki Phrase
as a label to check it.

129
00:10:42,665 --> 00:10:47,372
And the second one is based on DBpedia,

130
00:10:47,372 --> 00:10:53,940
not using Wiki Phrase but
using now Wiki Phrases.

131
00:10:53,940 --> 00:11:01,890
For the Wiki phrases, you probably can
see the difference is pretty high.

132
00:11:01,890 --> 00:11:06,686
Now, Wiki phrases, the phrase read out.

133
00:11:06,686 --> 00:11:12,091
Actually, it's,
you probably can see it's overall,

134
00:11:12,091 --> 00:11:18,404
this method generate a very high
quality comparing to the others.

135
00:11:22,492 --> 00:11:27,960
And also Segphrase algorithm is efficient,
is linearly scalable.

136
00:11:29,830 --> 00:11:32,629
So we look at some real
results people can see.

137
00:11:33,700 --> 00:11:38,713
To my, from the titles and
abstract of ACMCKDD,

138
00:11:38,713 --> 00:11:43,362
the KDD proceedings, you probably can see,

139
00:11:43,362 --> 00:11:48,989
we just need a title and
abstract by using SegPhrase and

140
00:11:48,989 --> 00:11:55,130
comparing with chunking method
using TF-IDF and C value.

141
00:11:56,570 --> 00:12:05,020
And these red ones are phrases
only generated in SegPhrase+ but

142
00:12:05,020 --> 00:12:09,240
not in chunking, these red ones.

143
00:12:09,240 --> 00:12:17,360
And these blue ones, those generated
only in chunking but not in SegPhrase.

144
00:12:17,360 --> 00:12:24,400
But you probably can see in that case,
you'll see like for KDD, time series,

145
00:12:24,400 --> 00:12:29,790
gene expression data, frequent subgraph,
categorical attribute.

146
00:12:29,790 --> 00:12:35,570
These are meaningful good phrases for KDD.

147
00:12:35,570 --> 00:12:40,660
But if you look at a important problem,
effective ways, small sets,

148
00:12:40,660 --> 00:12:46,960
these are the phrases almost
appear anywhere, not just in KDD.

149
00:12:46,960 --> 00:12:52,872
So that means chunking
methods may not generate

150
00:12:52,872 --> 00:12:58,653
as high quality phrases
as like SegPhrase+.

151
00:12:58,653 --> 00:13:02,367
And the interesting thing
is both ToPMine and

152
00:13:02,367 --> 00:13:08,925
SegPhrase+ are extensible to mining
quality phrases in multiple languages.

153
00:13:08,925 --> 00:13:14,693
For example, this is a Chinese
language from the Chinese Wikipedia.

154
00:13:14,693 --> 00:13:19,881
And we probably can see,
these Chinese phrases

155
00:13:19,881 --> 00:13:26,150
are pretty high quality is
generated using SegPhrase.

156
00:13:26,150 --> 00:13:31,588
And it can, my Arabic language,
even without the pre-processing,

157
00:13:31,588 --> 00:13:36,843
you probably can see the results
TopMine can generate those phrases

158
00:13:36,843 --> 00:13:43,780
without any training and pre-processing,
and generate some quality phrases as well.

159
00:13:45,600 --> 00:13:51,330
So in summary, we have studied
pattern mining applications,

160
00:13:51,330 --> 00:13:54,870
mining quality phrases from text data.

161
00:13:54,870 --> 00:14:00,100
We studied why frequent pattern mining
could be useful for phrase mining.

162
00:14:01,440 --> 00:14:06,080
We discussed previous phrase
pattern mining methods.

163
00:14:07,160 --> 00:14:09,090
We introduced two new algorithms.

164
00:14:09,090 --> 00:14:11,470
One is ToPMine, one is SegPhrase.

165
00:14:12,820 --> 00:14:18,810
And this is a set of references,
those published papers,

166
00:14:18,810 --> 00:14:24,750
we have been cited and
compared in our studies.

167
00:14:25,860 --> 00:14:26,360
Thank you.