1
00:00:00,008 --> 00:00:03,148
[SOUND] In this session,

2
00:00:03,148 --> 00:00:08,172
we are going to study another issue for

3
00:00:08,172 --> 00:00:15,086
clustering assessment
called Cluster Stability.

4
00:00:15,086 --> 00:00:20,277
Cluster stability basically say,
whether you got the right parameter,

5
00:00:20,277 --> 00:00:23,470
you get the very stable
clustering results.

6
00:00:24,650 --> 00:00:30,185
That means you can assume you get
the data set D, you take a sample

7
00:00:30,185 --> 00:00:35,836
get several datasets from the same
data set D, you do the clustering.

8
00:00:35,836 --> 00:00:40,822
And you should assume if you're clustering
finally get a very stable clustering

9
00:00:40,822 --> 00:00:44,010
results, that's a good clustering.

10
00:00:44,010 --> 00:00:47,757
Usually, you can use this to
find a good parameter values for

11
00:00:47,757 --> 00:00:49,753
a given clustering algorithm.

12
00:00:49,753 --> 00:00:55,780
For example, you can find a good value of
k, which is a good number of clusters.

13
00:00:56,890 --> 00:00:58,378
So how do you find this?

14
00:00:58,378 --> 00:01:02,550
You probably just look at this graph,
you probably can see.

15
00:01:02,550 --> 00:01:05,694
If you set k as three or even as two,

16
00:01:05,694 --> 00:01:10,586
you probably can find it
quite reasonable clusters.

17
00:01:10,586 --> 00:01:15,066
But if you set as four or five,
no matter how smart you are,

18
00:01:15,066 --> 00:01:19,660
probably this cluster you
find may not be quite stable.

19
00:01:19,660 --> 00:01:25,639
So to that extent, you may try
to test the cluster's stability,

20
00:01:25,639 --> 00:01:30,751
we introduce one method
called Bootstrapping approach

21
00:01:30,751 --> 00:01:35,878
to find the best value of k,
judged based on stability.

22
00:01:35,878 --> 00:01:37,990
How can we do this?

23
00:01:37,990 --> 00:01:43,110
This Bootstrapping basically says,
from D, you take tt samples of size n,

24
00:01:43,110 --> 00:01:46,795
but the,
the sampling is sampling with replacement.

25
00:01:46,795 --> 00:01:52,910
Then it's every time you take almost
from the same D, you take it t times.

26
00:01:52,910 --> 00:01:57,243
Then you get to the sample D sub 1,

27
00:01:57,243 --> 00:02:00,836
D sub to D sub j to D sub t.

28
00:02:00,836 --> 00:02:01,970
Okay.

29
00:02:01,970 --> 00:02:06,581
Then when you run the same
clustering algorithm with k

30
00:02:06,581 --> 00:02:10,586
value from two to the maximum
of the k you like.

31
00:02:10,586 --> 00:02:11,570
Okay.

32
00:02:11,570 --> 00:02:15,711
Then you can compare the distance
between all the pairs of clustering.

33
00:02:15,711 --> 00:02:21,887
For example, for each k,
you may try to see whether the D sub i and

34
00:02:21,887 --> 00:02:27,030
D sub j,
these two different sample datasets.

35
00:02:27,030 --> 00:02:31,501
You will see whether finer you get the,
the sample,

36
00:02:31,501 --> 00:02:37,503
you want to compute the expected
pairwise distance for each value of k.

37
00:02:37,503 --> 00:02:41,738
That means suppose k you get a ten,
you get at least ten clusters,

38
00:02:41,738 --> 00:02:45,160
you want to see their pairwise distance.

39
00:02:45,160 --> 00:02:45,961
Okay.

40
00:02:45,961 --> 00:02:49,299
Then what you can see is this value k,

41
00:02:49,299 --> 00:02:54,810
if it exhibits the least
deviation between clustering.

42
00:02:54,810 --> 00:02:59,876
That means you do clustering
those sample D sub i or

43
00:02:59,876 --> 00:03:04,295
sample D sub j,
they have the least deviation.

44
00:03:04,295 --> 00:03:04,820
Okay.

45
00:03:04,820 --> 00:03:08,711
That k should be the most stable one.

46
00:03:08,711 --> 00:03:12,030
Thus, you may want to have this k value.

47
00:03:12,030 --> 00:03:19,336
That's one application to test
the cluster stability to find the best k.

48
00:03:19,336 --> 00:03:26,586
Actually, there are many methods to find
the k, the appropriate number of clusters.

49
00:03:26,586 --> 00:03:28,996
One method called empirical method,

50
00:03:28,996 --> 00:03:32,870
actually people give you
this empirical formula.

51
00:03:32,870 --> 00:03:36,486
For example, for
total data sets of n points,

52
00:03:36,486 --> 00:03:40,586
you may take the square root
of half of this n points.

53
00:03:40,586 --> 00:03:41,150
Okay.

54
00:03:41,150 --> 00:03:43,462
For example, if n is 200,

55
00:03:43,462 --> 00:03:49,211
the expected value you will need to
get number of clusters should be 10.

56
00:03:49,211 --> 00:03:55,260
Of course, this may work out for
a small number of points.

57
00:03:55,260 --> 00:04:00,500
If you get a really big number of points,
for example, you get a,

58
00:04:00,500 --> 00:04:06,217
too many points, then you get this value,
you'll get a k as a solvent,

59
00:04:06,217 --> 00:04:12,253
probably you may not want to get a solvent
clusters even for too many points.

60
00:04:12,253 --> 00:04:15,710
Another method,
people use called elbow method.

61
00:04:15,710 --> 00:04:20,437
Elbow method means you tried
to based on the number of

62
00:04:20,437 --> 00:04:24,420
clusters that go one, two, three, four.

63
00:04:24,420 --> 00:04:31,170
You get number of clusters, then you get
it, the sum of the within square variance.

64
00:04:31,170 --> 00:04:37,340
That means you just to look at the average
of within to cluster variance,

65
00:04:37,340 --> 00:04:41,789
you try to look at this to see,
know what is the best,

66
00:04:41,789 --> 00:04:47,878
the number of k that you want to see
the elbow point, the turning point.

67
00:04:47,878 --> 00:04:51,200
So that's one method.

68
00:04:51,200 --> 00:04:56,920
Another method of finding the best
number k is use cross validation.

69
00:04:56,920 --> 00:05:01,620
That means you divide
a given dataset into m parts.

70
00:05:01,620 --> 00:05:07,336
So you take one part as a test data,
the remaining part to do the clustering.

71
00:05:07,336 --> 00:05:09,880
Okay.
You can do this m times, so

72
00:05:09,880 --> 00:05:14,379
you can check their overall
quality of the clustering.

73
00:05:14,379 --> 00:05:17,586
Then how we test the,
the quality of the examples?

74
00:05:17,586 --> 00:05:18,310
Okay.

75
00:05:18,310 --> 00:05:23,547
What we do is, is, supposed we use this
m minus 1 parts to do the clustering,

76
00:05:23,547 --> 00:05:25,170
we find the k clusters.

77
00:05:25,170 --> 00:05:29,699
Then for each point in the testing set,
you try to find the,

78
00:05:29,699 --> 00:05:32,600
it's closest centroid.

79
00:05:32,600 --> 00:05:38,019
Then you, based on this, you try to
find for all the points in the datasets,

80
00:05:38,019 --> 00:05:41,336
you try to find the sum
of the square distance.

81
00:05:41,336 --> 00:05:49,373
That means you try to find some of
the square error SSE, as we introduced.

82
00:05:49,373 --> 00:05:56,837
Then usually, you try to see whether this,
you get the best fit of the test datasets.

83
00:05:56,837 --> 00:06:01,628
That means you get the smallest
sum of square distance.

84
00:06:01,628 --> 00:06:06,502
Since for
cross validation you repeat this m times,

85
00:06:06,502 --> 00:06:11,263
so then what you can do is
you can compare the overall

86
00:06:11,263 --> 00:06:15,917
quality measure with
respect to different k's,

87
00:06:15,917 --> 00:06:21,711
then you'll find what is the best
number to fit the data best?

88
00:06:21,711 --> 00:06:25,729
That means you get
the overall quality measure,

89
00:06:25,729 --> 00:06:29,461
you get the lowest SSE for
this particular k.

90
00:06:29,461 --> 00:06:34,170
Usually, this is the right
number of clusters.

91
00:06:34,170 --> 00:06:44,170
[MUSIC]