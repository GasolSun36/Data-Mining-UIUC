1
00:00:00,000 --> 00:00:05,548
[SOUND] In this session,
we're going to study

2
00:00:05,548 --> 00:00:12,418
another important issue
called clustering tendency.

3
00:00:12,418 --> 00:00:14,580
What is clustering tendency?

4
00:00:14,580 --> 00:00:22,060
That means we want to study whether the
data sets we have really contain clusters.

5
00:00:22,060 --> 00:00:25,910
Whether data contains some
inherent grouping structures.

6
00:00:25,910 --> 00:00:32,690
Otherwise, even use random generated data,
you may find a certain number of clusters.

7
00:00:32,690 --> 00:00:35,120
However, it does not really make sense.

8
00:00:35,120 --> 00:00:39,840
That means we want to assess whether
the data is suitable for clustering.

9
00:00:41,940 --> 00:00:45,040
However, to determine
the clustering tendency, or

10
00:00:45,040 --> 00:00:47,560
clusterability, is a hard task.

11
00:00:47,560 --> 00:00:51,470
Just because there are so
many definitions of clusters.

12
00:00:51,470 --> 00:00:56,198
For example, we studied partitioning
methods, hierarchical methods,

13
00:00:56,198 --> 00:00:59,580
density-based methods,
graph-based methods.

14
00:00:59,580 --> 00:01:04,310
All these different methods may have
different definitions of clusters.

15
00:01:04,310 --> 00:01:10,160
That's why to study whether the data
is clusterable or not is pretty hard.

16
00:01:10,160 --> 00:01:12,829
However, even we just
fix the cluster type.

17
00:01:12,829 --> 00:01:17,021
For example,
we just study the partitioning methods,

18
00:01:17,021 --> 00:01:22,910
it is still hard to define an appropriate
null model for a data set.

19
00:01:22,910 --> 00:01:28,208
However, there are still some studies
on clusterability assessment.

20
00:01:28,208 --> 00:01:33,971
For example, there are methods
like spatial histogram method,

21
00:01:33,971 --> 00:01:38,800
distant distribution method,
a Hopkins statistic.

22
00:01:38,800 --> 00:01:44,219
The general philosophies, they try to
compare their measure with the measure

23
00:01:44,219 --> 00:01:50,340
generate from random samples,
to see whether they are rather different.

24
00:01:50,340 --> 00:01:56,510
For example, for spatial histogram
method is try to contrast

25
00:01:56,510 --> 00:02:02,020
the histogram of data with the histogram

26
00:02:02,020 --> 00:02:05,670
generated from the random sample to
see whether they are rather different.

27
00:02:07,050 --> 00:02:12,170
For distance distribution is try to
compare the pairwise point distance from

28
00:02:12,170 --> 00:02:19,500
the data, and the pairwise distance
from the randomly generated samples.

29
00:02:20,940 --> 00:02:27,370
Hopkins Statistic is to use a sparse
sampling test for spatial randomness.

30
00:02:28,620 --> 00:02:31,820
And since they carry quite similar spirit

31
00:02:31,820 --> 00:02:35,965
we will only introduce one
spatial histogram method.

32
00:02:37,810 --> 00:02:42,060
For spatial histogram approach,
the general philosophy is

33
00:02:42,060 --> 00:02:46,970
we try to construct a d-dimensional
histogram of the input.

34
00:02:46,970 --> 00:02:51,810
For example, this is the two d we
may constructed two dimensional

35
00:02:51,810 --> 00:02:54,610
histogram for the input dataset D.

36
00:02:56,020 --> 00:02:59,550
With the histogram generated
from random samples.

37
00:02:59,550 --> 00:03:05,070
That means we want compare the figure
generated from the left and

38
00:03:05,070 --> 00:03:07,590
that generated from the right figure.

39
00:03:07,590 --> 00:03:10,820
Okay, the data is clusterable

40
00:03:10,820 --> 00:03:14,230
if these two histogram
distributions are rather different.

41
00:03:14,230 --> 00:03:17,230
That means they really generate
very different distributions,

42
00:03:17,230 --> 00:03:20,956
then likely this dataset is clusterable,
okay?

43
00:03:20,956 --> 00:03:29,180
The concrete method is we try to divide
each dimension into equal waste bins.

44
00:03:30,480 --> 00:03:36,930
Then we try to see each cell here,
how many points are in the cell.

45
00:03:36,930 --> 00:03:42,690
Now we can generate an empirical joint
probability mass function, okay?

46
00:03:42,690 --> 00:03:45,895
Then for the random generate data sets,
we'll do the same.

47
00:03:45,895 --> 00:03:49,540
We'll generate empirical joint
probability mass function.

48
00:03:49,540 --> 00:03:53,960
Then what we will do is we try
to compare whether these two,

49
00:03:53,960 --> 00:03:55,170
they differ quite a lot.

50
00:03:55,170 --> 00:03:58,165
That means we're going to
compare how much they differ.

51
00:03:58,165 --> 00:04:02,260
The typical method is
we use KL divergence.

52
00:04:02,260 --> 00:04:05,250
That means we calculate
the KL divergence value.

53
00:04:05,250 --> 00:04:08,740
The KL divergence is Kullback-Leibler

54
00:04:08,740 --> 00:04:13,150
divergence is defined
based on some formulas.

55
00:04:13,150 --> 00:04:16,470
I'm now going to introduce
the detail of KL divergence.

56
00:04:16,470 --> 00:04:17,980
If you want to learn more,

57
00:04:17,980 --> 00:04:23,240
you can check any textbook in statistics
or mission learning or data mining.

58
00:04:23,240 --> 00:04:25,380
In general they introduce this measure.

59
00:04:25,380 --> 00:04:28,430
Then you'll find if they
are very different.

60
00:04:28,430 --> 00:04:33,950
Then, from the random samples, then you
will say this data set is clusterable.

61
00:04:35,730 --> 00:04:41,400
Finally, I'm going to introduce
a few textbooks, chapters,

62
00:04:41,400 --> 00:04:47,813
or general papers, that discuss
the clustering validation measures.

63
00:04:47,813 --> 00:04:54,950
Especially what he shows this way is
a summary of many research papers.

64
00:04:54,950 --> 00:04:59,030
And Zaki's book gives a lot
of details on other measures

65
00:04:59,030 --> 00:05:02,710
we did not cover in this lecture.

66
00:05:02,710 --> 00:05:05,474
Thank you.

67
00:05:05,474 --> 00:05:15,474
[MUSIC]