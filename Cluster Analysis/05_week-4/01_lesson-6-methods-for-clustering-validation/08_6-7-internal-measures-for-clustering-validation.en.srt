1
00:00:00,000 --> 00:00:03,530
[SOUND] In this session,

2
00:00:03,530 --> 00:00:09,531
we were introduce Internal Measures for

3
00:00:09,531 --> 00:00:13,420
Clustering Validation.

4
00:00:13,420 --> 00:00:19,171
Remember, we just discussed several
kinds of external measures.

5
00:00:19,171 --> 00:00:24,254
The lucky thing for external
measure is we do have [INAUDIBLE].

6
00:00:24,254 --> 00:00:27,836
We have expert knowledge,
we have a priors.

7
00:00:27,836 --> 00:00:32,296
Unfortunately, in many cases,
we do not have [INAUDIBLE].

8
00:00:32,296 --> 00:00:36,795
That's the reason we have to
rely on internal measures.

9
00:00:36,795 --> 00:00:40,170
The internal measures based
on the concept of clustering.

10
00:00:40,170 --> 00:00:43,254
It means we want the points
within the same cluster.

11
00:00:43,254 --> 00:00:46,003
That means intra-cluster,
they are very compact.

12
00:00:46,003 --> 00:00:48,250
They are close to each other and

13
00:00:48,250 --> 00:00:53,962
we wanted points in different clusters
that means inter-cluster distance.

14
00:00:53,962 --> 00:00:57,711
We want them as far apart as possible.

15
00:00:57,711 --> 00:01:00,838
That means we want them
to be far separated.

16
00:01:00,838 --> 00:01:07,361
That's the reason we want to maximizing
the intra-cluster compactness and

17
00:01:07,361 --> 00:01:11,978
also maximizing the inter-cluster
separation, but

18
00:01:11,978 --> 00:01:15,504
we want a tradeoff between these things.

19
00:01:15,504 --> 00:01:19,253
So let's look at how we define this.

20
00:01:19,253 --> 00:01:24,158
Suppose we give a clustering C with k

21
00:01:24,158 --> 00:01:28,253
clusters, C sub 1to C sub k.

22
00:01:28,253 --> 00:01:34,545
For each cluster C sub i,
it contains n sub i number of points.

23
00:01:34,545 --> 00:01:39,110
Then we introduce the function W of S, R,

24
00:01:39,110 --> 00:01:44,065
which is the sum of
the weights of all the edges

25
00:01:44,065 --> 00:01:48,379
with one verdicts in S, the other in R.

26
00:01:48,379 --> 00:01:51,003
For example, you'd get a two points.

27
00:01:51,003 --> 00:01:53,878
You say, this point is in this cluster.

28
00:01:53,878 --> 00:01:56,253
The other point is in the other cluster.

29
00:01:56,253 --> 00:01:58,754
You want to look at their weight.

30
00:01:58,754 --> 00:02:03,724
The weight usually can be defined
like a euclidean distance or

31
00:02:03,724 --> 00:02:06,753
whatever you are using for clustering.

32
00:02:06,753 --> 00:02:12,280
Then based on this, we can say,
the sum of all the intra-cluster

33
00:02:12,280 --> 00:02:16,920
weights over all clusters
should be defined this way.

34
00:02:16,920 --> 00:02:21,378
The reason is you can see this
is a C sub i, this is a C sub i.

35
00:02:21,378 --> 00:02:28,086
That means both vertices are in the same
cluster, so that's why it's inter-cluster.

36
00:02:28,086 --> 00:02:33,374
But at one point A and the other point B,
you actually use it will calculate twice,

37
00:02:33,374 --> 00:02:37,378
because A to B, B to A,
you're actually calculating twice.

38
00:02:37,378 --> 00:02:39,920
That's why you have to divide this by two.

39
00:02:39,920 --> 00:02:41,045
Okay.

40
00:02:41,045 --> 00:02:45,959
For all the clusters sum up
together is i from one to

41
00:02:45,959 --> 00:02:51,575
the k's clusters,
then we define the sum of all the inter

42
00:02:51,575 --> 00:02:56,378
cluster weights should
be calculated this way.

43
00:02:56,378 --> 00:03:00,841
There, which is very obvious,
because one point,

44
00:03:00,841 --> 00:03:06,836
one vertex is in the cluster C sub i and
the other one is not in the C sub i.

45
00:03:06,836 --> 00:03:11,128
Simply say, in the other clusters,
that's why it's intro cluster.

46
00:03:11,128 --> 00:03:14,493
But since the two points,
you calculate it twice,

47
00:03:14,493 --> 00:03:17,003
that's why you have the divide by 2.

48
00:03:17,003 --> 00:03:22,670
Then we sum up all these essentially,
it's i from one to k, all the clusters.

49
00:03:22,670 --> 00:03:25,003
We get a Wout, okay?

50
00:03:25,003 --> 00:03:30,586
That's the inter-cluster
weights sum them together.

51
00:03:30,586 --> 00:03:35,793
Now we also can see the number
of distinct intra-cluster edges,

52
00:03:35,793 --> 00:03:40,146
because for
i's cluster if n sub i number of points

53
00:03:40,146 --> 00:03:45,086
there intra-cluster link,
you have end points choose two.

54
00:03:45,086 --> 00:03:47,503
That's why you get this formula.

55
00:03:47,503 --> 00:03:54,461
Then get all the clusters one to k,
you sum up, that's why you get n sub m.

56
00:03:54,461 --> 00:03:59,445
Then for the inter-cluster
N sub out is notated by one

57
00:03:59,445 --> 00:04:04,336
point is in cluster i,
the other point is cluster j.

58
00:04:04,336 --> 00:04:07,003
They never are in the same cluster.

59
00:04:07,003 --> 00:04:12,170
So you want to calculate all the edges,
among different clusters.

60
00:04:12,170 --> 00:04:16,588
So that's the number of
distinct inter-cluster edges.

61
00:04:16,588 --> 00:04:21,698
Then for BetaCV measure is
defined as Wi divided by N and

62
00:04:21,698 --> 00:04:27,140
you can think this is the mean
intra-cluster distance and

63
00:04:27,140 --> 00:04:30,694
you get Wout divided by number of out,

64
00:04:30,694 --> 00:04:35,378
you can think is a mean
inter-cluster distance.

65
00:04:35,378 --> 00:04:41,865
So the BetaCV essentially
the ratio of mean inter-cluster

66
00:04:41,865 --> 00:04:47,711
distance versus the mean
intra-cluster distance.

67
00:04:47,711 --> 00:04:51,378
Of course, for this measure is
the smaller, the better clustering.

68
00:04:51,378 --> 00:04:56,586
Because a smaller means,
this is pretty small is quite a compact.

69
00:04:56,586 --> 00:05:00,003
And this is the bigger,
it means it's more separated.

70
00:05:00,003 --> 00:05:05,086
That's a BetaCV measure defined,
then we look at some other definition.

71
00:05:05,086 --> 00:05:09,086
One popular use one called normalized cut.

72
00:05:09,086 --> 00:05:12,086
Normalized cut is defined as follows.

73
00:05:12,086 --> 00:05:19,170
The normalized cut of this ratio is
defined by sum up of all the k clusters.

74
00:05:19,170 --> 00:05:22,130
For every cluster,
you calculate in this way or

75
00:05:22,130 --> 00:05:25,170
more explicitly,
we can interpret in this way.

76
00:05:25,170 --> 00:05:28,420
This is ways of the inter-cluster.

77
00:05:28,420 --> 00:05:34,586
This is a weights to sum up of,
of the intra and inter-cluster weights.

78
00:05:34,586 --> 00:05:40,235
And for this ratio,
you actually sum from i from one to k,

79
00:05:40,235 --> 00:05:44,378
you'll get these normalized cut ratio.

80
00:05:44,378 --> 00:05:49,368
Of course, based on this,
you want to the inter-cluster

81
00:05:49,368 --> 00:05:53,336
to be far separated,
comparing to the intro one.

82
00:05:53,336 --> 00:05:58,628
That's why the bigger normalized cut
value, the better the clustering.

83
00:05:58,628 --> 00:06:02,712
Then there's another
definition called modularity.

84
00:06:02,712 --> 00:06:07,711
This is especially popular for
graft clustering.

85
00:06:07,711 --> 00:06:09,045
It is defined in this way.

86
00:06:09,045 --> 00:06:13,030
You proceed, this part is the same
as normalized cut, cut formula.

87
00:06:13,030 --> 00:06:14,503
Okay.

88
00:06:14,503 --> 00:06:18,878
Like this, but
this part is the square of this.

89
00:06:18,878 --> 00:06:19,711
Okay.

90
00:06:19,711 --> 00:06:24,089
So that simply says,
this one is the observed

91
00:06:24,089 --> 00:06:28,170
the fraction of the weights
within clusters.

92
00:06:28,170 --> 00:06:30,753
This is the expected one.

93
00:06:30,753 --> 00:06:35,792
So modularity, actually measured the
difference between the observed one and

94
00:06:35,792 --> 00:06:37,003
the expected one.

95
00:06:37,003 --> 00:06:40,578
So the smaller the value,
the better the clustering,

96
00:06:40,578 --> 00:06:45,254
because the inter-cluster distance
are lower than the expected one.

97
00:06:45,254 --> 00:06:50,441
So this is the modularity definition

98
00:06:50,441 --> 00:06:54,461
used for graph clustering.

99
00:06:54,461 --> 00:07:04,461
[MUSIC]