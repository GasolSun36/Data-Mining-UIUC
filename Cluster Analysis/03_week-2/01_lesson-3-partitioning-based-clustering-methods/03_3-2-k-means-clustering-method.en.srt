1
00:00:06,090 --> 00:00:10,986
In this session we're going to introduce
the most popular clustering methods,

2
00:00:10,986 --> 00:00:13,390
the K-Means clustering method.

3
00:00:13,390 --> 00:00:18,850
So you may wonder, who first proposed
this K-Means clustering method?

4
00:00:18,850 --> 00:00:22,860
Some people refer to MacQueen
because he published a paper on

5
00:00:22,860 --> 00:00:27,160
the K-Means Clustering algorithm in 1967.

6
00:00:27,160 --> 00:00:32,320
But, some other people said,
Lloyd actually internally,

7
00:00:32,320 --> 00:00:37,370
in a company, published the,
the one in the internal report in 1957.

8
00:00:37,370 --> 00:00:41,520
Since the publication was
internally published mo,

9
00:00:41,520 --> 00:00:43,540
most people may not even know it.

10
00:00:43,540 --> 00:00:47,570
So, he republished this
one in a journal in 1982.

11
00:00:47,570 --> 00:00:51,490
That's the reason some people say
Lloyd first got this algorithm.

12
00:00:52,660 --> 00:00:57,790
But no matter what, actually, the general
common thing for this algorithm is,

13
00:00:57,790 --> 00:01:03,470
they consider every center is represented
by the center of the cluster,

14
00:01:03,470 --> 00:01:05,602
or we can say, centroid.

15
00:01:05,602 --> 00:01:09,830
The K-Means Algorithm essentially
can be outlined as below.

16
00:01:09,830 --> 00:01:12,790
It means, given K, the number of clusters,

17
00:01:12,790 --> 00:01:17,870
first we can select the K
points as initial centroid.

18
00:01:17,870 --> 00:01:22,020
Of course, you can randomly select it,
that's what our original K means,

19
00:01:22,020 --> 00:01:27,290
that, or you may have some small way
to select it we'll introduce later.

20
00:01:27,290 --> 00:01:29,930
Then, we can put this one on repeat and
here,

21
00:01:29,930 --> 00:01:34,110
look, there is once we
select the K centroid,

22
00:01:34,110 --> 00:01:40,210
we can form K clusters by assigning
each point to it's closest centroid.

23
00:01:40,210 --> 00:01:45,020
Once assigning this,
we may need to recompute the centroid

24
00:01:45,020 --> 00:01:50,440
because the initial randomly selected
centroid may not be a very good one.

25
00:01:50,440 --> 00:01:56,060
So we recompute the centroid that
means the mean point of each cluster.

26
00:01:56,060 --> 00:01:59,580
Once you recompute the centroid,

27
00:01:59,580 --> 00:02:04,720
some object initially assigned
to say centroid A now

28
00:02:04,720 --> 00:02:10,140
because of all the centroid changed this
point may be even closer to centroid B.

29
00:02:10,140 --> 00:02:11,310
Okay?

30
00:02:11,310 --> 00:02:16,110
That means we need to go
back to form K clusters

31
00:02:16,110 --> 00:02:20,970
by assigning or re-assigning each
point with its closest centroid now.

32
00:02:20,970 --> 00:02:22,080
Okay?

33
00:02:22,080 --> 00:02:24,580
Then we need to recompute
the new centroid.

34
00:02:25,590 --> 00:02:28,990
That's why we need to get
into this repeat until loop,

35
00:02:28,990 --> 00:02:32,180
until some convergence criterion is met.

36
00:02:33,810 --> 00:02:38,490
There are different kinds of
measures that can be used for

37
00:02:38,490 --> 00:02:44,246
calculating the distance and
assigning the, to its closest centroid.

38
00:02:45,460 --> 00:02:51,520
We can use Manhattan distance, L1 norm,
or Euclidean distance, L2 norm,

39
00:02:51,520 --> 00:02:56,410
but often, our initial algorithm
was using this Euclidean distance.

40
00:02:57,510 --> 00:02:59,290
We can even use Cosine similarity.

41
00:03:00,530 --> 00:03:02,560
Now let's look example.

42
00:03:02,560 --> 00:03:07,500
How we can execute this
K-Means Clustering Algorithm.

43
00:03:07,500 --> 00:03:11,085
Suppose we got the into 2-D space.

44
00:03:11,085 --> 00:03:16,000
Initially there were black points
on the initial data point.

45
00:03:16,000 --> 00:03:20,060
Then we can based on the first
line of the algorithm,

46
00:03:20,060 --> 00:03:24,090
we can select the K points
as initial centroid.

47
00:03:24,090 --> 00:03:25,490
Now, K is two.

48
00:03:25,490 --> 00:03:28,510
We can select the two initial centroid.

49
00:03:29,990 --> 00:03:32,610
Once we select this initial centroid

50
00:03:32,610 --> 00:03:37,490
we can form K clusters by assigning
each point to it's closest centroid.

51
00:03:37,490 --> 00:03:44,330
Now you probably can see we can assign
these, these black points to either

52
00:03:44,330 --> 00:03:49,809
red centroid or to the blue centroid,
you can see that's a current assignment.

53
00:03:50,860 --> 00:03:55,580
Once you assign these to
form two clusters, okay,

54
00:03:55,580 --> 00:04:00,340
then we need to recompute the centroid,
okay, where you see, you can

55
00:04:00,340 --> 00:04:05,060
recompute the centroid once you recompute
this red centroid mode down here, okay?

56
00:04:06,140 --> 00:04:10,940
Then this,
blue one actually is also moved, okay.

57
00:04:10,940 --> 00:04:16,090
Once we recompute the centroid,
we may need to

58
00:04:16,090 --> 00:04:21,870
go back to reassign these
point to its closest centroid.

59
00:04:21,870 --> 00:04:26,290
In this case you probably can see,
the closest centroid, like this,

60
00:04:26,290 --> 00:04:29,720
blue points assigned to the red ones, and

61
00:04:29,720 --> 00:04:34,560
these red, this red one assigned
to the blue side of the cluster.

62
00:04:34,560 --> 00:04:37,710
Okay?
Then we can calculate the centroid again.

63
00:04:37,710 --> 00:04:38,690
Okay?

64
00:04:38,690 --> 00:04:43,170
So this repeat the loop here
until it becomes stable or

65
00:04:43,170 --> 00:04:49,380
until the, the difference the different
assignment becomes really, really small.

66
00:04:49,380 --> 00:04:52,400
Now the first important
thing we should know is,

67
00:04:52,400 --> 00:04:56,180
this actually is a pretty efficient
algorithm for clustering.

68
00:04:56,180 --> 00:05:00,550
Because if we're told
we'll have n objects,

69
00:05:00,550 --> 00:05:02,880
we will want to find a K clusters.

70
00:05:02,880 --> 00:05:06,200
Suppose we get a t iteration
it becomes stable.

71
00:05:06,200 --> 00:05:11,320
You probably can see the computational
complexity is, every time every

72
00:05:11,320 --> 00:05:17,170
object that you need to see which
cluster is it belongs to, so this time,

73
00:05:17,170 --> 00:05:24,960
you're assigning to the corresponding K
centroid, so that's why it's n times K.

74
00:05:24,960 --> 00:05:27,500
In total, you have t iterations,

75
00:05:27,500 --> 00:05:33,660
that's why the computational complexity
is big O of t times K times n.

76
00:05:33,660 --> 00:05:37,000
usually K and t, number of cluster and

77
00:05:37,000 --> 00:05:41,930
number of iterations,
is far smaller than number of objects.

78
00:05:41,930 --> 00:05:48,080
That's why if you give me n,
supposed quite big number of objects,

79
00:05:48,080 --> 00:05:53,080
this complexity essentially
is a linear to the size of n.

80
00:05:53,080 --> 00:05:56,881
The number of objects,
that's efficient algorithms.

81
00:05:56,881 --> 00:06:03,400
However, K-means clustering often
may terminate at a local optimal.

82
00:06:04,660 --> 00:06:09,710
Then, the initialization becomes important
if you want to find a high quality

83
00:06:09,710 --> 00:06:10,670
Clusters.

84
00:06:10,670 --> 00:06:15,810
We need to have a smart initialization or
we need to randomly initialize this

85
00:06:15,810 --> 00:06:19,970
many times,
try to find a good clustering green dots.

86
00:06:21,650 --> 00:06:25,600
Another important thing is how to
specify K, the number of clusters.

87
00:06:26,720 --> 00:06:31,870
We need to specify this in a,
advance for the K means algorithm.

88
00:06:33,440 --> 00:06:36,680
There are many ways to
automatically determine the best K.

89
00:06:36,680 --> 00:06:41,480
There are some research papers, there
are lots of efforts contribute to this.

90
00:06:41,480 --> 00:06:45,390
In practice you also can

91
00:06:45,390 --> 00:06:50,480
run a range of values as a K then
select which one is the best.

92
00:06:50,480 --> 00:06:57,930
Then for K-Means messrs it's quite
sensitive to the noise data and outliers.

93
00:06:57,930 --> 00:07:00,928
So there are variations like K-medians, or

94
00:07:00,928 --> 00:07:05,630
K-medoids algorithm we try to overcome
this outlier noise data problem.

95
00:07:07,620 --> 00:07:13,810
Another thing is K-means actually works
only to objects in the continuous.

96
00:07:13,810 --> 00:07:18,350
That means numerical data in the,
in the, in dimensional space.

97
00:07:18,350 --> 00:07:22,160
How to handle clusters
on the categorical data?

98
00:07:22,160 --> 00:07:26,030
Actually, people invented something
called K-modes algorithms.

99
00:07:27,620 --> 00:07:32,140
Another important thing we should know is,
because we use the sum of the square

100
00:07:32,140 --> 00:07:37,080
distance, it then,
it is not a good idea to try to find

101
00:07:37,080 --> 00:07:41,920
a clusters with non-convex shapes or
non-circular shapes.

102
00:07:41,920 --> 00:07:43,550
Okay.
What you find is something

103
00:07:43,550 --> 00:07:44,430
closer to a ball.

104
00:07:46,310 --> 00:07:48,600
So we can use density based clustering or

105
00:07:48,600 --> 00:07:55,090
kernel K-means algorithm to do if
we get into a non-convex shapes.

106
00:07:55,090 --> 00:07:57,518
We're going to introduce
this algorithms later.

107
00:07:57,518 --> 00:08:02,250
So, because K-means

108
00:08:02,250 --> 00:08:06,850
came quite early and very popular used,
there are lots of studies

109
00:08:06,850 --> 00:08:11,290
to work out the different
variations of the K-means method.

110
00:08:11,290 --> 00:08:14,020
For example,
how to choose initial centroid.

111
00:08:15,676 --> 00:08:22,210
There are methods called K-means++,
Intelligent K-Means, or Genetic K-Means.

112
00:08:22,210 --> 00:08:26,660
We are going to introduce
K-Means++ in the next discussion.

113
00:08:28,160 --> 00:08:32,810
Another aspect is how to choose
different representative prototypes

114
00:08:32,810 --> 00:08:34,330
for the clusters.

115
00:08:34,330 --> 00:08:37,900
That means we may not use the mean point,

116
00:08:37,900 --> 00:08:42,730
then we may use K-Medoids or
K-Medians or K-Modes.

117
00:08:42,730 --> 00:08:45,929
We're going to introduce this
in the subsequent discussion.

118
00:08:48,120 --> 00:08:52,240
Another thing is how to apply
feature transformation techniques.

119
00:08:52,240 --> 00:08:58,360
So, we may be able to cluster things
even if they are not in convex shape.

120
00:08:58,360 --> 00:09:04,740
For example, they are massive developed
card, weighted K-Means or Kernel K-Means.

121
00:09:04,740 --> 00:09:09,896
We are going to introduce
the Kernel K-Means method

122
00:09:09,896 --> 00:09:19,896
[MUSIC]