1
00:00:00,000 --> 00:00:02,144
[MUSIC]

2
00:00:05,758 --> 00:00:11,491
Now, I'm going to introduce you
another interesting K partitioning

3
00:00:11,491 --> 00:00:16,660
clustering method called
the K-Medoids Clustering Method.

4
00:00:16,660 --> 00:00:20,930
Why do we need to study
K-Medoids Clustering Method?

5
00:00:20,930 --> 00:00:25,780
Just because the K-Means algorithm
is sensitive to outliers.

6
00:00:25,780 --> 00:00:28,510
Because a mean is
sensitive to the outliers.

7
00:00:30,130 --> 00:00:34,710
Just give you a simple example,
if you look at a company's salary,

8
00:00:34,710 --> 00:00:38,950
if you adding another very high salary,

9
00:00:38,950 --> 00:00:43,050
the average salary of the whole
company shifts quite a lot.

10
00:00:44,700 --> 00:00:49,490
So, let's look at the K-Medoids,
what is K-Medoids?

11
00:00:49,490 --> 00:00:53,490
That means instead of taking the mean
value of object in a cluster,

12
00:00:54,640 --> 00:00:58,660
as our centroid,
we actually can use the most

13
00:00:58,660 --> 00:01:02,850
centrally located object in the cluster or
we call medoids.

14
00:01:04,810 --> 00:01:09,390
That means the K-Medoids clustering
algorithm can go in a similar way,

15
00:01:09,390 --> 00:01:14,860
as we first select the K points as
initial representative objects,

16
00:01:14,860 --> 00:01:17,350
that means initial K-Medoids.

17
00:01:17,350 --> 00:01:23,730
The difference between K-Means is K-Means
can select the K virtual centroid.

18
00:01:23,730 --> 00:01:28,350
But this one should be the K
representative of real objects.

19
00:01:29,720 --> 00:01:32,720
Then we put this one into repeat loop.

20
00:01:32,720 --> 00:01:34,740
We can assign, similarly,

21
00:01:34,740 --> 00:01:40,500
we assign each point to the cluster
with the closest medoid.

22
00:01:41,830 --> 00:01:46,798
Then, we can randomly select
a non-representative object,

23
00:01:46,798 --> 00:01:50,690
suppose it's o sub i, or see whether we

24
00:01:51,910 --> 00:01:57,320
use o sub i to replace one medoid, m.

25
00:01:57,320 --> 00:02:01,160
Whether it will improve
the quality of the class ring,

26
00:02:01,160 --> 00:02:04,540
that means the total cost
of swapping is negative.

27
00:02:04,540 --> 00:02:11,510
Simply says, it was sloppy,
we can reduce some of the square arrows.

28
00:02:11,510 --> 00:02:19,050
Then, we are going to swap m with object
oi to form the new set of medoids.

29
00:02:19,050 --> 00:02:22,100
Then we need to redo the assignment.

30
00:02:22,100 --> 00:02:28,240
And here, this process goes, and here,
the convergence criteria is satisfied.

31
00:02:28,240 --> 00:02:36,210
Now, we'll see a small example how
a typical K-Medoids Algorithm is exacted.

32
00:02:36,210 --> 00:02:39,030
We look at the PAM, as an example.

33
00:02:39,030 --> 00:02:44,910
Suppose we are given ten small number
of points in this small graph.

34
00:02:45,950 --> 00:02:50,460
In this 2D space,
we want to find the two clusters.

35
00:02:52,240 --> 00:02:56,490
At the very beginning,
we arbitrarily choose k objects here.

36
00:02:56,490 --> 00:03:01,010
We choose two objects as initial medoids.

37
00:03:01,010 --> 00:03:05,850
Then we will find the clusters
of these medoids, as follows.

38
00:03:07,430 --> 00:03:08,030
Okay.

39
00:03:08,030 --> 00:03:16,328
Then, we will see whether we can randomly
choose another object like O random.

40
00:03:16,328 --> 00:03:20,310
Say these non-medoic object.

41
00:03:20,310 --> 00:03:23,320
We want to see whether it
could become a medoid.

42
00:03:23,320 --> 00:03:26,870
If it would reduce the total cost.

43
00:03:26,870 --> 00:03:30,450
Always say we get a better SSE.

44
00:03:30,450 --> 00:03:34,954
And in this case,
suppose we choose one here, but

45
00:03:34,954 --> 00:03:40,760
we found it does not really reduce any,
the total SSE.

46
00:03:40,760 --> 00:03:43,830
Then we actually can get another one.

47
00:03:43,830 --> 00:03:46,410
Like we get this orange one.

48
00:03:46,410 --> 00:03:50,950
Then we look at the cluster we can form.

49
00:03:50,950 --> 00:03:54,650
We know this one will
reduce the total SSE.

50
00:03:54,650 --> 00:03:58,220
That simply said the quality
of the cluster is improved.

51
00:03:58,220 --> 00:03:59,780
Then we will do the swapping.

52
00:04:01,530 --> 00:04:06,600
So this essentially is
listed here as we initially,

53
00:04:06,600 --> 00:04:10,680
we select initial K medoids randomly.

54
00:04:10,680 --> 00:04:13,430
Then, we will do object reassignment.

55
00:04:13,430 --> 00:04:18,455
Then we try to swap medoid,
m, with the random

56
00:04:18,455 --> 00:04:23,960
non-medoid object, o sub i,
if it improves the clustering quality.

57
00:04:23,960 --> 00:04:30,170
Then we'll do it again and again until
the convergence criterion is satisfied.

58
00:04:30,170 --> 00:04:35,118
So this is just a simple
execution to illustrate the ideas

59
00:04:35,118 --> 00:04:40,560
of this K-Medoids, how it is executing.

60
00:04:40,560 --> 00:04:45,020
Now we see these K-Medoids clustering
essentially is try to find the k

61
00:04:45,020 --> 00:04:47,840
representative objects, so
medoids in the clusters.

62
00:04:49,990 --> 00:04:56,945
And, the typical arrow is in PAM,
called Partitioning Around the Medoids,

63
00:04:56,945 --> 00:05:01,760
was developed in 1987 by
Kaufmann & Rousseeuw,

64
00:05:01,760 --> 00:05:06,170
starting from initial sets of medoids.

65
00:05:06,170 --> 00:05:11,133
Then we iteratively replace one of
the medoids by one of the non-medoids,

66
00:05:11,133 --> 00:05:15,330
if such a swapping improve the total
sum of the squared errors.

67
00:05:16,480 --> 00:05:18,380
That is the total quality of the cluster.

68
00:05:19,830 --> 00:05:23,129
This method works effectively for
small data sets.

69
00:05:24,440 --> 00:05:30,120
Because we can keep trying different
swapping, but it cannot scale well,

70
00:05:30,120 --> 00:05:34,390
because the computational
complexity is quite high.

71
00:05:34,390 --> 00:05:39,250
If we look at it into detail, actually,
this computational complexity for

72
00:05:39,250 --> 00:05:44,420
every swapping, actually, it's to
the square of the number of points.

73
00:05:44,420 --> 00:05:45,870
That's quite expensive.

74
00:05:47,540 --> 00:05:51,260
So, how to improve its efficiency.

75
00:05:51,260 --> 00:05:56,090
There's one proposal by same
authors in 1990 called CLARA.

76
00:05:57,280 --> 00:06:00,720
Essentially, it's PAM on samples.

77
00:06:00,720 --> 00:06:06,340
That means, instead of using the whole
points, we choose a sample, s.

78
00:06:06,340 --> 00:06:08,060
S is the sample size.

79
00:06:08,060 --> 00:06:12,110
Then, the computational complexity,
this square, actually,

80
00:06:12,110 --> 00:06:14,700
comes down to the size of the sample.

81
00:06:15,860 --> 00:06:19,850
However, if the sample,
initial sample selection, is no good,

82
00:06:19,850 --> 00:06:22,910
the final classroom quality could be poor.

83
00:06:24,150 --> 00:06:30,307
Then in 1994, there's another
algorithm called CLARANS proposed,

84
00:06:30,307 --> 00:06:34,620
as every iteration we do
randomized re-sampling.

85
00:06:34,620 --> 00:06:38,180
That means,
we do not keep exact the same sample.

86
00:06:38,180 --> 00:06:41,946
We do randomized re-sampling, that, or

87
00:06:41,946 --> 00:06:46,766
ensure the efficiency and
the quality of clustering.

88
00:06:46,766 --> 00:06:56,766
[MUSIC]