1
00:00:00,008 --> 00:00:03,877
[SOUND] In this session,

2
00:00:03,877 --> 00:00:09,488
we examine more detail on divisive

3
00:00:09,488 --> 00:00:13,748
clustering algorithms.

4
00:00:13,748 --> 00:00:18,800
We already introduced a general
concept of divisive clustering.

5
00:00:18,800 --> 00:00:24,860
Essentially is,
we start from a big single macro-cluster,

6
00:00:24,860 --> 00:00:30,770
and we try to find how to split
them into two smaller clusters.

7
00:00:30,770 --> 00:00:36,510
We continue doing this, finally, every
single node become a singleton cluster.

8
00:00:36,510 --> 00:00:37,260
Okay.

9
00:00:37,260 --> 00:00:42,180
So, this method,
actually described in Kaufmann and

10
00:00:42,180 --> 00:00:47,265
Rousseeuew's 1990 book,
called DIANA or Divisive Analysis,

11
00:00:47,265 --> 00:00:53,790
is also implemented in some
statistic packages, such as Splus.

12
00:00:53,790 --> 00:00:57,960
This is essentially inverse of AGNES.

13
00:00:57,960 --> 00:01:03,050
That means you start from bigger cluster,
you start splitting them continuously,

14
00:01:03,050 --> 00:01:09,120
recursively, finally you'll find that
each one is a single, singleton cluster.

15
00:01:09,120 --> 00:01:11,940
Divisive clustering is
a top down approach,

16
00:01:11,940 --> 00:01:15,060
because you start from all
the points as one cluster, then

17
00:01:15,060 --> 00:01:19,080
you'll recursively split the high level
cluster to build the dendogram, okay?

18
00:01:21,170 --> 00:01:23,930
It can be considered as a global approach.

19
00:01:23,930 --> 00:01:27,230
It is more efficient,
when compared to agglomerative clustering.

20
00:01:27,230 --> 00:01:29,040
Because you don't have much choice.

21
00:01:29,040 --> 00:01:30,450
You just decide every point.

22
00:01:30,450 --> 00:01:33,800
You just decide how to split,
and you do it recursively.

23
00:01:35,050 --> 00:01:38,950
But we will see how
which cluster to split.

24
00:01:38,950 --> 00:01:43,500
If you have multiple clusters now,
you can check the sum of

25
00:01:43,500 --> 00:01:48,808
the squared errors of the clusters,
and see which one is the largest one.

26
00:01:48,808 --> 00:01:54,200
That you, then simply say, this one is not
a very good, you, you want to split them.

27
00:01:54,200 --> 00:01:57,840
You want to reduce the sum of
the squared error, overall.

28
00:02:00,040 --> 00:02:04,990
Then, once you decide to split this
cluster, then, how do you split it?

29
00:02:06,250 --> 00:02:08,946
Okay.
Actually, you may try to split,

30
00:02:08,946 --> 00:02:10,672
in many ways.

31
00:02:10,672 --> 00:02:16,290
For that you can use Ward's criterion we
just introduced, to see which one, which

32
00:02:16,290 --> 00:02:22,945
split, give you the greatest reduction
in the difference of the SSE criterion.

33
00:02:22,945 --> 00:02:25,075
Then you choose that one to split.

34
00:02:26,205 --> 00:02:31,055
For categoric data we can use
Gini-index in a similar way.

35
00:02:32,625 --> 00:02:37,105
Then, the third issue is
how to handle the noise.

36
00:02:37,105 --> 00:02:40,055
Essentially, when you decide to split,

37
00:02:40,055 --> 00:02:45,030
you don't want to split into too
small cluster, mainly contain noises.

38
00:02:45,030 --> 00:02:50,790
That means you need to use a threshold to
determine the termination criterion, okay?

39
00:02:50,790 --> 00:02:56,989
So, these are the major issues,
that design divisive clustering.

40
00:02:56,989 --> 00:03:06,989
[MUSIC]