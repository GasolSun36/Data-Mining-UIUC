1
00:00:05,958 --> 00:00:08,445
In the last session of this lecture,

2
00:00:08,445 --> 00:00:13,110
we are going to introduce
probabilistic hierarchical clustering.

3
00:00:14,420 --> 00:00:17,620
What is probabilistic
hierarchical clustering?

4
00:00:17,620 --> 00:00:23,220
Let's first examine the problems of
algorithmic hierarchical clustering.

5
00:00:25,050 --> 00:00:29,990
The first problem for
algorithmic hierarchical clustering is it

6
00:00:29,990 --> 00:00:34,530
is nontrivial to choose
a good distance measure.

7
00:00:34,530 --> 00:00:38,890
For example,
we already see the single link,

8
00:00:38,890 --> 00:00:43,440
the complete link,
the average link and the central link.

9
00:00:43,440 --> 00:00:45,860
They're all different measures, but

10
00:00:45,860 --> 00:00:50,490
it's hard to choose a good distance
measure based on the application problems.

11
00:00:51,780 --> 00:00:57,080
The second problem is it is hard to
handle missing attribute values because

12
00:00:57,080 --> 00:01:03,108
when the attribute value is missing, it
may impact on the quality of clustering.

13
00:01:03,108 --> 00:01:09,060
The third one is the optimization goal of
algorithmic hierarchical clustering is

14
00:01:09,060 --> 00:01:15,850
not so clear because it is a heuristic
algorithm, it more relies on local search.

15
00:01:15,850 --> 00:01:21,120
Now, there is another approach called
probabilistic hierarchical clustering.

16
00:01:22,260 --> 00:01:27,630
This method essentially uses
probabilistic models to measure

17
00:01:27,630 --> 00:01:29,850
distance between clusters.

18
00:01:31,310 --> 00:01:35,495
It is largely a generative
model which means it

19
00:01:35,495 --> 00:01:40,765
regards the set of data objects
to be clustered as a sample

20
00:01:40,765 --> 00:01:44,765
of the underlying data generation
mechanism to be analyzed.

21
00:01:46,075 --> 00:01:51,025
And then you finally find the parameter,
find out how they generate this data set.

22
00:01:52,690 --> 00:01:54,390
It is easy to understand.

23
00:01:54,390 --> 00:01:59,550
[COUGH] It has the same efficiency as
algorithmic agglomerative clustering

24
00:01:59,550 --> 00:02:04,440
method and, also,
it can handle partially observed data.

25
00:02:05,830 --> 00:02:10,160
In practice, we can assume the generative

26
00:02:10,160 --> 00:02:15,030
models adopt common
distribution functions.

27
00:02:15,030 --> 00:02:19,830
For example, we can assume it's
a Gaussian distribution or

28
00:02:19,830 --> 00:02:24,080
Bernoulli distribution, and
governed by a set of parameters.

29
00:02:25,730 --> 00:02:27,000
Let's look at the generative model.

30
00:02:28,640 --> 00:02:35,242
In the generative model, we can assume,
given a set of one-dimensional points,

31
00:02:35,242 --> 00:02:41,206
X, we can assume they are generated
by a Gaussian distribution like this.

32
00:02:41,206 --> 00:02:44,950
That means you're essentially
using different mu and sigma.

33
00:02:44,950 --> 00:02:47,948
You may generate the distribution
according to the typical,

34
00:02:47,948 --> 00:02:50,780
normal distribution or
Gaussian distributing formula.

35
00:02:52,520 --> 00:02:58,050
Then, the probability a single point,
x sub i, which is

36
00:02:58,050 --> 00:03:04,060
one point in the set of X,
is generated by the model.

37
00:03:04,060 --> 00:03:07,350
Essentially, it's what's
the probability to generate

38
00:03:07,350 --> 00:03:12,170
x sub i under the condition of mu and
sigma is based on this formula.

39
00:03:13,330 --> 00:03:19,360
Then for the likelihood that
this whole set of X generated,

40
00:03:19,360 --> 00:03:23,101
essentially, is the likelihood for

41
00:03:23,101 --> 00:03:29,026
the normal distribution to
generate this set of x points.

42
00:03:29,026 --> 00:03:34,000
Essentially, it's the probability
of a set of points, x,

43
00:03:34,000 --> 00:03:37,730
under the condition of mu and
sigma squared.

44
00:03:37,730 --> 00:03:42,200
For i from 1 to N,
all of these probability multiplying

45
00:03:42,200 --> 00:03:46,080
together we get the probability
generated at the data set, x.

46
00:03:47,900 --> 00:03:52,344
Then the task of learning this
generative model, essentially,

47
00:03:52,344 --> 00:03:55,593
we try to find the parameters,
a mu and a sigma.

48
00:03:55,593 --> 00:03:59,465
So for this particular distribution,

49
00:03:59,465 --> 00:04:04,626
we try to find the maximum
likelihood this set of x has

50
00:04:04,626 --> 00:04:09,570
generated, according to
these two parameters.

51
00:04:11,310 --> 00:04:15,990
So the Gaussian distribution usually,
I think most people know, for

52
00:04:15,990 --> 00:04:20,910
example, for a bean machine,
if you drop a ball with pins,

53
00:04:20,910 --> 00:04:25,510
if you use many, many balls,

54
00:04:25,510 --> 00:04:30,380
finally you will find the distribution
is a Gaussian distribution.

55
00:04:30,380 --> 00:04:34,960
Then if we have may points,
we can assume these points are actually

56
00:04:34,960 --> 00:04:38,270
generated by the Gaussian
distribution like this.

57
00:04:39,295 --> 00:04:41,860
Actually, for a one-dimensional Gaussian,

58
00:04:41,860 --> 00:04:46,960
this different mu, different in mean and
different of variance,

59
00:04:46,960 --> 00:04:49,780
you probably can see that it
generate different distribution.

60
00:04:51,080 --> 00:04:55,984
For a two-dimensional Gaussian,
an image generated in 3-D space,

61
00:04:55,984 --> 00:05:00,900
you probably can clearly see
the distribution of this 2-D Gaussian.

62
00:05:03,430 --> 00:05:07,224
Then a probabilistic hierarchical
clustering algorithm,

63
00:05:07,224 --> 00:05:12,780
essentially, is suppose
we regard the partition,

64
00:05:12,780 --> 00:05:19,230
the whole set of points,
into m clusters, C sub one to C sub m.

65
00:05:19,230 --> 00:05:24,320
Then the quality of the clustering
can be measured using this quality

66
00:05:24,320 --> 00:05:29,329
function that's the product
of all those probabilities.

67
00:05:31,840 --> 00:05:36,768
Then if we want to merge two clusters,
C sub j1 and

68
00:05:36,768 --> 00:05:42,130
C sub j2, then the change in
quality of the overall clustering

69
00:05:42,130 --> 00:05:45,850
is one criterion.

70
00:05:45,850 --> 00:05:51,710
So you probably can see this is the merged
cluster, your generating new cluster,

71
00:05:51,710 --> 00:05:56,680
and this originally two cluster disappear
from the original set of clusters.

72
00:05:56,680 --> 00:06:01,860
So that's the new clustering, that's the
original clustering, because if you merge

73
00:06:01,860 --> 00:06:07,490
this, essentially, the sum of
the square arrows will be increasing.

74
00:06:07,490 --> 00:06:10,940
So that's the reason you're trying
to minimize that increasing.

75
00:06:10,940 --> 00:06:15,050
That's the probability of P(C sub i).

76
00:06:15,050 --> 00:06:19,820
Now, you get the probability of
these two merged together, okay.

77
00:06:19,820 --> 00:06:25,370
The you have to minus the original
probability, i from 1 to m.

78
00:06:25,370 --> 00:06:32,220
Then the distance between clusters C1 and
C2 is, this is the probability of C1 and

79
00:06:32,220 --> 00:06:37,050
C2 together as one cluster,
these are the independent clusters.

80
00:06:37,050 --> 00:06:41,590
If this distance is less than zero,
we should merge these two clusters.

81
00:06:41,590 --> 00:06:44,502
So that's the idea, or the essence,

82
00:06:44,502 --> 00:06:49,144
of the probabilistic hierarchical
clustering algorithm.

83
00:06:49,144 --> 00:06:59,144
[MUSIC]