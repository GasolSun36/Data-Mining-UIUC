[声音]
翻译: PeiyS |审阅:
Coursera Global Translator Community 我们可以计算这个最大估计值 通过使用最大期望算法 所以在E-Step中，我们现在需要介绍更多的隐藏变量 因为我们有更多的主题，所以现在我们的隐藏变量z 是一个可取超值过两个的主题指标 亦即我们将取k+1个值 用b表示背景 一旦锁定，即用来代表所有的k个主题 所以，正如你所回忆的，现在E-Step是在增强数据 通过预测隐藏变量的值 所以我们将要来对一个单词进行预测 看这个单词是否来自这些k+1个分布之一 这个公式使我们可以预测 文档d中的单词w产生于主题z中的j的概率 下面的这个是预测概率 关于这个单词产生于这个背景 请注意我们在这里使用文档d来为单词做索引 为什么呢？ 因为不管一个单词是否来自于一个特定的主题 实际上它都依赖于一个文档 你知道为什么吗？ 对，这是通过π的 这些π与每个文档相关联 每个文档都可以有潜在的不同的π值 然后这些π值将会影响我们的预测 所以这些π值在这里 而这随文档而定 并且这可能给出了不同的猜想 关于在不同文件中的一个单词， 而这也是理想的 在这两种情况下我们使用的都是贝叶斯法则正如我之前所解释的 基本地评估从每个分组中产生单词的可能性 这是标准化的 那么m-step是怎么样的呢 我们或许还记得m-step是我们利用推断z值 去分裂计数 然后收集正确的计数来重新估计参数 所以在这种情况下我们可以重新估计我们的概率的范围 而且这是以收集文件中所有的单词为基础再重新估算 这就是为什么我们要在文档中统计单词的数量 和对所有单词求和 这个 比如主题θ的主体j 而这部分是我们从每一步的猜测 这可以告诉我们这个单词确实来自于主题θ的主体j的盖里 而且当我们将它们乘在一起 我们就得到了属于主题θ主体j的折扣计数 而当我们将所有的主题标准化 我们可以得到所有主题的分布来表示概率范围 同样地底部这个是一个主题内的一个单词的估计概率 而且在这种情况下我们使用精确相同的计数，你可以看到这是 相同的折扣计数，这告诉我们 应该怎样在上面的主体z中分配这个单词，但是这个标准化程序是不同的 因为在这种情况下我们感兴趣的是单词分布所以 我们只是简单的将所有单词标准化 这是不同的，作为对照我们标准化所有主题 取两者进行对比应该是有益的 这为我们提供了不同的分布情况 而且这告诉我们应该如何改进参数 正如我之前解释过的，在这两个公式中我们都有一个最大的 估计值是以主题θ的主体j的分配单词计数为基础 现在这个现象实际上在所有的最大期望算法中是很常见的 在m步骤中你 基于E步骤的结果的事件，然后你只需要收集相关的计数 关于特定的参数并进行具有代表性的估算和标准化 π 实际上只要持续解释各种事件然后对他们进行标准化 并且当我们这样想的时候 我们对呈现最大期望算法也会有一个更加简明的方法 它实际上帮助我们更好的理解这些公式 所以我将对这方面进行更细节的复习 因此作为一个运算法则，我们首先随机地初始化所有的未知参数 好的 所以对我们来说我们感兴趣的是那些范围参数和π 还有单词分布等等，而我们只是随机的对他们进行标准化 这是初始化步骤然后我们重复进行直到概率收敛 现在我们怎么知道概率是否会收敛呢 我们可以在每一步骤的时候进行可能性计算 并且将现在的可能性和之前的可能性进行比较 如果不发生很大的变化那么我们将要停止 所以在每一步中我们将要进行E步骤和M步骤 在E步骤中我们将要增强数据 通过预测隐藏变量 在这种情况下隐藏变量z和小变量d以及w 表示子集d中的单词w是否来自于一个主题或者背景 并且如果他是来自于某一个主题具体是哪个主题 所以如果你们看一下E步骤的公式 本质上我们是在对这些计数进行标准化 基于这样一个分布观察到某个词的概率 所以你可以看到基本上关于单词的预测 来自主题z子集j是基于 选择主题θ子集j作为单词分布来产生单词的可能性 乘以在那个分布里观察到单词的概率 并且我讲过这是成比例的因为在最大期望算法的应用中 你可以持续记述这个数量的计数 最后你只需要对其进行标准化 所以这里的标准化过程是针对所有主题的 然后你将会得到一个概率 现在在M步骤中我们做一样的事情，我们将要收集这些 并分配到每一个主题 并且我们在主题之中分离单词 然后我们将会对它们进行不同方式的标准化过程 来获得真实的估计址 因此举个例子我们可以对这之中所有的主题进行标准化来得到π的重新估计值 覆盖范围 或者我们可以对所有的单词进行重新标准化 这将给予我们一个单词分布 所以以这种方式来思考算法是很有用的因为当我们应用它时 你可以仅仅是使用这些变量但是在每一种情况下持续追踪这些数量 然后你只需要对这些标量进行表昏花来是他们呈现分布 现在我没有给这个加约束条件 因为我有意留下这个作为给你们的一个练习 你可以看到这个式子的正规化子是什么 这是一个稍有不同的公式但是本质上 与你在这里看到的这个是一样的 所以一般来说在最大期望算法的构想中你会看到你积累计数 各种计数然后你对他们进行标准化 所以作为总结我们介绍了概率潜在语义分析模型 这是一个代表k个主题的k个一元语言模型的混合模型 而且我们还增加了一个预先确定的背景语言模型 来辅助发现差别主题 因为这种背景语言模型可以辅助吸引常用术语 而且我们选择最大估计值 使我们可以从文本数据中发现主题知识 在这种情况下PLSA可以使我们发现两件事，一是k个单词分布 每一个分布代表一个主题 另一个是在每一个文档中每一个主题的一部分 并且，文档中关于主题覆盖性的这样详细的描述特性 可以启用大量的图片分析 比如，我们可以聚集一个特定的时间范围内的文档 来评估一定时间范围内一个特定主题的覆盖率。 这样可以让我们生成主题的时间链 我们也可以聚集文件里涉及的与一个特定作者相关的所有主题 然后我们可以将这位作者所写的主题归类，等等 并且除了这一点，我们也可以聚类术语和集群文件 实际上，每一个主题都可以被视为一个集群 所以我们已经有了术语集群 在较高的概率下，这些单词可以被认为是 属于同一个由主题代表的集群 相似地，文档也可以以同样的方法被聚类 我们可以指派一篇文档给一个特定的主题集群 在文档中涵盖的最多的这个主题集群 所以请记住，π表明每个主题在文档中的被包含程度 我们可以分配文档给有最高的π值的主题集群 而且通常这项技术有许多有效的实际应用 [背景音乐]
翻译: MingShi |审阅:
Coursera Global Translator Community