这节课 继续讨论用于文本聚类的生成概率模型 这节课继续讨论文本聚类 主要讲的是生成概率模型 这个幻灯片是你们之前看过的 我们写出的针对一个有两个分布的文档的似然函数 作为用于文档聚类的双组成混合模型 这节课我们会把这个推广到包括K-聚类 现在请你看着这个公式 想一下如何推广 你会意识到我们只需要加上更多的项 就像我们在这里看到的 你可以加上更多的θ(theta)和θ的概率 以及从θ生成d的概率 这正好就是我们要用的 这也是文本聚类混合模型的通用表达方式 还会有更多的案例将在使用生成模型时遵循这些步骤 考虑我们的数据 在本案例中我们的数据是一组文档 文档用d下标i表示(di) 然后讨论其他的模型 考虑其他的建模 在这个案例里 我们设计了一个混合的K元语言模型 这个和主题模型有一点点不同 但参数是一样的 我们有一组θ来表示我们的 对应到K单词语言模型的分布 我们对每一个θi都有P(θi)作为 在k个分布中选择生成文档的概率 注意虽然我们的目标是找到聚类 但实际上我们已经用了一个更一般的表示 每个聚类概率的表达方式 稍后你会看到 这将允许我们把文档分配给 有最大概率生成该文档的聚类 因此 我们也能发掘一些其他的 有趣属性 稍后会看到 模型基本上做了如下假设 来生成一个文档 我们首先根据θi的概率选择一个θ 然后用这个分布生成文档中的所有词 注意一个重要部分 我们对文档中的所有单词应用该分布 这与主题模型非常不同 所以似然函数会像你在这里看到的 看看这里的公式 我们使用到了不同的记号 在等式的第二行 你将看到现在产出中的记号已经变为 使用词典中的唯一词 而不再是文档中的特定位置 所以从X替换为W 是记号的改变 这种改变让我们能更容易展示出公式的意思 在主题模型的介绍中你也见过这种改变 但它基本上依然是所有词的概率综合产出 因此 有了这个似然函数 我们现在讨论如何做参数预估 我们可以简单使用最大似然估计法 这是个做事的标准方式 现在对你们来说应该熟悉了 只是个不同的模型 在我们估计过参数后 我们该如何分配聚类到文档呢 让我们仔细看看这个情形 我们为这个混合模型重复一下参数 请考虑我们从估计这样一个模型能得到什么 我们能得到的信息比做聚类所需的要多 对不对 例如 θi 代表了聚类i的内容 这实际上是个副产品 可以帮助我们总结聚类是关于什么的 如果看看聚类或者词分布中的 排行头部词汇 我们可以得知聚类是关于什么的 p(θi)可以被解释为聚类的尺寸 因为它告诉我们该类用于生成文档的可能性 一个类越可能用于生成一个文档 可以假设该类的尺寸越大 注意和PLSA中的不同 θi的概率不依赖与d 你现在可能回想起在文档上选择的主题 实际上是依赖于d的 这意味着每个文档都可以有一个潜在的不同主题选择范围 而这里我们对所有文档都有一个一般的概率选择范围 当然 给出一个特定文档时我们依然需要 推断哪个主题更可能生成该文档 从这种意义上来说 我们还是可以有一个依赖于文档的聚类概率 现在让我们看关键的分配文档到聚类问题 或者说分配聚类到文档 这里设Cd为表示应当分配给d的聚类 Cd会被赋值为1到k的其中一个 你首先想到的 可能是对它做某种似然估计 对应到分配d与θi的主题对应的聚类 并且(该主题)最有可能用于生成d 这就意味这我们需要从那些分布中 选择一个给d有最大概率的 换句话说 我们看哪个分布的内容与d的匹配程度最好 这个符合直觉 但是这种做法 并没有考虑聚类的大小 这是我们可用的 更好的一个办法是结合先验知识使用似然估计 这个例子里先验知识就是p(θi) 结合 我们将使用基本公式来计算 给定条件d时θ的后验概率 如果我们基于这种后验概率来选择θ 我们就会得到这个幻灯片底部的公式 这个例子里 我们会选择有大值P(θi)的θ 这意味着大尺寸的聚类也有更大的概率生成d 所以我们会偏好的聚类会尺寸大并且 同时与文档(d)保持一致 这符合直觉因为 文档处在大聚类中的概率一般比在小聚类里小 这意味着当我们能估计此模型的参数时 我们就可以很容易地解决文档聚类问题 下一课我们会讨论 实际上如何计算模型的估计值 GTC字幕组翻译