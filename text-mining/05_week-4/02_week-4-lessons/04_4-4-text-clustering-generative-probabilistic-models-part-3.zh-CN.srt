1
00:00:00,012 --> 00:00:08,289
这节课

2
00:00:08,289 --> 00:00:12,379
是文本聚类的生成概率模型的延续讨论

3
00:00:14,210 --> 00:00:17,210
在这节课中我们将要完成对于生成概率模型的讨论

4
00:00:17,210 --> 00:00:19,630
为了文本交叉而生成概率模型

5
00:00:21,590 --> 00:00:26,635
所以这是大家之前看过的一张幻灯片这里我们展示了我们如何定义

6
00:00:26,635 --> 00:00:32,371
我们定义文本聚类的混合模型以及似然函数的样子

7
00:00:32,371 --> 00:00:36,879
而且我们还可以计算最大似然估计值

8
00:00:36,879 --> 00:00:39,186
来估测参数

9
00:00:39,186 --> 00:00:43,804
这节课中我们将要更多的讨论我们将如何更精确的

10
00:00:43,804 --> 00:00:46,569
计算最大似然估计值

11
00:00:46,569 --> 00:00:55,185
在大多数情况下最大期望算法可以被用来解决混合模型中的这个问题

12
00:00:55,185 --> 00:01:00,860
因此以下是这个文档聚类最大期望算法的详细内容

13
00:01:00,860 --> 00:01:04,450
现在如果你已经明白算法是怎么工作的

14
00:01:04,450 --> 00:01:09,490
像TRSA一样的主题模型并且我认为这会是非常相似的

15
00:01:09,490 --> 00:01:15,790
而我们只需要改变一点来适应这个新的混合模型

16
00:01:15,790 --> 00:01:22,784
所以正如你可能还记得最大期望算法开始于对所有参数进行的初始化

17
00:01:22,784 --> 00:01:27,140
所以这与之前所讲的主题模型是一样的

18
00:01:28,490 --> 00:01:33,340
然后我们将要重复知道可能性收敛并且

19
00:01:33,340 --> 00:01:37,310
在每一步我们将要做E步骤和M步骤

20
00:01:37,310 --> 00:01:38,060
在M步骤

21
00:01:38,060 --> 00:01:44,040
我们将要推测被用来产生每个文档的分布

22
00:01:44,040 --> 00:01:48,540
所以我必须引入一个隐藏变量Zd

23
00:01:48,540 --> 00:01:54,610
每个文档以及这个变量可以在1到k中取值

24
00:01:54,610 --> 00:01:56,910
代表k的不同分布

25
00:01:59,570 --> 00:02:04,140
更具体来说基本上我们将要运用基本规则来推测

26
00:02:04,140 --> 00:02:09,240
哪一个分布最有可能生成这个文档或者

27
00:02:09,240 --> 00:02:16,290
计算给定文档的分布的后验概率

28
00:02:17,390 --> 00:02:21,490
并且我们知道这与概率成正比

29
00:02:21,490 --> 00:02:26,880
选择这个θ的分布

30
00:02:26,880 --> 00:02:32,080
以及从这个分布中产生这整个文档的概率

31
00:02:32,080 --> 00:02:40,000
是你在这里看到的这个文档所有单词的概率的乘积

32
00:02:40,000 --> 00:02:43,980
现在

33
00:02:45,050 --> 00:02:50,300
这个概率的正规化子或者约束条件

34
00:02:50,300 --> 00:02:56,059
所以在这种情况下我们知道在这个约束下的概率

35
00:02:56,059 --> 00:03:02,173
在E步骤中是所有z的概率等于i并且之和必须是1

36
00:03:02,173 --> 00:03:06,658
因为文件必须都是精确的由这些k主题其中之一生成

37
00:03:06,658 --> 00:03:07,870
主题

38
00:03:07,870 --> 00:03:11,860
所以从他们其中每一个中生成的概率应该总和为1

39
00:03:11,860 --> 00:03:18,220
如果你知道这个约束那么你可以简单地计算这个分布

40
00:03:18,220 --> 00:03:24,350
只要你知道什么是成比例的

41
00:03:24,350 --> 00:03:29,379
所以只要你计算出这里你看到的这个结果然后你只要简单的正常化

42
00:03:31,220 --> 00:03:35,070
这些概率使它们在全部的主题中相加之和为1

43
00:03:35,070 --> 00:03:40,070
所以这是E步骤    E步骤之后我们想要了解哪一个分布是

44
00:03:40,070 --> 00:03:44,030
最可能生成这个文件d的以及哪一个最不可能

45
00:03:45,460 --> 00:03:49,988
然后在M步骤中我们将要重新估计所有的参数基于

46
00:03:49,988 --> 00:03:54,443
进一步的z值或者进一步关于哪一个分布

47
00:03:54,443 --> 00:03:57,089
已经被用来生成哪个文档

48
00:03:57,089 --> 00:04:02,522
所以重新估计涉及两种参数一种是θ的p值

49
00:04:02,522 --> 00:04:08,400
这是选择一个特定的分布的概率

50
00:04:08,400 --> 00:04:09,980
在我们观察出什么之前

51
00:04:09,980 --> 00:04:13,230
我们不知道关于哪一个集群是最有可能的

52
00:04:13,230 --> 00:04:17,558
但是经过我们研究这些文件

53
00:04:17,558 --> 00:04:23,544
然后我们可以破译证据来推测哪一个集群是最有可能的

54
00:04:23,544 --> 00:04:28,166
所以这与总和保持相同的比例

55
00:04:28,166 --> 00:04:32,940
z的子集dj是等于i的概率

56
00:04:34,680 --> 00:04:40,640
所以这给了我们所有的关于使用主题i的证据

57
00:04:40,640 --> 00:04:43,380
θi来生成一个文档

58
00:04:43,380 --> 00:04:48,590
把它们放到一起然后再次的我们使它们正常化到概率

59
00:04:50,420 --> 00:04:53,300
所以这是θ子集i的关键

60
00:04:54,560 --> 00:04:58,969
现在另一种参数是每一个文档中单词的概率

61
00:04:58,969 --> 00:05:01,144
在每个集群的分布

62
00:05:01,144 --> 00:05:05,384
这与pic的情况很相似

63
00:05:05,384 --> 00:05:10,230
这里我们只是报告这种文字

64
00:05:10,230 --> 00:05:15,442
经过推断已经被生成的文档

65
00:05:15,442 --> 00:05:20,380
已经从这里的θi中一个特定的主题中生成

66
00:05:20,380 --> 00:05:25,240
这将会让我们在之后估计有多少文字已经

67
00:05:25,240 --> 00:05:28,807
实际上从θi被生成

68
00:05:28,807 --> 00:05:32,694
然后我们将要将这些账户重新标准化为概率所以

69
00:05:32,694 --> 00:05:36,550
全部的单词的概率将会被加合

70
00:05:36,550 --> 00:05:40,560
注意了解这些限制条件是非常重要的

71
00:05:40,560 --> 00:05:45,890
他们刚好是所有这些公式的标准化

72
00:05:45,890 --> 00:05:53,380
而且明白这个分布是针对哪些也是非常重要的

73
00:05:54,490 --> 00:05:59,730
例如θ的概率是针对所有的主题k

74
00:05:59,730 --> 00:06:02,730
这也是为什么这些主题k的概率加合将会为1

75
00:06:02,730 --> 00:06:07,304
然而给定单词θ的概率是一个概率分布

76
00:06:07,304 --> 00:06:08,527
涵盖全部的单词

77
00:06:08,527 --> 00:06:13,100
所以这会有很多的概率并且它们必须相加为1

78
00:06:13,100 --> 00:06:17,279
所以现在让我们来看一个简单的关于两个集群的例子

79
00:06:17,279 --> 00:06:21,340
我有两个集群并且已经初始化一些数值作为

80
00:06:21,340 --> 00:06:23,440
两个分布

81
00:06:23,440 --> 00:06:28,270
让我们假设我们随机的初始化两个概率

82
00:06:28,270 --> 00:06:33,140
选择每一个集群为0.5所以同样有可能地

83
00:06:33,140 --> 00:06:36,400
然后我们考虑你在这里看到的这个文档

84
00:06:36,400 --> 00:06:41,350
这里有两个text文本出现以及两个mining文本出现

85
00:06:41,350 --> 00:06:44,910
所以这里一共有四个单词医疗和健康

86
00:06:44,910 --> 00:06:47,100
健康没有出现在这个文档中

87
00:06:47,100 --> 00:06:49,209
所以让我们思考一下隐藏变量

88
00:06:50,360 --> 00:06:54,970
现在对于每一个文档我们必须使用一个隐藏变量

89
00:06:54,970 --> 00:06:58,907
在pic之前我们使用了一个隐藏变量为了

90
00:06:58,907 --> 00:07:03,804
每一个工作因为那是一个混合模型的输出

91
00:07:03,804 --> 00:07:06,562
所以在我们这种情况下混合模型的输出或者

92
00:07:06,562 --> 00:07:10,810
混合模型的观察是一个文档不是一个单词

93
00:07:10,810 --> 00:07:14,920
所以现在我们有一个隐藏变量附加在文档上

94
00:07:14,920 --> 00:07:18,338
现在那个隐藏变量必须告诉我们哪一个分布已经被用来

95
00:07:18,338 --> 00:07:19,525
生成文件

96
00:07:19,525 --> 00:07:24,010
所以这将要采取两个值1和2来代表两个主题

97
00:07:25,350 --> 00:07:29,940
所以现在我们怎么来推测文档d中哪一个分布已经被使用

98
00:07:29,940 --> 00:07:33,380
那么他已被用于基本规则所以它看起来像这样

99
00:07:33,380 --> 00:07:39,071
为了使第一个主题θ1生成一个文档

100
00:07:39,071 --> 00:07:41,530
两件事必须发生

101
00:07:41,530 --> 00:07:45,210
首先θ子集1必须已经被选择

102
00:07:45,210 --> 00:07:48,050
所以它由θ1的p值给出

103
00:07:48,050 --> 00:07:54,144
第二文件中必须已经生成四个单词

104
00:07:54,144 --> 00:07:59,004
这就是说这里有两个文本出现以及两次进一步挖掘

105
00:07:59,004 --> 00:08:04,283
这就是为什么你看到分子中有这些概率的乘积

106
00:08:04,283 --> 00:08:10,182
选择θ1以及从θ1中生成文档的概率

107
00:08:10,182 --> 00:08:15,011
所以分母仅是两种可能性之和

108
00:08:15,011 --> 00:08:16,146
这个文件

109
00:08:16,146 --> 00:08:21,138
在这种情况下你可以插入数值来验证

110
00:08:21,138 --> 00:08:25,283
这个文档更可能由θ1生成

111
00:08:25,283 --> 00:08:27,915
比从θ2更有可能

112
00:08:27,915 --> 00:08:30,236
所以一旦我们有了这个概率

113
00:08:30,236 --> 00:08:35,950
给定这个文件我们可以轻松的计算当z等于2时的概率

114
00:08:35,950 --> 00:08:36,690
怎么做呢?

115
00:08:36,690 --> 00:08:38,720
那么我们可以使用约束

116
00:08:38,720 --> 00:08:43,339
那将会是1减去101分之100

117
00:08:43,339 --> 00:08:47,500
所以现在重要的是要注意在这样的计算中

118
00:08:47,500 --> 00:08:50,520
这里有下溢的潜在问题

119
00:08:50,520 --> 00:08:55,700
那是因为如果你看原来的分子和分母

120
00:08:55,700 --> 00:09:00,530
它涉及许多小概率的乘积的竞争

121
00:09:00,530 --> 00:09:03,050
想象一下如果一个文档有许多单词

122
00:09:03,050 --> 00:09:09,360
这将是一个非常小的值可能会导致下溢的问题

123
00:09:09,360 --> 00:09:14,294
所以为了解决这个问题我们可以使用标准化

124
00:09:14,294 --> 00:09:18,537
所以在这里你看到我们进行取平均值对所有的这两个

125
00:09:18,537 --> 00:09:23,340
分布来计算另一个平均分布叫θbar

126
00:09:24,530 --> 00:09:28,590
而这个平均分布将会是可比的与

127
00:09:28,590 --> 00:09:33,650
这些分布的每一个在数量和幅度方面

128
00:09:33,650 --> 00:09:38,440
所以我们可以划分分子和

129
00:09:38,440 --> 00:09:42,070
分母都通过这个标准因子

130
00:09:42,070 --> 00:09:47,688
所以基本上这规范了

131
00:09:47,688 --> 00:09:52,990
用这个平均单词分布来生成这个文档的概率

132
00:09:52,990 --> 00:09:56,310
所以你可以看到标准因子在这里

133
00:09:56,310 --> 00:10:00,690
并且因为我们已经使用完全相同的标准因子给分子和

134
00:10:00,690 --> 00:10:02,240
分母

135
00:10:02,240 --> 00:10:07,940
这个表达式的整体值不会变但是通过这种标准化

136
00:10:07,940 --> 00:10:14,480
你可以看到我们可以使分子和分母更加的易于处理

137
00:10:14,480 --> 00:10:19,890
这样的话每一个总体值将不会非常小

138
00:10:19,890 --> 00:10:22,660
因此我们可以避免下溢问题

139
00:10:24,580 --> 00:10:29,530
在其他情况下我们有时也会使用乘积的对数

140
00:10:29,530 --> 00:10:33,570
来将其转换为概率对数的总和

141
00:10:33,570 --> 00:10:36,080
这也有助于维持准确度但是

142
00:10:36,080 --> 00:10:40,720
在这种情况下我们不能使用对数来解决问题

143
00:10:40,720 --> 00:10:44,030
因为这里的分母有总和但是

144
00:10:44,030 --> 00:10:49,230
这种标准化对于解决这个问题是有效的

145
00:10:49,230 --> 00:10:53,754
所以这是一种有时在其他情况下有效果的技巧

146
00:10:53,754 --> 00:10:55,057
情况也是如此

147
00:10:55,057 --> 00:10:56,630
现在让我们来看一下M步骤

148
00:10:56,630 --> 00:11:01,557
所以从E步骤我们可以看到我们对于那一个分布更有可能

149
00:11:01,557 --> 00:11:03,521
已经在d生成一个文档的估计

150
00:11:03,521 --> 00:11:08,157
并且你可以看到d1更有可能是来自于第一个主题

151
00:11:08,157 --> 00:11:12,090
同时d2更有可能来自第二个主题等等

152
00:11:12,090 --> 00:11:15,800
现在让我们考虑一下我们需要在M步骤中计算什么

153
00:11:15,800 --> 00:11:18,460
基本上我们需要重新估计所有的参数

154
00:11:18,460 --> 00:11:22,750
首先看一下θ1和θ2的p值

155
00:11:22,750 --> 00:11:24,010
我们怎么估计它呢

156
00:11:24,010 --> 00:11:31,625
直观地你可以仅仅把这些z汇集起来，从E步骤中得到的概率

157
00:11:31,625 --> 00:11:36,335
所以如果全部的这些文档表明他们更有可能来自于θ1

158
00:11:36,335 --> 00:11:42,240
那么我们将会直观地给θ1更高的概率

159
00:11:42,240 --> 00:11:46,240
在这种情况下我们可以只取平均值对于

160
00:11:46,240 --> 00:11:50,920
你在这里看到的这些概率并且我们已经获得了θ1的概率为0.6

161
00:11:50,920 --> 00:11:53,860
所以θ1比θ2更有可能

162
00:11:53,860 --> 00:11:59,542
所以你可以看到θ2的概率将自然会是0.4

163
00:11:59,542 --> 00:12:01,680
那么这些单词概率呢

164
00:12:01,680 --> 00:12:04,490
我们做同样的事情因为直觉是一样的

165
00:12:04,490 --> 00:12:05,802
所以我们将会看到

166
00:12:05,802 --> 00:12:09,148
为了估计θ1中单词的概率

167
00:12:09,148 --> 00:12:13,295
我们会看看哪个文件已经从θ1生成

168
00:12:13,295 --> 00:12:17,510
而且我们将会把这些文件中的单词合并在一起并将它们标准化

169
00:12:17,510 --> 00:12:19,400
所以这基本上就是我刚才讲的

170
00:12:20,510 --> 00:12:25,970
更具体的说我们将要使用这些文档里的所有类型的文本

171
00:12:25,970 --> 00:12:31,103
来估计给定的θ1中文本的概率

172
00:12:31,103 --> 00:12:34,480
但是我们将不会使用他们的原始计数或者总数

173
00:12:34,480 --> 00:12:40,110
相反我们将要对它们打折扣用

174
00:12:40,110 --> 00:12:43,567
每一个文件从θ1中生成的概率

175
00:12:43,567 --> 00:12:47,490
所以这些给我们一些分数树脂

176
00:12:47,490 --> 00:12:50,480
然后这些数值将会被标准化

177
00:12:50,480 --> 00:12:52,530
为了得到概率

178
00:12:52,530 --> 00:12:53,720
现在我们如果将它们标准化呢

179
00:12:53,720 --> 00:12:57,810
那么这些单词的概率必须赋值为1

180
00:12:57,810 --> 00:13:02,790
所以为了总结我们关于聚类生成模型的讨论

181
00:13:02,790 --> 00:13:07,500
我们可以看出主题模型的些微变化可以被用来

182
00:13:07,500 --> 00:13:08,760
聚类文件

183
00:13:08,760 --> 00:13:12,730
这也显示了一般生成模型的力量

184
00:13:12,730 --> 00:13:18,180
通过改变生成假设病稍微改变模型我们可以实现

185
00:13:18,180 --> 00:13:23,010
不同的目标并且我们可以捕获不同的模式以及不同类型的数据

186
00:13:23,010 --> 00:13:27,430
所以在这种情况下每一个集群都用单一语言模型单词分布来表示

187
00:13:27,430 --> 00:13:31,940
而且这与主题模型是很相似的

188
00:13:31,940 --> 00:13:35,680
所以这里你可以看到单词分布实际上会生成一个术语集群

189
00:13:35,680 --> 00:13:37,900
作为副产品

190
00:13:37,900 --> 00:13:41,090
通过首先选择单一语言模型生成的文档

191
00:13:41,090 --> 00:13:44,190
然后生成文档中所有的单词

192
00:13:44,190 --> 00:13:45,960
仅使用一种单一的语言模型

193
00:13:45,960 --> 00:13:49,830
这与再次复制模型十分不同

194
00:13:49,830 --> 00:13:54,910
我们可以通过使用多种单一语言模型而生成文档中的单词

195
00:13:56,740 --> 00:14:01,530
然后估计的模型参数给出了

196
00:14:01,530 --> 00:14:02,950
每一个集群的主题特征

197
00:14:02,950 --> 00:14:06,190
还有将每一个文档分配到一个集群的概率

198
00:14:07,290 --> 00:14:12,140
这个概率分配有时对某些应用是有效的

199
00:14:12,140 --> 00:14:15,710
但是如果我们想要对更加复杂的集群实现

200
00:14:16,800 --> 00:14:20,160
主要从分割文件到不连贯的集群

201
00:14:20,160 --> 00:14:25,073
然后我们可以强制一个文档到集群中来对应单词

202
00:14:25,073 --> 00:14:29,467
最有可能生成文档的分布

203
00:14:29,467 --> 00:14:33,669
我们已经表明运算法则可以被用来计算最大可能性

204
00:14:33,669 --> 00:14:34,990
或者预测

205
00:14:34,990 --> 00:14:38,798
在这种情况下我们需要使用一个特殊的

206
00:14:38,798 --> 00:14:43,393
数字添加技术来避免下溢

207
00:14:43,393 --> 00:14:53,393
[背景音乐]
翻译: MingShi |审阅: 
Coursera Global Translator Community