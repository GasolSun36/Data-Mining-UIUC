这节课 是文本聚类的生成概率模型的延续讨论 在这节课中我们将要完成对于生成概率模型的讨论 为了文本交叉而生成概率模型 所以这是大家之前看过的一张幻灯片这里我们展示了我们如何定义 我们定义文本聚类的混合模型以及似然函数的样子 而且我们还可以计算最大似然估计值 来估测参数 这节课中我们将要更多的讨论我们将如何更精确的 计算最大似然估计值 在大多数情况下最大期望算法可以被用来解决混合模型中的这个问题 因此以下是这个文档聚类最大期望算法的详细内容 现在如果你已经明白算法是怎么工作的 像TRSA一样的主题模型并且我认为这会是非常相似的 而我们只需要改变一点来适应这个新的混合模型 所以正如你可能还记得最大期望算法开始于对所有参数进行的初始化 所以这与之前所讲的主题模型是一样的 然后我们将要重复知道可能性收敛并且 在每一步我们将要做E步骤和M步骤 在M步骤 我们将要推测被用来产生每个文档的分布 所以我必须引入一个隐藏变量Zd 每个文档以及这个变量可以在1到k中取值 代表k的不同分布 更具体来说基本上我们将要运用基本规则来推测 哪一个分布最有可能生成这个文档或者 计算给定文档的分布的后验概率 并且我们知道这与概率成正比 选择这个θ的分布 以及从这个分布中产生这整个文档的概率 是你在这里看到的这个文档所有单词的概率的乘积 现在 这个概率的正规化子或者约束条件 所以在这种情况下我们知道在这个约束下的概率 在E步骤中是所有z的概率等于i并且之和必须是1 因为文件必须都是精确的由这些k主题其中之一生成 主题 所以从他们其中每一个中生成的概率应该总和为1 如果你知道这个约束那么你可以简单地计算这个分布 只要你知道什么是成比例的 所以只要你计算出这里你看到的这个结果然后你只要简单的正常化 这些概率使它们在全部的主题中相加之和为1 所以这是E步骤    E步骤之后我们想要了解哪一个分布是 最可能生成这个文件d的以及哪一个最不可能 然后在M步骤中我们将要重新估计所有的参数基于 进一步的z值或者进一步关于哪一个分布 已经被用来生成哪个文档 所以重新估计涉及两种参数一种是θ的p值 这是选择一个特定的分布的概率 在我们观察出什么之前 我们不知道关于哪一个集群是最有可能的 但是经过我们研究这些文件 然后我们可以破译证据来推测哪一个集群是最有可能的 所以这与总和保持相同的比例 z的子集dj是等于i的概率 所以这给了我们所有的关于使用主题i的证据 θi来生成一个文档 把它们放到一起然后再次的我们使它们正常化到概率 所以这是θ子集i的关键 现在另一种参数是每一个文档中单词的概率 在每个集群的分布 这与pic的情况很相似 这里我们只是报告这种文字 经过推断已经被生成的文档 已经从这里的θi中一个特定的主题中生成 这将会让我们在之后估计有多少文字已经 实际上从θi被生成 然后我们将要将这些账户重新标准化为概率所以 全部的单词的概率将会被加合 注意了解这些限制条件是非常重要的 他们刚好是所有这些公式的标准化 而且明白这个分布是针对哪些也是非常重要的 例如θ的概率是针对所有的主题k 这也是为什么这些主题k的概率加合将会为1 然而给定单词θ的概率是一个概率分布 涵盖全部的单词 所以这会有很多的概率并且它们必须相加为1 所以现在让我们来看一个简单的关于两个集群的例子 我有两个集群并且已经初始化一些数值作为 两个分布 让我们假设我们随机的初始化两个概率 选择每一个集群为0.5所以同样有可能地 然后我们考虑你在这里看到的这个文档 这里有两个text文本出现以及两个mining文本出现 所以这里一共有四个单词医疗和健康 健康没有出现在这个文档中 所以让我们思考一下隐藏变量 现在对于每一个文档我们必须使用一个隐藏变量 在pic之前我们使用了一个隐藏变量为了 每一个工作因为那是一个混合模型的输出 所以在我们这种情况下混合模型的输出或者 混合模型的观察是一个文档不是一个单词 所以现在我们有一个隐藏变量附加在文档上 现在那个隐藏变量必须告诉我们哪一个分布已经被用来 生成文件 所以这将要采取两个值1和2来代表两个主题 所以现在我们怎么来推测文档d中哪一个分布已经被使用 那么他已被用于基本规则所以它看起来像这样 为了使第一个主题θ1生成一个文档 两件事必须发生 首先θ子集1必须已经被选择 所以它由θ1的p值给出 第二文件中必须已经生成四个单词 这就是说这里有两个文本出现以及两次进一步挖掘 这就是为什么你看到分子中有这些概率的乘积 选择θ1以及从θ1中生成文档的概率 所以分母仅是两种可能性之和 这个文件 在这种情况下你可以插入数值来验证 这个文档更可能由θ1生成 比从θ2更有可能 所以一旦我们有了这个概率 给定这个文件我们可以轻松的计算当z等于2时的概率 怎么做呢? 那么我们可以使用约束 那将会是1减去101分之100 所以现在重要的是要注意在这样的计算中 这里有下溢的潜在问题 那是因为如果你看原来的分子和分母 它涉及许多小概率的乘积的竞争 想象一下如果一个文档有许多单词 这将是一个非常小的值可能会导致下溢的问题 所以为了解决这个问题我们可以使用标准化 所以在这里你看到我们进行取平均值对所有的这两个 分布来计算另一个平均分布叫θbar 而这个平均分布将会是可比的与 这些分布的每一个在数量和幅度方面 所以我们可以划分分子和 分母都通过这个标准因子 所以基本上这规范了 用这个平均单词分布来生成这个文档的概率 所以你可以看到标准因子在这里 并且因为我们已经使用完全相同的标准因子给分子和 分母 这个表达式的整体值不会变但是通过这种标准化 你可以看到我们可以使分子和分母更加的易于处理 这样的话每一个总体值将不会非常小 因此我们可以避免下溢问题 在其他情况下我们有时也会使用乘积的对数 来将其转换为概率对数的总和 这也有助于维持准确度但是 在这种情况下我们不能使用对数来解决问题 因为这里的分母有总和但是 这种标准化对于解决这个问题是有效的 所以这是一种有时在其他情况下有效果的技巧 情况也是如此 现在让我们来看一下M步骤 所以从E步骤我们可以看到我们对于那一个分布更有可能 已经在d生成一个文档的估计 并且你可以看到d1更有可能是来自于第一个主题 同时d2更有可能来自第二个主题等等 现在让我们考虑一下我们需要在M步骤中计算什么 基本上我们需要重新估计所有的参数 首先看一下θ1和θ2的p值 我们怎么估计它呢 直观地你可以仅仅把这些z汇集起来，从E步骤中得到的概率 所以如果全部的这些文档表明他们更有可能来自于θ1 那么我们将会直观地给θ1更高的概率 在这种情况下我们可以只取平均值对于 你在这里看到的这些概率并且我们已经获得了θ1的概率为0.6 所以θ1比θ2更有可能 所以你可以看到θ2的概率将自然会是0.4 那么这些单词概率呢 我们做同样的事情因为直觉是一样的 所以我们将会看到 为了估计θ1中单词的概率 我们会看看哪个文件已经从θ1生成 而且我们将会把这些文件中的单词合并在一起并将它们标准化 所以这基本上就是我刚才讲的 更具体的说我们将要使用这些文档里的所有类型的文本 来估计给定的θ1中文本的概率 但是我们将不会使用他们的原始计数或者总数 相反我们将要对它们打折扣用 每一个文件从θ1中生成的概率 所以这些给我们一些分数树脂 然后这些数值将会被标准化 为了得到概率 现在我们如果将它们标准化呢 那么这些单词的概率必须赋值为1 所以为了总结我们关于聚类生成模型的讨论 我们可以看出主题模型的些微变化可以被用来 聚类文件 这也显示了一般生成模型的力量 通过改变生成假设病稍微改变模型我们可以实现 不同的目标并且我们可以捕获不同的模式以及不同类型的数据 所以在这种情况下每一个集群都用单一语言模型单词分布来表示 而且这与主题模型是很相似的 所以这里你可以看到单词分布实际上会生成一个术语集群 作为副产品 通过首先选择单一语言模型生成的文档 然后生成文档中所有的单词 仅使用一种单一的语言模型 这与再次复制模型十分不同 我们可以通过使用多种单一语言模型而生成文档中的单词 然后估计的模型参数给出了 每一个集群的主题特征 还有将每一个文档分配到一个集群的概率 这个概率分配有时对某些应用是有效的 但是如果我们想要对更加复杂的集群实现 主要从分割文件到不连贯的集群 然后我们可以强制一个文档到集群中来对应单词 最有可能生成文档的分布 我们已经表明运算法则可以被用来计算最大可能性 或者预测 在这种情况下我们需要使用一个特殊的 数字添加技术来避免下溢 [背景音乐]
翻译: MingShi |审阅:
Coursera Global Translator Community