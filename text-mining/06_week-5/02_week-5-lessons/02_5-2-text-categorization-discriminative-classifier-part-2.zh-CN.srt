1
00:00:07,290 --> 00:00:11,068
这一讲继续讨论

2
00:00:11,068 --> 00:00:15,610
文本分类的判别分类器

3
00:00:15,610 --> 00:00:18,096
这一讲里
我们要介绍

4
00:00:18,096 --> 00:00:22,450
另一个判别分类器
叫做支持向量机（SVM）

5
00:00:22,450 --> 00:00:25,050
这种分类方法十分流行

6
00:00:25,050 --> 00:00:28,790
在文本分类上也很有效

7
00:00:31,350 --> 00:00:34,380
为了介绍这一分类方法

8
00:00:34,380 --> 00:00:38,060
我们考虑存在两大类的简单情况

9
00:00:38,060 --> 00:00:43,300
我们有两个话题类
这里的01和02

10
00:00:43,300 --> 00:00:47,760
我们要将文档
分入这两类

11
00:00:47,760 --> 00:00:51,820
然后通过一个特征因子x来表示一篇文档

12
00:00:53,200 --> 00:00:58,020
现在这个分类器的想法
是构造一个线性分类器

13
00:00:59,150 --> 00:01:01,360
像你看到的这样

14
00:01:01,360 --> 00:01:05,820
这与回归十分相似，是吧？

15
00:01:05,820 --> 00:01:11,240
然后我们可以说
如果函数值为正

16
00:01:11,240 --> 00:01:16,690
我们就说对象归入分类1

17
00:01:16,690 --> 00:01:20,470
否则我们就说它属于分类2

18
00:01:20,470 --> 00:01:27,700
这样就使得0成为了几类之间的决策边界

19
00:01:28,830 --> 00:01:33,990
一般来讲隐藏的边缘空间
像是0

20
00:01:33,990 --> 00:01:37,070
对应一个超平面

21
00:01:38,210 --> 00:01:43,180
现在我们展示一个二维空间下的简单情况
只有X1和X2

22
00:01:43,180 --> 00:01:49,910
这可以对应你看到的这条线

23
00:01:51,220 --> 00:01:55,980
所以这是一条由

24
00:01:55,980 --> 00:02:00,970
三个参数决定的线
贝塔0、贝塔1和贝塔2

25
00:02:02,390 --> 00:02:07,320
现在这条线指向这个方向

26
00:02:07,320 --> 00:02:13,450
如果我们增加X1
X2也会增加

27
00:02:13,450 --> 00:02:17,780
那么我们知道贝塔1和贝塔2有不同的符号
一个为负[英文原文有误]

28
00:02:17,780 --> 00:02:18,920
另一个为正

29
00:02:20,800 --> 00:02:26,790
我们假设贝塔1为负
贝塔2为正

30
00:02:28,810 --> 00:02:31,250
现在我们可以检验

31
00:02:31,250 --> 00:02:34,800
两侧的数据实例

32
00:02:34,800 --> 00:02:39,690
这里数据用圆表示一类

33
00:02:39,690 --> 00:02:41,800
钻石表示另一类

34
00:02:43,140 --> 00:02:49,090
那么问题就是选择像这样的一个点
然后考察

35
00:02:49,090 --> 00:02:54,110
这个数据点上表达式
或者说分类器的值是什么

36
00:02:55,350 --> 00:02:57,000
那么你怎么认为？

37
00:02:57,000 --> 00:03:00,650
一般我们可以利用这个函数来估计

38
00:03:01,740 --> 00:03:06,190
像之前说的如果值为正
我们就将其归入分类1

39
00:03:06,190 --> 00:03:09,610
如果为负就归入分类2

40
00:03:09,610 --> 00:03:15,343
直观上讲这条线分开了两个类别
我们希望

41
00:03:15,343 --> 00:03:19,870
一侧的点值为正
另一侧为负

42
00:03:19,870 --> 00:03:23,200
我们的问题建立在
我刚提到的假设之上

43
00:03:23,200 --> 00:03:25,320
我们来检验一个点
像这个

44
00:03:27,590 --> 00:03:30,480
你认为这个表达式的值是什么呢？

45
00:03:31,610 --> 00:03:37,830
通过看这个表达式的值我们可以检验这个符号

46
00:03:37,830 --> 00:03:40,950
我们可以将

47
00:03:42,050 --> 00:03:46,950
线上的值与这个点的值相比较

48
00:03:48,440 --> 00:03:53,520
它们有相同的X1
但是一个有更高的X2值

49
00:03:54,740 --> 00:03:59,790
我们现在来看X2的系数符号

50
00:03:59,790 --> 00:04:01,610
我们看这里是正的

51
00:04:02,850 --> 00:04:06,260
也就是说
该点的函数值

52
00:04:06,260 --> 00:04:10,400
比线上点的函数值要高

53
00:04:10,400 --> 00:04:14,800
所以就是正的对吧？

54
00:04:16,190 --> 00:04:19,900
那么我们也就大概知道
所有这一侧的点

55
00:04:20,960 --> 00:04:25,380
函数值为正

56
00:04:25,380 --> 00:04:29,380
你也可以确定这一侧的所有点
函数值为负

57
00:04:29,380 --> 00:04:31,750
这种线性分类器

58
00:04:31,750 --> 00:04:35,940
或者说线性分离方法
就是这样将点分为两类的

59
00:04:37,810 --> 00:04:42,830
那么自然就要问
哪个线性分类器是最好的呢？

60
00:04:42,830 --> 00:04:47,687
我这里划了一条线
能够分出这两类

61
00:04:47,687 --> 00:04:53,190
这条线当然是由
系数贝塔来决定

62
00:04:53,190 --> 00:04:55,210
不同的系数
对应不同的线

63
00:04:55,210 --> 00:04:58,770
那么我们可以想象
也会有其他线可以达到相同的效果

64
00:04:58,770 --> 00:05:00,630
比如说伽马

65
00:05:00,630 --> 00:05:04,860
可以定义一条新的线
用来分离数据

66
00:05:06,010 --> 00:05:09,710
当然也有一些线无法起到分离作用
它们就不是好的选择

67
00:05:09,710 --> 00:05:12,310
但是问题是
如果我们有多条线

68
00:05:12,310 --> 00:05:15,950
都可以分开两类
哪一个最好呢？

69
00:05:15,950 --> 00:05:21,740
实际上你可以想象
选择这条线可以用许多不同的方法

70
00:05:21,740 --> 00:05:27,310
那么逻辑回归分类器
就是你之前看到的

71
00:05:27,310 --> 00:05:33,060
通过一些标准判断这条线的位置
用来作为线性分类器

72
00:05:33,060 --> 00:05:36,610
利用对训练集的条件概率

73
00:05:36,610 --> 00:05:38,310
决定哪条线最好

74
00:05:38,310 --> 00:05:41,130
但是在支持向量机中我们会用另一个标准

75
00:05:41,130 --> 00:05:43,500
来决定哪条线最好

76
00:05:43,500 --> 00:05:44,230
这一次 

77
00:05:44,230 --> 00:05:48,300
你会看到这个标准
与分类箭头联系更紧密

78
00:05:49,460 --> 00:05:56,120
基本想法就是选择
最大化空白

79
00:05:56,120 --> 00:05:57,180
什么是空白呢？

80
00:05:57,180 --> 00:06:03,540
我这里选了一些虚线
用来表示

81
00:06:03,540 --> 00:06:09,020
每类数据点的界限

82
00:06:09,020 --> 00:06:13,890
空白就是这些线、

83
00:06:13,890 --> 00:06:17,420
分类器和各类最近点的距离

84
00:06:18,490 --> 00:06:23,830
你可以看到这里的空白
我展示的这里

85
00:06:23,830 --> 00:06:25,810
你也可以定义另一侧的空白

86
00:06:27,020 --> 00:06:31,190
寻找最大化空白的分类器

87
00:06:31,190 --> 00:06:35,700
应该是在两个边界之间

88
00:06:35,700 --> 00:06:40,050
你不想分类器与一边过于接近

89
00:06:40,050 --> 00:06:42,800
这样更符合直觉

90
00:06:44,460 --> 00:06:47,050
这就是支持向量机的基本思想

91
00:06:47,050 --> 00:06:50,020
我们要选择最大化空白的一个线性分类器

92
00:06:52,130 --> 00:06:55,450
这里我也调整了定义

93
00:06:55,450 --> 00:06:58,460
这样我不再用贝塔代指这些参数

94
00:06:58,460 --> 00:07:03,740
这里我改用w
虽然之前单词也可以用w

95
00:07:03,740 --> 00:07:05,370
这里不要混淆了

96
00:07:05,370 --> 00:07:09,618
w这里表示的是宽度
一定的宽度

97
00:07:12,734 --> 00:07:19,030
那么我这里用小写b
表示有偏的常数贝塔0

98
00:07:20,030 --> 00:07:24,100
这里数据用x表示

99
00:07:24,100 --> 00:07:28,790
我们会用向量乘法来计算

100
00:07:28,790 --> 00:07:34,110
这里我们将向量w转置
然后与特征向量相乘[英文可能有误]

101
00:07:35,290 --> 00:07:42,080
b是一个有偏的常数
w是各个特征的权重

102
00:07:42,080 --> 00:07:45,260
我们有m个特征
就有m个权重

103
00:07:45,260 --> 00:07:46,420
由向量表示

104
00:07:47,640 --> 00:07:51,260
类似地这里的数据
文本对象

105
00:07:51,260 --> 00:07:55,940
由相同数量元素组成的特征向量表示

106
00:07:55,940 --> 00:07:59,100
Xi是一个特征值

107
00:07:59,100 --> 00:08:04,418
比如词频
你可以确认

108
00:08:04,418 --> 00:08:08,960
当我们将两个向量相乘的时候
点乘

109
00:08:08,960 --> 00:08:14,335
我们可以得到与之前看到一类的线性分类器

110
00:08:14,335 --> 00:08:16,713
这只是个不同的表示方式

111
00:08:16,713 --> 00:08:21,267
我现在有一个与定义更为一致的方式

112
00:08:21,267 --> 00:08:24,750
这个定义在人们讲到支持向量机的时候通常会用

113
00:08:24,750 --> 00:08:29,470
这个方法可以更好地将幻灯片与其他阅读材料结合起来

114
00:08:31,190 --> 00:08:39,780
好那么当我们最大化分类器产生的空白的时候

115
00:08:39,780 --> 00:08:44,730
意味着分类器的边界只由

116
00:08:44,730 --> 00:08:49,800
一些数据点决定
这些数据点我们叫做支持向量

117
00:08:49,800 --> 00:08:54,600
这里展示了两个支持向量
一表示其中一类

118
00:08:54,600 --> 00:08:56,220
二表示另一类

119
00:08:56,220 --> 00:09:00,900
这些点大致定义了空白区域

120
00:09:00,900 --> 00:09:05,350
你可以想象哪些是支持向量

121
00:09:06,430 --> 00:09:09,750
那么中间的这条分类线会由它们决定

122
00:09:09,750 --> 00:09:16,320
那么其他的数据并没有那么大的影响

123
00:09:16,320 --> 00:09:20,420
你看如果你改变其他的数据点
并不会影响到空白

124
00:09:20,420 --> 00:09:22,905
分类器会维持原状

125
00:09:22,905 --> 00:09:26,514
主要受支持向量机的影响

126
00:09:26,514 --> 00:09:29,705
抱歉
是主要受支持向量的影响

127
00:09:29,705 --> 00:09:32,639
所以它被叫做支持向量机

128
00:09:32,639 --> 00:09:37,968
好下一个问题当然就是

129
00:09:37,968 --> 00:09:42,730
我们如何优化这条线？

130
00:09:42,730 --> 00:09:47,430
我们如何找到这条线
或者是分类器？

131
00:09:47,430 --> 00:09:51,390
这与寻找w和b的值一样

132
00:09:51,390 --> 00:09:55,779
因为他们是决定分类器的参数

133
00:09:58,010 --> 00:10:04,700
在最简单的情形下
线性支持向量机是一个简单的最优化问题

134
00:10:04,700 --> 00:10:10,230
回想一下我们的分类其实一个线性分类器

135
00:10:10,230 --> 00:10:15,980
我们有每个特征的权重
主要目标是消除权重w和b

136
00:10:15,980 --> 00:10:21,040
分类器会判断X属于类theta1
如果其为正

137
00:10:21,040 --> 00:10:23,950
否则我们就说它属于另一类

138
00:10:23,950 --> 00:10:27,220
这就是我们的假设

139
00:10:27,220 --> 00:10:32,406
所以在线性支持向量机中
我们要寻找这些参数值

140
00:10:32,406 --> 00:10:37,510
优化空白以及训练偏误

141
00:10:38,800 --> 00:10:41,920
训练数据基本上与其他分类器一样

142
00:10:41,920 --> 00:10:45,940
我们有一个训练集
我们知道其中X向量的值

143
00:10:45,940 --> 00:10:50,290
我们也知道对应的标签y_i

144
00:10:50,290 --> 00:10:54,310
这里我们定义y_i只有两个值

145
00:10:54,310 --> 00:10:58,358
它们不是你之前看到的0和1
而是-1和1

146
00:10:58,358 --> 00:11:03,990
对应到这里的两个类别

147
00:11:03,990 --> 00:11:08,330
现在你可能在想为什么我们不用0和1

148
00:11:08,330 --> 00:11:11,770
而是-1和1

149
00:11:11,770 --> 00:11:15,520
这只是为了数学上的便利
一会你就会看到

150
00:11:16,700 --> 00:11:19,450
那么最优化过程的第一步

151
00:11:19,450 --> 00:11:23,700
就是确保训练集的标签都要做对

152
00:11:23,700 --> 00:11:28,240
也就是说如果x_i的标签y_i是1

153
00:11:28,240 --> 00:11:33,610
我们希望这个分类值要大

154
00:11:33,610 --> 00:11:36,740
这里我们就选1作为阈值

155
00:11:36,740 --> 00:11:41,875
但如果你用其他阈值
你可以将这个常数

156
00:11:41,875 --> 00:11:47,300
调整到参数b和w当中
使得右侧仍只留下1

157
00:11:48,950 --> 00:11:54,780
另一方面如果y是-1
意味着是在另一类里

158
00:11:54,780 --> 00:11:58,460
那么我们就希望分类值足够小

159
00:11:58,460 --> 00:12:04,860
对于一个负数来说我们就希望它
小于或等于-1

160
00:12:04,860 --> 00:12:11,110
现在我们有了两个不同的例子
不同的类别

161
00:12:11,110 --> 00:12:13,714
我们如何将它们结合起来呢？

162
00:12:13,714 --> 00:12:18,622
这就是我们用-1表示

163
00:12:18,622 --> 00:12:20,200
另一类的方便之处了

164
00:12:20,200 --> 00:12:25,830
因为这里我们可以将两个限制条件合为一个

165
00:12:26,832 --> 00:12:32,085
y_i乘以分类值必须大于或等于1

166
00:12:33,210 --> 00:12:35,484
如果y_i等于1

167
00:12:35,484 --> 00:12:39,968
你可以看到跟左边的限制条件是一样的

168
00:12:39,968 --> 00:12:48,020
如果y_i为-1
你可以看到也是一样

169
00:12:48,020 --> 00:12:53,060
也就是说这个式子实际上将两个限制统一了起来

170
00:12:53,060 --> 00:12:56,960
这是限制条件的一种便捷的表示方式

171
00:12:56,960 --> 00:12:58,137
第二个目标是什么？

172
00:12:58,137 --> 00:13:00,414
我们要最大化空白

173
00:13:00,414 --> 00:13:04,600
我们希望确保分类器在训练数据中表现良好

174
00:13:04,600 --> 00:13:08,109
但之后在能够区分数据的所有情况当中

175
00:13:08,109 --> 00:13:12,172
我们还希望找到拥有最大空白的分类器

176
00:13:12,172 --> 00:13:18,758
这里空白可以假设是与权重的大小有关的

177
00:13:18,758 --> 00:13:23,777
那么我们将w转置乘以w

178
00:13:23,777 --> 00:13:29,893
就可以得到所有权重的平方和

179
00:13:29,893 --> 00:13:35,691
那么要在这个表达式中得到较小的值

180
00:13:35,691 --> 00:13:40,430
我们就需要所有wi都要小

181
00:13:42,440 --> 00:13:45,710
刚才我们假设
我们有一个约束条件

182
00:13:46,930 --> 00:13:50,890
那就是对训练集中的数据要分类正确

183
00:13:50,890 --> 00:13:57,649
现在我们有了一个新的目标
就是与最大化空白相关

184
00:13:57,649 --> 00:14:03,013
简单点说就是最小化w转置乘以w

185
00:14:03,013 --> 00:14:06,251
我们通常将其命名为Φ(w)

186
00:14:06,251 --> 00:14:10,616
你可以看到
这基本上就是一个最优化问题

187
00:14:10,616 --> 00:14:15,044
我们有一些需要优化的变量
就是权重w和b

188
00:14:15,044 --> 00:14:17,540
我们也有一些约束条件

189
00:14:17,540 --> 00:14:18,949
它们是线性的约束条件

190
00:14:18,949 --> 00:14:22,380
目标函数是一个含有权重的多项式函数

191
00:14:22,380 --> 00:14:25,370
这就是个含有线性约束的多项式程序

192
00:14:25,370 --> 00:14:30,050
解决这一问题有标准的算法

193
00:14:30,050 --> 00:14:34,190
解决了问题
我们就可以得到权重w和b

194
00:14:34,190 --> 00:14:37,080
我们就能得到一个明确的分类器

195
00:14:37,080 --> 00:14:42,160
我们可以利用这个分类器
判别任何新出现的文本对象

196
00:14:42,160 --> 00:14:47,190
之前的判别式不容许任何错分

197
00:14:47,190 --> 00:14:50,448
但是有时候数据对于分类器来说可能不是线性的

198
00:14:50,448 --> 00:14:54,690
也就是说结果可能不会像你看到的那么好

199
00:14:54,690 --> 00:14:59,300
之前你看到一条线恰好能够区分所有的点

200
00:14:59,300 --> 00:15:02,850
如果我们容许一些错分
结果会怎么样呢？

201
00:15:02,850 --> 00:15:04,980
原理上是一样的

202
00:15:04,980 --> 00:15:09,305
我们想最小化偏误
同时最大化空白

203
00:15:09,305 --> 00:15:12,270
但这种情况下
我们遇到的是一个软边界

204
00:15:12,270 --> 00:15:16,000
因为数据点可能不是完全可分的

205
00:15:17,030 --> 00:15:24,650
我们可以简单修改
支持向量机的设定来适应这一点

206
00:15:24,650 --> 00:15:28,090
这里你看到的和之前类似

207
00:15:28,090 --> 00:15:31,760
但是我们将引入额外的变量ξi

208
00:15:31,760 --> 00:15:35,610
我们将其对应到每一个数据实例

209
00:15:35,610 --> 00:15:40,780
它将会记录每个实例容许的错分

210
00:15:40,780 --> 00:15:43,245
最优化问题是非常相似的

211
00:15:43,245 --> 00:15:44,783
特别地

212
00:15:44,783 --> 00:15:50,170
你能看到我们在最优化问题中加了些东西

213
00:15:50,170 --> 00:15:56,861
首先我们在约束条件中加入了一些偏误

214
00:15:56,861 --> 00:16:02,119
现在我们允许分类器

215
00:16:02,119 --> 00:16:06,760
犯一些错误

216
00:16:06,760 --> 00:16:12,860
ξi表示允许的偏误

217
00:16:12,860 --> 00:16:16,560
如果ξi定为0
那么就与原先的约束条件一致

218
00:16:16,560 --> 00:16:20,260
我们希望每个数据都能被准确分类

219
00:16:20,260 --> 00:16:26,420
如果我们允许它不等于0，那么我们就允许一些偏误了

220
00:16:26,420 --> 00:16:30,730
事实上如果ξi非常大
偏误也会变得非常大

221
00:16:30,730 --> 00:16:33,270
所以很自然
我们不希望这种情况发生

222
00:16:33,270 --> 00:16:37,570
之后我们希望最小化ξi

223
00:16:37,570 --> 00:16:41,940
因为ξi最小化可以控制偏误的程度

224
00:16:42,940 --> 00:16:46,020
因此目标函数中

225
00:16:46,020 --> 00:16:50,910
我们也在只有W的原有情况下加入了ξi

226
00:16:50,910 --> 00:16:55,190
确保我们不仅会最小化权重

227
00:16:55,190 --> 00:16:59,130
还会最小化偏误
就像你看到的这样

228
00:16:59,130 --> 00:17:02,705
这里我们对所有数据做一个简单的求和

229
00:17:02,705 --> 00:17:07,695
每一项都有ξi来记录它的偏误

230
00:17:07,695 --> 00:17:10,413
我们把它们结合起来

231
00:17:10,413 --> 00:17:14,680
我们希望最小化它们所有的偏误

232
00:17:16,350 --> 00:17:21,001
你可以看到这里有个参数C
这是一个常数

233
00:17:21,001 --> 00:17:25,740
用来权衡最小化偏误与最大化空白

234
00:17:25,740 --> 00:17:27,888
如果C为0你可以看到

235
00:17:27,888 --> 00:17:33,070
我们就退回了原有目标函数
在那里我们只进行了空白的最大化

236
00:17:34,340 --> 00:17:38,368
我们并没有最优化训练偏误

237
00:17:38,368 --> 00:17:43,730
ξi就可以为一个很大的值
满足约束条件

238
00:17:43,730 --> 00:17:46,512
这当然并不好

239
00:17:46,512 --> 00:17:50,884
C应该是一个非零正值

240
00:17:50,884 --> 00:17:53,412
但是如果C非常大

241
00:17:53,412 --> 00:17:58,143
我们可以看到目标函数会主要由训练偏误决定

242
00:17:58,143 --> 00:18:02,420
最大化空白就会起次要作用了

243
00:18:02,420 --> 00:18:06,350
如果这样的话

244
00:18:07,420 --> 00:18:11,420
我们在最小化训练偏误上能够做到最好

245
00:18:11,420 --> 00:18:14,730
但是我们不会管空白的情况

246
00:18:14,730 --> 00:18:19,270
这会影响到对未来数据区分因子的概括

247
00:18:19,270 --> 00:18:20,548
这是不好的

248
00:18:20,548 --> 00:18:28,175
所以C在设定上尤其要小心

249
00:18:28,175 --> 00:18:32,045
这跟k聚类的情况一样
你需要

250
00:18:32,045 --> 00:18:34,080
最优化邻居的数量

251
00:18:34,080 --> 00:18:35,510
这里你需要对C取最优

252
00:18:35,510 --> 00:18:40,510
这通常是通过交叉验证实现的

253
00:18:40,510 --> 00:18:43,331
通常你观察实证数据

254
00:18:43,331 --> 00:18:47,610
来决定C的值
最优化分类器的表现

255
00:18:49,050 --> 00:18:50,390
在修改之后

256
00:18:50,390 --> 00:18:54,250
问题依然是一个线性约束条件下的多项式规划问题

257
00:18:54,250 --> 00:19:00,003
所以最优算法仍然可以解决这种规划问题

258
00:19:02,080 --> 00:19:05,780
同样如果我们得到了权重和偏误

259
00:19:05,780 --> 00:19:11,360
我们就得到了分类新的对象的分类器

260
00:19:11,360 --> 00:19:13,566
这就是支持向量机的基本思路

261
00:19:16,993 --> 00:19:20,402
来总结一下文本分类的方法

262
00:19:20,402 --> 00:19:25,170
我们引入了许多方法
其中一些是生成模型

263
00:19:25,170 --> 00:19:27,140
有一些是判别方法

264
00:19:27,140 --> 00:19:32,230
它们在优化后的表现是类似的

265
00:19:32,230 --> 00:19:37,920
所以没有最好的方案
每个都有自己的优缺点

266
00:19:37,920 --> 00:19:42,460
效果也会因数据、

267
00:19:42,460 --> 00:19:44,320
问题而异

268
00:19:44,320 --> 00:19:50,610
一个原因是因为特征表示在其中是十分重要的

269
00:19:52,280 --> 00:19:56,470
这些方法都需要有效的特征表示

270
00:19:56,470 --> 00:19:59,400
设计一个有效的特征集

271
00:19:59,400 --> 00:20:03,530
我们需要领域知识
这里人起到了很重要的作用

272
00:20:03,530 --> 00:20:05,608
虽然有许多新的机器学习方法和算法

273
00:20:05,608 --> 00:20:10,020
在特征表示学习上可以起到作用

274
00:20:12,640 --> 00:20:18,169
另一个常见的情况是

275
00:20:18,169 --> 00:20:23,546
它们可能在表现上类似

276
00:20:23,546 --> 00:20:28,220
但是犯了不同的错误

277
00:20:28,220 --> 00:20:30,913
所以表现上可能相似

278
00:20:30,913 --> 00:20:34,070
它们的错误可能是不同的

279
00:20:34,070 --> 00:20:37,630
这也意味着对不同方法
在同一问题下的结果进行比较

280
00:20:37,630 --> 00:20:42,690
会很有用
可能可以组合多种方法

281
00:20:42,690 --> 00:20:49,092
提升结果的稳健性
不会犯相同的错误

282
00:20:49,092 --> 00:20:54,192
所以组合不同的方法的结果

283
00:20:54,192 --> 00:20:59,990
会更稳健
在实际中也更为实用

284
00:20:59,990 --> 00:21:04,530
我们这里介绍的大多数技术属于监督式学习

285
00:21:04,530 --> 00:21:06,990
是一种一般性的方法

286
00:21:06,990 --> 00:21:10,975
也就是说这些方法可以应用在任何文本

287
00:21:10,975 --> 00:21:12,580
或是分类问题当中

288
00:21:12,580 --> 00:21:17,554
只要我们人能够协助
定义一些训练集

289
00:21:17,554 --> 00:21:23,493
设计特征
那么监督式学习和所有这些分类器

290
00:21:23,493 --> 00:21:29,255
可以用于解决这些分类问题

291
00:21:29,255 --> 00:21:34,431
让我们可以将文本内容简化到类别上

292
00:21:34,431 --> 00:21:38,716
或是预测与文本相关的

293
00:21:38,716 --> 00:21:43,250
真实世界变量的整体情况

294
00:21:43,250 --> 00:21:47,875
计算机尝试优化

295
00:21:47,875 --> 00:21:49,908
人类提供的特征组合

296
00:21:49,908 --> 00:21:53,357
就像我说的
组合的方法多种多样

297
00:21:53,357 --> 00:21:56,130
目标函数也不尽相同

298
00:21:58,180 --> 00:22:02,240
但是为了获得好的表现
它们都需要有效的特征

299
00:22:02,240 --> 00:22:03,750
以及大量的训练数据

300
00:22:04,770 --> 00:22:08,870
一般来讲如果你能够优化特征表示

301
00:22:08,870 --> 00:22:13,860
提供更多的训练数据
你就可以做的更好

302
00:22:13,860 --> 00:22:18,390
表现通常受到特征有效性更为明显的影响

303
00:22:18,390 --> 00:22:23,030
而不是对特定分类器的选择

304
00:22:23,030 --> 00:22:26,972
所以特征设计比

305
00:22:26,972 --> 00:22:27,768
选择特定的分类器要更重要

306
00:22:30,844 --> 00:22:34,170
那么如何设计出有效的特征呢？

307
00:22:34,170 --> 00:22:37,360
不走运的是这个可能与具体应用密切相关

308
00:22:37,360 --> 00:22:43,108
所以没什么一般的规律可循

309
00:22:43,108 --> 00:22:47,672
但是我们可以对分类问题进行分析

310
00:22:47,672 --> 00:22:54,400
尝试理解哪些特征
对于我们进行类别的区分会有帮助

311
00:22:54,400 --> 00:22:59,720
一般来讲我们需要许多领域内知识
来辅助特征设计

312
00:23:01,640 --> 00:23:06,180
另一种寻找有效特征的方法就是

313
00:23:06,180 --> 00:23:10,230
对分类结果进行误差分析

314
00:23:10,230 --> 00:23:11,080
比如你可以

315
00:23:11,080 --> 00:23:16,110
看哪一类更容易与其他类相混淆

316
00:23:16,110 --> 00:23:20,890
你可以用一个混淆矩阵来系统检验

317
00:23:20,890 --> 00:23:22,340
类别间的误差情况

318
00:23:22,340 --> 00:23:25,320
之后你可以根据实际情况

319
00:23:25,320 --> 00:23:29,780
找到是犯了哪种错误
哪个特征可以防止错误的发生

320
00:23:29,780 --> 00:23:35,260
这就可以为你的特征设计提供新的思路

321
00:23:35,260 --> 00:23:37,840
所以误差分析通常是很重要的

322
00:23:37,840 --> 00:23:40,860
也是针对你的特定问题提供思路的地方

323
00:23:42,150 --> 00:23:45,220
最后我们可以在机器学习技术充分利用这一点

324
00:23:45,220 --> 00:23:48,710
比如特征选择这个技术我们并没有讲

325
00:23:48,710 --> 00:23:50,390
但是它很重要

326
00:23:50,390 --> 00:23:54,830
它可以选出最有用的特征

327
00:23:54,830 --> 00:23:56,276
在你训练出一个完整的分类器之前

328
00:23:56,276 --> 00:24:00,900
有时训练一个分类器可以帮助你识别哪些特征有较高的

329
00:24:00,900 --> 00:24:01,419
数值。

330
00:24:01,419 --> 00:24:04,658
还有其他方法来确保这个空隙

331
00:24:04,658 --> 00:24:07,538
在模型中指的是识别空白的宽度

332
00:24:07,538 --> 00:24:12,870
比如支持向量机最小化特征权重

333
00:24:12,870 --> 00:24:16,630
你可以限制一些特征

334
00:24:16,630 --> 00:24:19,019
强制只使用一部分特征

335
00:24:21,080 --> 00:24:25,030
还有降维的技术

336
00:24:25,030 --> 00:24:29,450
可以讲一个高维的特征空间转化为一个低维的空间

337
00:24:29,450 --> 00:24:33,150
这是通过对特征进行各式各样的聚类实现的

338
00:24:33,150 --> 00:24:38,150
度量因子会在这里用到

339
00:24:38,150 --> 00:24:42,860
这些与我们之前提到的模型很相似

340
00:24:42,860 --> 00:24:44,820
比如话题模型

341
00:24:44,820 --> 00:24:48,220
比如概率性潜在语义分析或是

342
00:24:48,220 --> 00:24:52,570
潜在狄里克雷分配可以降低特征的维数

343
00:24:52,570 --> 00:24:56,331
想象单词是原有的特征表示

344
00:24:56,331 --> 00:25:01,970
但这种表示可以对应到话题空间当中
假设我们有k个话题

345
00:25:01,970 --> 00:25:04,380
那么一篇文档可以表示为

346
00:25:04,380 --> 00:25:08,750
由话题对应的k个值组成的向量

347
00:25:08,750 --> 00:25:12,380
我们可以用每个话题表示一个维度
我们就得到了k维空间

348
00:25:12,380 --> 00:25:17,920
而不是对应到单词的高维空间

349
00:25:17,920 --> 00:25:21,720
这就是另一种学习有效特征的方法

350
00:25:21,720 --> 00:25:26,200
我们可以利用这些类别来监督

351
00:25:26,200 --> 00:25:28,370
如此低维的结构

352
00:25:29,850 --> 00:25:36,070
原有单词层面的特征也可以结合到

353
00:25:36,070 --> 00:25:40,480
这些维度特征或是低维空间特征当中

354
00:25:40,480 --> 00:25:44,810
提供多种方案
这通常很有用

355
00:25:44,810 --> 00:25:49,940
深度学习是机器学习中一个新开发的技术

356
00:25:51,190 --> 00:25:54,890
学习表征上它尤其有效

357
00:25:54,890 --> 00:25:59,840
深度学习指的是深度神经网络
是另外一种分类器

358
00:25:59,840 --> 00:26:07,110
你可以在模型中嵌入中间特征

359
00:26:07,110 --> 00:26:11,570
这是一个高度非线性的分类器

360
00:26:11,570 --> 00:26:17,220
一些新近的研究让我们
可以有效训练这种复杂网络

361
00:26:17,220 --> 00:26:23,300
这种方法在语音识别和计算机推理中十分有效

362
00:26:23,300 --> 00:26:27,620
最近也被应用到文本上

363
00:26:27,620 --> 00:26:29,530
它已显示出了一些希望

364
00:26:29,530 --> 00:26:33,010
这个方法一个主要的优点是

365
00:26:34,270 --> 00:26:39,010
在特征设计的关系上

366
00:26:39,010 --> 00:26:43,920
它可以学习中间的表征或是自动混合特征

367
00:26:43,920 --> 00:26:49,193
这在学习表征中十分有价值

368
00:26:49,193 --> 00:26:51,660
对于进行文本重新校准来说

369
00:26:51,660 --> 00:26:57,390
虽然在文本域
单词是文本内容的典型表征

370
00:26:57,390 --> 00:27:01,620
因为它们是人类交流使用的成像

371
00:27:01,620 --> 00:27:08,160
它们也足以表征许多任务

372
00:27:08,160 --> 00:27:11,430
如果需要新的表征

373
00:27:11,430 --> 00:27:15,250
人们会发明一个新词

374
00:27:15,250 --> 00:27:18,320
所以我们认为深度学习对于

375
00:27:18,320 --> 00:27:22,610
文本处理的价值会比计算机推理以及语音识别中要低

376
00:27:22,610 --> 00:27:26,490
因为存在其他对应的信息

377
00:27:26,490 --> 00:27:29,920
可以用在特征设计当中

378
00:27:31,160 --> 00:27:35,020
但是人们依然对有效特征的学习充满希望

379
00:27:35,020 --> 00:27:35,857
尤其是在复杂任务当中

380
00:27:35,857 --> 00:27:39,850
比如一个分析之所以有效

381
00:27:41,230 --> 00:27:44,760
是因为他可以提供单词之外的信息

382
00:27:47,030 --> 00:27:50,240
从训练示例来看

383
00:27:50,240 --> 00:27:53,940
一般很难获得大量的训练例子
因为它涉及

384
00:27:53,940 --> 00:27:54,560
人工劳动

385
00:27:56,310 --> 00:27:58,570
但是我们还是有一些办法

386
00:27:58,570 --> 00:28:04,830
一种是假设一些低质量的训练示例也可以利用起来

387
00:28:04,830 --> 00:28:07,800
这些可以叫做伪训练示例

388
00:28:07,800 --> 00:28:13,220
比如如果你在网上发评论
可能会有综合评价

389
00:28:13,220 --> 00:28:21,250
那么训练一个分类器
意味着我们想要正或负值

390
00:28:21,250 --> 00:28:24,860
我们把评论分为两类

391
00:28:24,860 --> 00:28:31,570
我们可以假设所有五星评价都是正面的训练示例

392
00:28:31,570 --> 00:28:33,270
一星是负面的

393
00:28:33,270 --> 00:28:34,190
当然了

394
00:28:34,190 --> 00:28:38,520
当然有时候五星也可以提到负面评价

395
00:28:38,520 --> 00:28:43,180
训练样本的质量不高
但是它们仍然有用

396
00:28:45,200 --> 00:28:47,970
另一个想法是利用未标注的数据

397
00:28:47,970 --> 00:28:50,830
有一类技术叫做半监督机器学习技术

398
00:28:50,830 --> 00:28:55,685
可以让你将已标注的数据与未标注的结合起来

399
00:28:55,685 --> 00:29:01,070
在我们这里很容易看到
这种混合模型可以

400
00:29:01,070 --> 00:29:03,760
用于文本聚类和文本分类

401
00:29:03,760 --> 00:29:09,220
你可以想象如果你有许多
未标记的文本数据用来进行分类

402
00:29:09,220 --> 00:29:15,620
你可以对文本数据进行聚类
学习其中类别

403
00:29:15,620 --> 00:29:18,088
然后尝试以某种方式列出这些类别

404
00:29:18,088 --> 00:29:23,230
通过训练数据定义的类别

405
00:29:23,230 --> 00:29:26,390
我们已经知道哪个文档属于哪一类别

406
00:29:26,390 --> 00:29:31,620
因此你可以利用算法来组合它们

407
00:29:31,620 --> 00:29:37,390
它从本质上可以选取有用的单词并标注它们

408
00:29:37,390 --> 00:29:39,320
你可以从另一个方向来想

409
00:29:39,320 --> 00:29:43,804
一般我们

410
00:29:43,804 --> 00:29:48,480
对没有标注的文本进行分类

411
00:29:48,480 --> 00:29:54,040
我们会假设具有高置信度的分类结果是可信的

412
00:29:54,040 --> 00:29:58,600
之后如果你有了更多的训练数据

413
00:29:58,600 --> 00:30:03,450
你可以知道一些可以标为分类1
一些是分类2

414
00:30:03,450 --> 00:30:06,380
虽然标注不是完全可信的

415
00:30:06,380 --> 00:30:07,830
但是它们仍然很有用

416
00:30:07,830 --> 00:30:14,720
假设它们是标注训练实例

417
00:30:14,720 --> 00:30:19,940
我们可以将其与真实训练示例结合
优化分类方法

418
00:30:19,940 --> 00:30:22,110
这个想法是很有用的

419
00:30:23,980 --> 00:30:28,280
如果启用数据与训练数据不同

420
00:30:28,280 --> 00:30:32,410
我们可能需要使用其他高级的机器学习方法

421
00:30:32,410 --> 00:30:35,150
比如域自适应学习或迁移学习

422
00:30:35,150 --> 00:30:37,580
这样我们就能

423
00:30:37,580 --> 00:30:42,450
借用相关问题的一些不同的训练集

424
00:30:42,450 --> 00:30:44,470
或者利用其他的分类任务

425
00:30:46,780 --> 00:30:52,130
这些任务与现有任务存在着不同的数据分布

426
00:30:52,130 --> 00:30:54,190
不过通常使用两个不同域的方法时

427
00:30:54,190 --> 00:30:57,640
我们应该很谨慎
不要对训练域进行过度你和

428
00:30:57,640 --> 00:31:02,300
但是我们仍希望利用相关训练数据的一些特征

429
00:31:02,300 --> 00:31:07,270
比如对新闻的训练分类

430
00:31:07,270 --> 00:31:12,410
可能并不能为Twitter分析提供有效的分类器

431
00:31:12,410 --> 00:31:19,490
但是你仍可以从中学到一些
研究tweets的方法

432
00:31:19,490 --> 00:31:25,470
因此机器学习可以有效地协助你解决这一问题

433
00:31:25,470 --> 00:31:30,259
这里是推荐的文献
你可以从中得到更多细节

434
00:31:30,259 --> 00:31:33,271
是关于我们讲过的方法的

435
00:31:33,271 --> 00:31:43,271
[背景音乐]
翻译: MingShi |审阅: 
Coursera Global Translator Community